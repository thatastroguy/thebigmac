{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Multiple AGN Catalog (Big MAC) DR1 - ``Offset AGN'' Candidates (== Dual SMBH Candidates)\n",
    "# Author: R. W. Pfeifle\n",
    "# Original Date Created: 10 Sept. 2020\n",
    "# Last Revision Date: 2 Oct. 2020\n",
    "\n",
    "# New Form Creation Date: 13 January 2022\n",
    "# Last Revision: 16 February 2024\n",
    "\n",
    "# Purpose: Combine various catalogs of double-peaked optically selected dual AGN candidates\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in packages for pandas, astropy, etc. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.io import ascii\n",
    "from astropy.table import Column, MaskedColumn\n",
    "from astropy.io.ascii import masked\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.cosmology import LambdaCDM \n",
    "from astroquery.simbad import Simbad\n",
    "from astroquery.sdss import SDSS\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "import os \n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.3, Ode0=0.7) #Creating our choice of cosmology here...\n",
    "\n",
    "pd.set_option('display.max_columns', 300) # Setting max number of rows per df to be the size of the df\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_coords(df,dfcol):\n",
    "    if (len(dfcol[0])) == 14:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=1) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=6) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=6, stop=13) # Stripping the RA parts\n",
    "        df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=6) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=8) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #print(dfcol.apply(len))\n",
    "    elif (len(dfcol[0])) == 19:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=1) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "        df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #elif dfcol.apply(len) ==\n",
    "    else:\n",
    "        print('Error Encountered')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm rewriting our matching algorithm using the search_around_sky() function\n",
    "# It may not always be the best option, but at least for these double peaked catalogs, I think I'm going to run \n",
    "# with it\n",
    "\n",
    "def match_tables_fib(t1,t2,match_tol):\n",
    "    if 'level_0' in t1.columns:\n",
    "        t1.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t1.reset_index(drop=False, inplace=True)\n",
    "    if 'level_0' in t2.columns:\n",
    "        t2.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t2.reset_index(inplace=True, drop=False)\n",
    "    t1['Table_flag'] = 'Table1'\n",
    "    t2['Table_flag'] = 'Table2'\n",
    "    # First we begin by matching RA1 and Dec1 of t1 to RA1 and Dec1 of t2\n",
    "    c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "    c2 = SkyCoord(ra=t2['RA1_deg']*u.degree, dec=t2['Dec1_deg']*u.degree) # storing coordinates for table 2\n",
    "    # Adding a match tolerance here, with user input for the function\n",
    "    max_sep = match_tol * u.arcsec # The max match tolerance will be 5''\n",
    "    #idx2, d2d2, d3d2 = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "    idx1, idx2, _, _ = c2.search_around_sky(c1, max_sep) \n",
    "    # idx1 and idx2 are the indices in table 1 and table 2 which are the closest matching rows to each other\n",
    "    # Note, we should not need to cross match RA1 vs. RA2, across table because the double peaked sources only have\n",
    "    # a single set of coordinates at this point\n",
    "    # We need to make tables for t1 and t2 that do not include the matched items\n",
    "    t1unique = (t1[~t1['index'].isin(idx1)]).reset_index(drop=True)\n",
    "    t2unique = (t2[~t2['index'].isin(idx2)]).reset_index(drop=True)\n",
    "    # And then we need a table for the matches items where we ensure they are properly matching (SDSS names should \\\n",
    "    # be the same), and then remove the duplicates, store the relevant info from the second table, and concatenate \\\n",
    "    # this with the primary table\n",
    "    tmatches = pd.concat([(t1.iloc[idx1]),(t2.iloc[idx2])]).sort_values(by='Name1').reset_index(drop=True)\n",
    "    tunique = pd.concat([t1unique, t2unique]).sort_values(by='Name1').reset_index(drop=True)\n",
    "    #\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'Paper(s)'] += \" ; \" + t2['Paper(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'BibCode(s)'] += \" ; \" + t2['BibCode(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'DOI(s)'] += \" ; \" + t2['DOI(s)'][0]\n",
    "    return tunique, tmatches, idx1, idx2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,smith2012,5)\n",
    "\n",
    "# # Adding the DOI, author, and bibcode info to all of the Smith+2010 rows here in the matches table...\n",
    "# for index, row in tmatches.iterrows():\n",
    "#     if row['Table_flag']!='Table2':\n",
    "#         tmatches.at[index, 'Paper(s)'] += ' ; Smith+2012'\n",
    "#         tmatches.at[index, 'BibCode(s)'] += ' ; 2012ApJ...752...63S' \n",
    "#         tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/752/1/63'\n",
    "\n",
    "# # Now clipping out all Smith+2010 rows from the matches table\n",
    "# tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "\n",
    "# # Concatenating everything together to generate a master table here\n",
    "# the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "# the_whills.drop(labels=['index','Table_flag'], axis=1, inplace=True) #'level_0',\n",
    "\n",
    "# tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,fu2011d,3)\n",
    "\n",
    "# #len(tmatches)\n",
    "\n",
    "# # This finds 16 matches, which is exactly what we'd expect (there are 16 doubles in the table from Fu+2011)\n",
    "\n",
    "# for i, j in zip(idx1, idx2):\n",
    "#     #print(\"i:\", i, \"j:\", j)\n",
    "#     #print(\"Sep:\", mcgurk2015t4.at[j, 'NIRC2sep(as)'])\n",
    "#     #print(\"dV:\", mcgurk2015t4.at[j, 'dV[OIII]'])\n",
    "#     the_whills.at[i, 'Sep'] = fu2011d.at[j, 'Sep_as']\n",
    "#     the_whills.at[i, 'Paper(s)'] += ' ; Fu+2011'\n",
    "#     the_whills.at[i, 'BibCode(s)'] += ' ; 2011ApJ...733..103F' \n",
    "#     the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/733/2/103'\n",
    "#     the_whills.at[i, 'dV'] = fu2011d.at[j, 'dV']\n",
    "#     the_whills.at[i, 'Notes'] += ' Fu+2011 companions within 3 arcseconds,'\n",
    "\n",
    "# for index, row in the_whills.iterrows():\n",
    "#     if index in idx1:\n",
    "#         the_whills.at[index, 'Paper(s)'] += ' ; Fu+2012'\n",
    "#         the_whills.at[index, 'BibCode(s)'] += ' ; 2012ApJ...745...67F' \n",
    "#         the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/745/1/67'\n",
    "#         the_whills.at[index, 'Notes'] += ' Companion(s) within 3 arcseconds.'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we're going too load in the 'lost' offset AGN candidates from Comerford+2009\n",
    "# we will NOT be matching it to the tables in this notebook. Instead we will match it directly with the main \\\n",
    "# table in the matching catalogs notebook\n",
    "\n",
    "# Here we're loading in the double-peaked emission line galaxy catalog of comerford2009\n",
    "comerford2009 = pd.read_csv('Tables/comerford2009/Comerford2009_offsetagns.csv', sep=',')\n",
    "\n",
    "comerford2009['Name1'] = comerford2009['ID']\n",
    "comerford2009['Name2'] = \"-99\"\n",
    "comerford2009['z1'] = comerford2009['zabs']\n",
    "comerford2009['z2'] = -99\n",
    "comerford2009['z1_type'] = \"spec\"\n",
    "comerford2009['z2_type'] = \"-99\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(comerford2009,comerford2009['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = comerford2009['RA1'], dec = comerford2009['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "comerford2009['RA1_deg'] = coordconvert.ra.degree\n",
    "comerford2009['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "comerford2009['RA2'] = comerford2009['RA1']\n",
    "comerford2009['Dec2'] = comerford2009['Dec1']\n",
    "\n",
    "# just duplicates of RA1 for now. We will correc this later....\n",
    "comerford2009['RA2_deg'] = comerford2009['RA1_deg']\n",
    "comerford2009['Dec2_deg'] = comerford2009['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "comerford2009['Equinox1'] = \"J2000\"\n",
    "comerford2009['Coordinate_waveband1'] = \"Optical\"\n",
    "comerford2009['Coordinate_Source1'] = \"EGSD2\"\n",
    "\n",
    "comerford2009['System Type'] = 'Dual SMBH Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "comerford2009['Brightness1'] = -99\n",
    "comerford2009['Brightness_band1'] = \"-99\"\n",
    "comerford2009['Brightness_type1'] = \"-99\"\n",
    "\n",
    "comerford2009['Brightness2'] = -99\n",
    "comerford2009['Brightness_band2'] = \"-99\"\n",
    "comerford2009['Brightness_type2'] = \"-99\"\n",
    "\n",
    "comerford2009['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "comerford2009['dV'] = comerford2009['vem-vabs(kms-1)']\n",
    "## dV here is ...\n",
    "\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "comerford2009['Selection Method'] = \"Optical Spectroscopy / Slit Optical Spectroscopy / Velocity Offset Narrow Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "comerford2009['Confirmation Method'] = \"-99\"\n",
    "comerford2009['Paper(s)'] = \"Comerford+2009a\"\n",
    "comerford2009['BibCode(s)'] = \"2009ApJ...698..956C\"\n",
    "comerford2009['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/698/1/956\"\n",
    "\n",
    "cols_to_drop = ['ID','U-B','MB(-5logh)','[Oiii]λ5007/Hβ','zabs','zem','vem-vabs(kms-1)','vem-vabs(kms-1)_pm']\n",
    "\n",
    "for i in cols_to_drop:\n",
    "    try:\n",
    "        comerford2009.drop(labels=str(i), axis=1, inplace=True)\n",
    "    except:\n",
    "        print('This column is not available in this version. Must have modified another notebook.')\n",
    "\n",
    "comerford2009.to_csv('Comerford2009_offsettargets.csv', sep=',', index=False)\n",
    "\n",
    "#comerford2009\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're loading in the double-peaked emission line galaxy catalog of Comerford2013\n",
    "comerford2013 = ((Table.read('Tables/Comerford2013/table1.dat', readme = 'Tables/Comerford2013/ReadMe', format='ascii.cds')).to_pandas()).drop(columns=['---','HbVoff','e_HbVoff','O3Voff','e_O3Voff','HaVoff','e_HaVoff','O3/Hb','e_O3/Hb','N2/Ha','e_N2/Ha','S2/Ha','e_S2/Ha','O1/Ha','e_O1/Ha','Class','n_Class'])\n",
    "# table1.dat is a modified version of Wang's catalog in which I've added a duplicate row for each target\n",
    "# since all of these are candidate dual AGN systems\n",
    "\n",
    "# Comerford2013 looked at 173 Type 2 AGNs in Deep2 and found only two double-peaked dual AGN candidates \\\n",
    "# and found five offset AGNs. \n",
    "\n",
    "NDWFS = ['J143044.06+335224.5','J143053.69+345836.4','J143316.48+353259.3','J143317.07+344912.0','J143710.03+343530.1']\n",
    "comerford2013 = comerford2013[comerford2013['NDWFS'].isin(NDWFS)]\n",
    "\n",
    "comerford2013['Coordinates'] = comerford2013['NDWFS'].str.slice(start=1)\n",
    "comerford2013['RA_test'] = comerford2013['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "comerford2013['Dec_test'] = comerford2013['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "comerford2013['RA1'] = comerford2013['RA_test'].str.slice(start=0, stop=2)+\":\"+comerford2013['RA_test'].str.slice(start=2, stop=4)+\":\"+comerford2013['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "comerford2013['Dec1'] = comerford2013['Dec_test'].str.slice(start=0, stop=3)+\":\"+comerford2013['Dec_test'].str.slice(start=3, stop=5)+\":\"+comerford2013['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "comerford2013.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "\n",
    "\n",
    "comerford2013['Name1'] = comerford2013['NDWFS']\n",
    "comerford2013['Name2'] = \"-99\"\n",
    "comerford2013['z1'] = comerford2013['z']\n",
    "comerford2013['z2'] = -99\n",
    "comerford2013['z1_type'] = \"spec\"\n",
    "comerford2013['z2_type'] = \"-99\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(comerford2013,comerford2013['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = comerford2013['RA1'], dec = comerford2013['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "comerford2013['RA1_deg'] = coordconvert.ra.degree\n",
    "comerford2013['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "comerford2013['RA2'] = comerford2013['RA1']\n",
    "comerford2013['Dec2'] = comerford2013['Dec1']\n",
    "\n",
    "comerford2013['RA2_deg'] = comerford2013['RA1_deg']\n",
    "comerford2013['Dec2_deg'] = comerford2013['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "comerford2013['Equinox1'] = \"J2000\"\n",
    "comerford2013['Coordinate_waveband1'] = \"Optical\"\n",
    "comerford2013['Coordinate_Source1'] = \"AGES\"\n",
    "\n",
    "comerford2013['System Type'] = 'Dual SMBH Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "comerford2013['Brightness1'] = -99\n",
    "comerford2013['Brightness_band1'] = \"-99\"\n",
    "comerford2013['Brightness_type1'] = \"-99\"\n",
    "\n",
    "comerford2013['Brightness2'] = -99\n",
    "comerford2013['Brightness_band2'] = \"-99\"\n",
    "comerford2013['Brightness_type2'] = \"-99\"\n",
    "\n",
    "comerford2013['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "comerford2013['dV'] = -99\n",
    "for index, row in comerford2013.iterrows():\n",
    "    if row['NDWFS']=='J143044.06+335224.5':\n",
    "        comerford2013.at[index, 'dV'] = 217.7\n",
    "    if row['NDWFS']=='J143053.69+345836.4':\n",
    "        comerford2013.at[index, 'dV'] = -123.4\n",
    "    if row['NDWFS']=='J143316.48+353259.3':\n",
    "        comerford2013.at[index, 'dV'] = -100.9\n",
    "    if row['NDWFS']=='J143317.07+344912.0':\n",
    "        comerford2013.at[index, 'dV'] = -143.0\n",
    "    if row['NDWFS']=='J143710.03+343530.1':\n",
    "        comerford2013.at[index, 'dV'] = -97.6\n",
    "# dV here is the weighted mean velocity offste measured from the Balmer and forbidden lines\n",
    "\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "comerford2013['Selection Method'] = \"Optical Spectroscopy / Fiber Spectroscopy / Velocity Offset Narrow Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "comerford2013['Confirmation Method'] = \"-99\"\n",
    "comerford2013['Paper(s)'] = \"Comerford+2013\"\n",
    "comerford2013['BibCode(s)'] = \"2013ApJ...777...64C\"\n",
    "comerford2013['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/777/1/64\"\n",
    "\n",
    "#comerford2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the tables from Comerford+2014 here now...\n",
    "\n",
    "comerford2014 = ((Table.read('Tables/Comerford2014/table1.dat', readme = 'Tables/Comerford2014/ReadMe', format='ascii.cds')).to_pandas()).drop(columns=['---'])\n",
    "\n",
    "comerford2014['Name1'] = comerford2014['SDSS']\n",
    "comerford2014['Name2'] = \"-99\"\n",
    "comerford2014['z1'] = comerford2014['z']\n",
    "comerford2014['z2'] = -99\n",
    "comerford2014['z1_type'] = \"spec\"\n",
    "comerford2014['z2_type'] = \"-99\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(comerford2014,comerford2014['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "comerford2014['RA1'] = comerford2014['RA']\n",
    "comerford2014['Dec1'] = comerford2014['Dec']\n",
    "\n",
    "coordconvert = SkyCoord(ra = comerford2014['RA1'], dec = comerford2014['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "comerford2014['RA1_deg'] = coordconvert.ra.degree\n",
    "comerford2014['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "comerford2014['RA2'] = comerford2014['RA']\n",
    "comerford2014['Dec2'] = comerford2014['Dec']\n",
    "\n",
    "comerford2014['RA2_deg'] = comerford2014['RA1_deg']\n",
    "comerford2014['Dec2_deg'] = comerford2014['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "comerford2014['Equinox1'] = \"J2000\"\n",
    "comerford2014['Coordinate_waveband1'] = \"Optical\"\n",
    "comerford2014['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "comerford2014['System Type'] = 'Dual SMBH Candidate'\n",
    "# these objects could technically also be recoil candidates\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "comerford2014['Brightness1'] = -100\n",
    "comerford2014['Brightness_band1'] = \"-100\"\n",
    "comerford2014['Brightness_type1'] = \"-100\"\n",
    "\n",
    "comerford2014['Brightness2'] = -100\n",
    "comerford2014['Brightness_band2'] = \"-100\"\n",
    "comerford2014['Brightness_type2'] = \"-100\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "comerford2014['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "\n",
    "#comerford2014['Sep(kpc)'] = comerford2014['Sep']*((cosmo.arcsec_per_kpc_proper(comerford2014['z']))**(-1))\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "comerford2014['dV'] = comerford2014['DelVW'] \n",
    "# dV here is taken from the weighted average of the velocity offsets measured for the Balmer and the forbidden lines\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "comerford2014['Selection Method'] = \"Optical Spectroscopy / Fiber Spectroscopy / Velocity Offset Narrow Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "comerford2014['Confirmation Method'] = \"-99\"\n",
    "comerford2014['Paper(s)'] = \"Comerford+2014\"\n",
    "comerford2014['BibCode(s)'] = \"2014ApJ...789..112C\"\n",
    "comerford2014['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/789/2/112\"\n",
    "\n",
    "comerford2014['Notes'] = ''\n",
    "\n",
    "\n",
    "#comerford2014\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(comerford2013,comerford2014,25)\n",
    "\n",
    "# Apparently no matches to within 20''! \n",
    "print(len(tmatches))\n",
    "\n",
    "# concatenating now...\n",
    "\n",
    "the_whills = pd.concat([comerford2013,comerford2014])\n",
    "the_whills.reset_index(drop=True, inplace=True)\n",
    "# Verified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in information from Allen+2015\n",
    "\n",
    "allenobjs = ['J120401.97+012641.6','J011333.06+002947.9'] # Allen+ used these: J120401.97+012641.5 J011333.05+002948.0\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Comerford+2014 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in allenobjs:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Allen+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015MNRAS.451.2780A' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stv1121'\n",
    "\n",
    "allenobjs = ['J120401.97+012641.6']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in allenobjs:\n",
    "        the_whills.at[index, 'Notes']+=' Allen+ use IFS observations to show that the offset emission line(s) used to flag this target originally is not the result of a binary system but due to large-scale motions of gas in the galaxy. If this system does constitute a merger they cannot rule out the possible presence of two SMBHs. There is a suggestion that it could be a merger but no firm evidence. '\n",
    "        the_whills.at[index, 'System Type'] = 'Likely Single AGN'\n",
    "\n",
    "allenobjs = ['J011333.06+002947.9']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in allenobjs:\n",
    "        the_whills.at[index, 'Notes']+=' Allen+ uses IFS observations to show that the offset emission line(s) are likely due to an outflow rather than the motion of a NLR. They argue strongly against a binary scenario.'\n",
    "        the_whills.at[index, 'System Type'] = 'Likely Single AGN'\n",
    "\n",
    "#the_whills\n",
    "\n",
    "# verified that this matching works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in the information from Barrows+\n",
    "\n",
    "barrowsobjs = ['J111519.97+542316.6'] #J111519.98+542316.65\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in barrowsobjs:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Barrows+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...829...37B' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/829/1/37'\n",
    "        #the_whills.at[index, 'System Type']='Dual AGN'\n",
    "        the_whills.at[index, 'Notes']+=' Selected by Barrows+ as an offset AGN candidate.'\n",
    "        #the_whills.at[index, 'Confirmation Method'] = 'X-ray Imaging / X-ray Spectroscopy / Optical Spectroscopy'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in information from Muller-Sanchez+2016\n",
    "\n",
    "# these were Muller-Sanchez's original names:\n",
    "#msobjs = ['J101847.57+294114.10','J105553.64+152027.40','J111729.21+614015.30','J134640.79+522836.60']\n",
    "msobjs = ['J101847.57+294114.1','J105553.64+152027.5','J111729.22+614015.2','J134640.79+522836.5']\n",
    "    \n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in msobjs:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Müller-Sánchez+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...830...50M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/830/1/50'\n",
    "        #the_whills.at[index, 'Notes']+='AGNs confirmed via X-rays and reanalyses of optical spectroscopy.'\n",
    "\n",
    "\n",
    "\n",
    "#msobjs = ['J101847.57+294114.10','J105553.64+152027.40','J134640.79+522836.60']\n",
    "msobjs = ['J101847.57+294114.1','J105553.64+152027.5','J134640.79+522836.5']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in msobjs:\n",
    "        the_whills.at[index, 'Notes']+=' Muller-Sanchez+2016 used OSIRIS IFS observations to show that the observed spatial distribution and kinematics of the ionized gas indicate the presence of outflows and are spatially offset from the center of the galaxy. The spatially offset peaks in the emission can be explained via shocks. The integrated OSIRIS spectrum shows the same velocity offset as observed in SDSS but the spectral profiles are different; the near-IR lines show clear asymmetric wings not seen in the optical.'\n",
    "\n",
    "msobjs = ['J111729.22+614015.2'] \n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in msobjs:\n",
    "        the_whills.at[index, 'Notes']+=' Muller-Sanchez+2016 used OSIRIS IFS observations to show a counter-rotating circumnuclear disk contains the peak of the offset Pa-alpha emission. The most plausible origin of this enhanced emission is that it lies at an intersection zone between the nuclear disk and bar of the galaxy where cloud-cloud collisions drive the emission. The integrated OSIRIS spectrum shows the same velocity offset as observed in SDSS.'\n",
    "\n",
    "\n",
    "\n",
    "# Verified that this matches properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add in any missing tables from Comerford+ and all of the Barrows+ tables\n",
    "\n",
    "# I think we're missiing comerford2009? And maybe another one post-2010\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here we're going to load in and format the Barrows+2016 table\n",
    "# NOTE: we have removed the objects already flagged in other tables\n",
    "\n",
    "# Namely:\n",
    "#J080523.29+281815.8\n",
    "#J090714.45+520343.4\n",
    "#J105842.44+314457.6\n",
    "#J111519.23+542310.9\n",
    "#\n",
    "#J111458.01+403611.4\n",
    "#134442.1781\n",
    "#J110851.04+065901.4\n",
    "#J123420.14+475155.9\n",
    "\n",
    "barrows2016 = pd.read_csv('Tables/Barrows2016/Barrows2016.csv', sep=',')\n",
    "\n",
    "# here we're going too load in the 'lost' offset AGN candidates from Barrows+\n",
    "# we will NOT be matching it to the tables in this notebook. Instead we will match it directly with the main \\\n",
    "# table in the matching catalogs notebook\n",
    "\n",
    "\n",
    "barrows2016['Name1'] = barrows2016['Name1']\n",
    "barrows2016['Name2'] = \"-99\"\n",
    "barrows2016['z1'] = barrows2016['z1']\n",
    "barrows2016['z2'] = -99\n",
    "barrows2016['z1_type'] = \"spec\"\n",
    "barrows2016['z2_type'] = \"-99\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(barrows2016,barrows2016['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = barrows2016['RA1'], dec = barrows2016['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "barrows2016['RA1_deg'] = coordconvert.ra.degree\n",
    "barrows2016['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "barrows2016['RA2'] = barrows2016['RA1']\n",
    "barrows2016['Dec2'] = barrows2016['Dec1']\n",
    "\n",
    "# just duplicates of RA1 for now. We will correc this later....\n",
    "barrows2016['RA2_deg'] = barrows2016['RA1_deg']\n",
    "barrows2016['Dec2_deg'] = barrows2016['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "barrows2016['Equinox1'] = \"J2000\"\n",
    "barrows2016['Coordinate_waveband1'] = \"Optical\"\n",
    "barrows2016['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "barrows2016['System Type'] = 'Dual SMBH Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "barrows2016['Brightness1'] = -99\n",
    "barrows2016['Brightness_band1'] = \"-99\"\n",
    "barrows2016['Brightness_type1'] = \"-99\"\n",
    "\n",
    "barrows2016['Brightness2'] = -99\n",
    "barrows2016['Brightness_band2'] = \"-99\"\n",
    "barrows2016['Brightness_type2'] = \"-99\"\n",
    "\n",
    "barrows2016['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#barrows2016['dV'] = barrows2016['vem-vabs(kms-1)']\n",
    "#for index, row in barrows2016.iterrows():\n",
    "#    if row['NDWFS']=='J143044.06+335224.5':\n",
    "#        barrows2016.at[index, 'dV'] = 217.7\n",
    "#    if row['NDWFS']=='J143053.69+345836.4':\n",
    "#        barrows2016.at[index, 'dV'] = -123.4\n",
    "#    if row['NDWFS']=='J143316.48+353259.3':\n",
    "#        barrows2016.at[index, 'dV'] = -100.9\n",
    "#    if row['NDWFS']=='J143317.07+344912.0':\n",
    "#        barrows2016.at[index, 'dV'] = -143.0\n",
    "#    if row['NDWFS']=='J143710.03+343530.1':\n",
    "#        barrows2016.at[index, 'dV'] = -97.6\n",
    "## dV here is the weighted mean velocity offste measured from the Balmer and forbidden lines\n",
    "\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "barrows2016['Selection Method'] = \"Optical Spectroscopy / Optical Fiber Spectroscopy / Optical Spectroscopic Emission Line Ratios / X-ray Imaging / X-ray Positional Offset\" #DPSELs\n",
    "barrows2016['Confirmation Method'] = \"-99\"\n",
    "barrows2016['Paper(s)'] = \"Barrows+2016\"\n",
    "barrows2016['BibCode(s)'] = \"2016ApJ...829...37B\"\n",
    "barrows2016['DOI(s)'] = \"https://doi.org/10.3847/0004-637X/829/1/37\"\n",
    "\n",
    "barrows2016.to_csv('barrows2016_offsettargets.csv', sep=',', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now formatting the table...\n",
    "the_whills.drop(labels=['index','NDWFS','z','Table_flag','SDSS','e_z','DelVB','e_DelVB','DelVF','e_DelVF',\\\n",
    "                        'DelVW','e_DelVW'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now saving the table...\n",
    "the_whills.to_csv('Offset_AGN_tables_DR1.csv', sep=',', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
