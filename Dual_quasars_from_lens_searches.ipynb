{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79974a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Washington Multiple AGN (WMAGN) Catalog DR1 - Dual Quasars/Candidates from quasar lens searches\n",
    "# Author: R. W. Pfeifle\n",
    "# Creation Date: 13 January 2022\n",
    "# Last Revision: 11 December 2023\n",
    "\n",
    "# Purpose: Combine various catalogs of double-peaked optically selected dual AGN candidates\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in packages for pandas, astropy, etc. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.io import ascii\n",
    "from astropy.table import Column, MaskedColumn\n",
    "from astropy.io.ascii import masked\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.cosmology import LambdaCDM \n",
    "from astroquery.simbad import Simbad\n",
    "from astroquery.sdss import SDSS\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "import os \n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.3, Ode0=0.7) #Creating our choice of cosmology here...\n",
    "\n",
    "pd.set_option('display.max_columns', 300) # Setting max number of rows per df to be the size of the df\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_coords1(df,dfcol):\n",
    "    if (len(dfcol[0])) == 18:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=5) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=6) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=6, stop=13) # Stripping the RA parts\n",
    "        df['RA1'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=6) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec1'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=8) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #print(dfcol.apply(len))\n",
    "    elif (len(dfcol[0])) == 23:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=5) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "        df['RA1'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec1'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #elif dfcol.apply(len) ==\n",
    "    else:\n",
    "        print('Error Encountered')\n",
    "        \n",
    "def name_to_coords2(df,dfcol):\n",
    "    if (len(dfcol[0])) == 18:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=5) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=6) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=6, stop=13) # Stripping the RA parts\n",
    "        df['RA2'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=6) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec2'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=8) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #print(dfcol.apply(len))\n",
    "    elif (len(dfcol[0])) == 23:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=5) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "        df['RA2'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec2'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #elif dfcol.apply(len) ==\n",
    "    else:\n",
    "        print('Error Encountered')\n",
    "\n",
    "def name_to_coords(df,dfcol):\n",
    "    if (len(dfcol[0])) == 18:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=1) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=6) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=6, stop=13) # Stripping the RA parts\n",
    "        df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=6) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=8) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #print(dfcol.apply(len))\n",
    "    elif (len(dfcol[0])) == 23:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=5) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "        df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #elif dfcol.apply(len) ==\n",
    "    else:\n",
    "        print('Error Encountered')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Convert Ra and Dec from degrees to sexagesimal format\n",
    "#coords = SkyCoord(ra=df['ra']*u.degree, dec=df['dec']*u.degree, frame='icrs')\n",
    "#df['ra_sexagesimal'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "#df['dec_sexagesimal'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "#\n",
    "## Convert RA and Dec to the desired format\n",
    "#ra_format = coords.ra.to_string(unit=u.hour, sep='', precision=2, pad=True)\n",
    "#dec_format = coords.dec.to_string(unit=u.deg, sep='', precision=2, alwayssign=True, pad=True)\n",
    "#\n",
    "## Concatenate to form J_format using a loop\n",
    "#df['J_format'] = [\"J\" + ra + dec for ra, dec in zip(ra_format, dec_format)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08531cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm rewriting our matching algorithm using the search_around_sky() function\n",
    "# It may not always be the best option, but at least for these double peaked catalogs, I think I'm going to run \n",
    "# with it\n",
    "\n",
    "def match_tables_pairs(t1,t2,match_tol):\n",
    "    if 'level_0' in t1.columns:\n",
    "        t1.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t1.reset_index(drop=False, inplace=True)\n",
    "    if 'level_0' in t2.columns:\n",
    "        t2.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t2.reset_index(inplace=True, drop=False)\n",
    "    t1['Table_flag'] = 'Table1'\n",
    "    t2['Table_flag'] = 'Table2'\n",
    "    # First we begin by matching RA1 and Dec1 of t1 to RA1 and Dec1 of t2\n",
    "    c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "    c2 = SkyCoord(ra=t2['RA1_deg']*u.degree, dec=t2['Dec1_deg']*u.degree) # storing coordinates for table 2\n",
    "    # Adding a match tolerance here, with user input for the function\n",
    "    max_sep = match_tol * u.arcsec # The max match tolerance will be 5''\n",
    "    #idx2, d2d2, d3d2 = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "    idx1, idx2, _, _ = c2.search_around_sky(c1, max_sep) \n",
    "    # idx1 and idx2 are the indices in table 1 and table 2 which are the closest matching rows to each other\n",
    "    # Note, we should not need to cross match RA1 vs. RA2, across table because the double peaked sources only have\n",
    "    # a single set of coordinates at this point\n",
    "    # We need to make tables for t1 and t2 that do not include the matched items\n",
    "    t1unique = (t1[~t1['index'].isin(idx1)]).reset_index(drop=True)\n",
    "    t2unique = (t2[~t2['index'].isin(idx2)]).reset_index(drop=True)\n",
    "    # And then we need a table for the matches items where we ensure they are properly matching (SDSS names should \\\n",
    "    # be the same), and then remove the duplicates, store the relevant info from the second table, and concatenate \\\n",
    "    # this with the primary table\n",
    "    tmatches_1 = pd.concat([(t1.iloc[idx1]),(t2.iloc[idx2])]).sort_values(by='Name1').reset_index(drop=True)\n",
    "    tunique_1 = pd.concat([t1unique, t2unique]).sort_values(by='Name1').reset_index(drop=True)\n",
    "    #\n",
    "    # Now we need to match RA1 and Dec1 of t1 to RA2 and Dec2 of t2 in case the pairs are mixeup between tables\n",
    "    c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "    c2 = SkyCoord(ra=t2['RA2_deg']*u.degree, dec=t2['Dec2_deg']*u.degree) # storing coordinates for table 2\n",
    "    # Adding a match tolerance here, with user input for the function\n",
    "    max_sep = match_tol * u.arcsec # The max match tolerance will be 5''\n",
    "    #idx2, d2d2, d3d2 = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "    idx1_2, idx2_2, _, _ = c2.search_around_sky(c1, max_sep) \n",
    "    # idx1 and idx2 are the indices in table 1 and table 2 which are the closest matching rows to each other\n",
    "    # Note, we should not need to cross match RA1 vs. RA2, across table because the double peaked sources only have\n",
    "    # a single set of coordinates at this point\n",
    "    # We need to make tables for t1 and t2 that do not include the matched items\n",
    "    t1unique = (t1[~t1['index'].isin(idx1_2)]).reset_index(drop=True)\n",
    "    t2unique = (t2[~t2['index'].isin(idx2_2)]).reset_index(drop=True)\n",
    "    # And then we need a table for the matches items where we ensure they are properly matching (SDSS names should \\\n",
    "    # be the same), and then remove the duplicates, store the relevant info from the second table, and concatenate \\\n",
    "    # this with the primary table\n",
    "    tmatches_2 = pd.concat([(t1.iloc[idx1_2]),(t2.iloc[idx2_2])]).sort_values(by='Name1').reset_index(drop=True)\n",
    "    tunique_2 = pd.concat([t1unique, t2unique]).sort_values(by='Name1').reset_index(drop=True)\n",
    "    #\n",
    "    return tunique_1, tmatches_1, idx1, idx2, tunique_2, tmatches_2, idx1_2, idx2_2\n",
    "\n",
    "\n",
    "# Here I'm rewriting our matching algorithm using the search_around_sky() function\n",
    "# It may not always be the best option, but at least for these double peaked catalogs, I think I'm going to run \n",
    "# with it\n",
    "\n",
    "def match_tables_fib(t1,t2,match_tol):\n",
    "    if 'level_0' in t1.columns:\n",
    "        t1.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t1.reset_index(drop=False, inplace=True)\n",
    "    if 'level_0' in t2.columns:\n",
    "        t2.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t2.reset_index(inplace=True, drop=False)\n",
    "    t1['Table_flag'] = 'Table1'\n",
    "    t2['Table_flag'] = 'Table2'\n",
    "    # First we begin by matching RA1 and Dec1 of t1 to RA1 and Dec1 of t2\n",
    "    c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "    c2 = SkyCoord(ra=t2['RA1_deg']*u.degree, dec=t2['Dec1_deg']*u.degree) # storing coordinates for table 2\n",
    "    # Adding a match tolerance here, with user input for the function\n",
    "    max_sep = match_tol * u.arcsec # The max match tolerance will be 5''\n",
    "    #idx2, d2d2, d3d2 = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "    idx1, idx2, _, _ = c2.search_around_sky(c1, max_sep) \n",
    "    # idx1 and idx2 are the indices in table 1 and table 2 which are the closest matching rows to each other\n",
    "    # Note, we should not need to cross match RA1 vs. RA2, across table because the double peaked sources only have\n",
    "    # a single set of coordinates at this point\n",
    "    # We need to make tables for t1 and t2 that do not include the matched items\n",
    "    t1unique = (t1[~t1['index'].isin(idx1)]).reset_index(drop=True)\n",
    "    t2unique = (t2[~t2['index'].isin(idx2)]).reset_index(drop=True)\n",
    "    # And then we need a table for the matches items where we ensure they are properly matching (SDSS names should \\\n",
    "    # be the same), and then remove the duplicates, store the relevant info from the second table, and concatenate \\\n",
    "    # this with the primary table\n",
    "    tmatches = pd.concat([(t1.iloc[idx1]),(t2.iloc[idx2])]).sort_values(by='Name').reset_index(drop=True)\n",
    "    tunique = pd.concat([t1unique, t2unique]).sort_values(by='Name').reset_index(drop=True)\n",
    "    #\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'Paper(s)'] += \" ; \" + t2['Paper(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'BibCode(s)'] += \" ; \" + t2['BibCode(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'DOI(s)'] += \" ; \" + t2['DOI(s)'][0]\n",
    "    return tunique, tmatches, idx1, idx2\n",
    "\n",
    "\n",
    "def match_tables_fib_2RA(t1,t2,match_tol):\n",
    "    if 'level_0' in t1.columns:\n",
    "        t1.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t1.reset_index(drop=False, inplace=True)\n",
    "    if 'level_0' in t2.columns:\n",
    "        t2.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t2.reset_index(inplace=True, drop=False)\n",
    "    t1['Table_flag'] = 'Table1'\n",
    "    t2['Table_flag'] = 'Table2'\n",
    "    # First we begin by matching RA1 and Dec1 of t1 to RA1 and Dec1 of t2\n",
    "    c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "    c2 = SkyCoord(ra=t2['RA2_deg']*u.degree, dec=t2['Dec2_deg']*u.degree) # storing coordinates for table 2\n",
    "    # Adding a match tolerance here, with user input for the function\n",
    "    max_sep = match_tol * u.arcsec # The max match tolerance will be 5''\n",
    "    #idx2, d2d2, d3d2 = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "    idx1, idx2, _, _ = c2.search_around_sky(c1, max_sep) \n",
    "    # idx1 and idx2 are the indices in table 1 and table 2 which are the closest matching rows to each other\n",
    "    # Note, we should not need to cross match RA1 vs. RA2, across table because the double peaked sources only have\n",
    "    # a single set of coordinates at this point\n",
    "    # We need to make tables for t1 and t2 that do not include the matched items\n",
    "    t1unique = (t1[~t1['index'].isin(idx1)]).reset_index(drop=True)\n",
    "    t2unique = (t2[~t2['index'].isin(idx2)]).reset_index(drop=True)\n",
    "    # And then we need a table for the matches items where we ensure they are properly matching (SDSS names should \\\n",
    "    # be the same), and then remove the duplicates, store the relevant info from the second table, and concatenate \\\n",
    "    # this with the primary table\n",
    "    tmatches = pd.concat([(t1.iloc[idx1]),(t2.iloc[idx2])]).sort_values(by='Name').reset_index(drop=True)\n",
    "    tunique = pd.concat([t1unique, t2unique]).sort_values(by='Name').reset_index(drop=True)\n",
    "    #\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'Paper(s)'] += \" ; \" + t2['Paper(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'BibCode(s)'] += \" ; \" + t2['BibCode(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'DOI(s)'] += \" ; \" + t2['DOI(s)'][0]\n",
    "    return tunique, tmatches, idx1, idx2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e16c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3273f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6decbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inada+2008\n",
    "\n",
    "# J093207.15+072251.3 --> Redshift in first column 1.993 does not match exactly what is shown in the last column\n",
    "# --> 1.994\n",
    "# J100859.55+035104.4 --> redshift in first column 1.746 does not match exactly what is shown in the last column\n",
    "# --> 1.745\n",
    "\n",
    "#for table 2 first\n",
    "\n",
    "# I included all listed pairs, as well as anything that said 'no lensing object' or simply listed a separation\n",
    "\n",
    "inada2008t2 = pd.read_csv('Tables/Inada2008/Inada2008_t2.csv', sep=',')\n",
    "\n",
    "# Excluding the no lens objects because they believe these are still likely lenses\n",
    "#inada2008t2 = inada2008t2[inada2008t2['Comment']!='No lens object']\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "inada2008t2['Name1'] = inada2008t2['iSDSS']\n",
    "inada2008t2['Name2'] = '-99'\n",
    "#inada2008t2['z2'] = -99\n",
    "#inada2008t2['z1_type'] = \"spec\"\n",
    "#inada2008t2['z2_type'] = \"-99\"\n",
    "\n",
    "for index, row in inada2008t2.iterrows():\n",
    "    if row['z1']<0:\n",
    "        inada2008t2.at[index, 'z1'] = -99\n",
    "        inada2008t2.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']<0:\n",
    "        inada2008t2.at[index, 'z2'] = -99\n",
    "        inada2008t2.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(inada2008t2,inada2008t2['Name1'])\n",
    "#name_to_coords2(inada2008t2,inada2008t2['Name2'])\n",
    "\n",
    "inada2008t2['RA1'] = inada2008t2['RA']\n",
    "inada2008t2['Dec1'] = inada2008t2['Dec']\n",
    "\n",
    "## Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = inada2008t2['RA1'], dec = inada2008t2['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2008t2['RA1_deg'] = coordconvert.ra.degree\n",
    "inada2008t2['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "inada2008t2['RA2'] = -99\n",
    "inada2008t2['Dec2'] = -99\n",
    "inada2008t2['RA2_deg'] = -99\n",
    "inada2008t2['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "inada2008t2['Equinox1'] = \"J2000\"\n",
    "inada2008t2['Coordinate_waveband1'] = \"Optical\"\n",
    "inada2008t2['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "inada2008t2['Equinox2'] = '-99'\n",
    "inada2008t2['Coordinate_waveband2'] = '-99'\n",
    "inada2008t2['Coordinate_Source2'] = '-99'\n",
    "\n",
    "inada2008t2['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "inada2008t2['Brightness1'] = -100\n",
    "inada2008t2['Brightness_band1'] = -100\n",
    "inada2008t2['Brightness_type1'] = -100\n",
    "\n",
    "inada2008t2['Brightness2'] = -100\n",
    "inada2008t2['Brightness_band2'] = -100\n",
    "inada2008t2['Brightness_type2'] = -100\n",
    "\n",
    "inada2008t2['Sep'] = inada2008t2['theta_SDSS']\n",
    "\n",
    "#inada2008t2['Sep(kpc)'] = inada2008t2['Sep']*((cosmo.arcsec_per_kpc_proper(inada2008t2['z']))**(-1))\n",
    "\n",
    "inada2008t2['dV'] = -99 #(2.99e+5)*((1+inada2008t2['z1'])**2 - (1+inada2008t2['z2'])**2)/((1+inada2008t2['z1'])**2+(1+inada2008t2['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "for index, row in inada2008t2.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        inada2008t2.at[index, 'dV'] = (2.99e+5)*((1+inada2008t2.at[index,'z1'])**2 - (1+inada2008t2.at[index,'z2'])**2)/((1+inada2008t2.at[index,'z1'])**2+(1+inada2008t2.at[index,'z2'])**2)\n",
    "    else:\n",
    "        inada2008t2.at[index, 'dV'] = -99\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "inada2008t2['Selection Method'] = \"-99\" #DPSELs\n",
    "inada2008t2['Confirmation Method'] = \"-99\"\n",
    "inada2008t2['Paper(s)'] = \"Inada+2008\"\n",
    "inada2008t2['BibCode(s)'] = \"2008AJ....135..496I\"\n",
    "inada2008t2['DOI(s)'] = \"https://doi.org/10.1088/0004-6256/135/2/496\"\n",
    "\n",
    "inada2008t2['Notes'] = '-99'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#inada2008t2.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#inada2008t2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inada+2008\n",
    "\n",
    "# J093207.15+072251.3 --> Redshift in first column 1.993 does not match exactly what is shown in the last column\n",
    "# --> 1.994\n",
    "# J100859.55+035104.4 --> redshift in first column 1.746 does not match exactly what is shown in the last column\n",
    "# --> 1.745\n",
    "\n",
    "# This set of objects is excluded since it presumably makes up a triple image:\n",
    "#SDSSJ161953.24+351321.8,1.901,18.64,,,\n",
    "#SDSSJ161953.45+351323.5,,19.50, 3.13,\n",
    "#No lensing object,SDSSJ161952.82+351315.4,,19.89, 8.20,No lensing object,\n",
    "\n",
    "# All inconclusive cases are included; note they don't use 'inconclusive', that's me using that term here\n",
    "# originally missed one object at the end, but it's not included (30 Sept 2023)\n",
    "\n",
    "inada2008t3 = pd.read_csv('Tables/Inada2008/Inada2008_t3.csv', sep=',')\n",
    "\n",
    "# Excluding the no lens objects because they believe these are still likely lenses\n",
    "#inada2008t3 = inada2008t3[inada2008t3['Comment']!='No lens object']\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "#inada2008t3['z1_type'] = \"spec\"\n",
    "#inada2008t3['z2_type'] = \"spec\"\n",
    "\n",
    "for index, row in inada2008t3.iterrows():\n",
    "    if row['z1']<0:\n",
    "        inada2008t3.at[index, 'z1'] = -99\n",
    "        inada2008t3.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']<0:\n",
    "        inada2008t3.at[index, 'z2'] = -99\n",
    "        inada2008t3.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords1(inada2008t3,inada2008t3['Name1'])\n",
    "name_to_coords2(inada2008t3,inada2008t3['Name2'])\n",
    "\n",
    "## Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = inada2008t3['RA1'], dec = inada2008t3['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2008t3['RA1_deg'] = coordconvert.ra.degree\n",
    "inada2008t3['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "coordconvert = SkyCoord(ra = inada2008t3['RA2'], dec = inada2008t3['Dec2'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2008t3['RA2_deg'] = coordconvert.ra.degree\n",
    "inada2008t3['Dec2_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding details about the coordinates\n",
    "inada2008t3['Equinox1'] = \"J2000\"\n",
    "inada2008t3['Coordinate_waveband1'] = \"Optical\"\n",
    "inada2008t3['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "inada2008t3['Equinox2'] = \"J2000\"\n",
    "inada2008t3['Coordinate_waveband2'] = \"Optical\"\n",
    "inada2008t3['Coordinate_Source2'] = \"SDSS\"\n",
    "\n",
    "inada2008t3['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "inada2008t3['Brightness1'] = -100\n",
    "inada2008t3['Brightness_band1'] = -100\n",
    "inada2008t3['Brightness_type1'] = -100\n",
    "\n",
    "inada2008t3['Brightness2'] = -100\n",
    "inada2008t3['Brightness_band2'] = -100\n",
    "inada2008t3['Brightness_type2'] = -100\n",
    "\n",
    "inada2008t3['Sep'] = inada2008t3['thetaSDSS2']\n",
    "\n",
    "#inada2008t3['Sep(kpc)'] = inada2008t3['Sep']*((cosmo.arcsec_per_kpc_proper(inada2008t3['z']))**(-1))\n",
    "\n",
    "inada2008t3['dV'] = -99 #(2.99e+5)*((1+inada2008t3['z1'])**2 - (1+inada2008t3['z2'])**2)/((1+inada2008t3['z1'])**2+(1+inada2008t3['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "for index, row in inada2008t3.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        inada2008t3.at[index, 'dV'] = (2.99e+5)*((1+inada2008t3.at[index,'z1'])**2 - (1+inada2008t3.at[index,'z2'])**2)/((1+inada2008t3.at[index,'z1'])**2+(1+inada2008t3.at[index,'z2'])**2)\n",
    "    else:\n",
    "        inada2008t3.at[index, 'dV'] = -99\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "inada2008t3['Selection Method'] = \"-99\" #DPSELs\n",
    "inada2008t3['Confirmation Method'] = \"-99\"\n",
    "inada2008t3['Paper(s)'] = \"Inada+2008\"\n",
    "inada2008t3['BibCode(s)'] = \"2008AJ....135..496I\"\n",
    "inada2008t3['DOI(s)'] = \"https://doi.org/10.1088/0004-6256/135/2/496\"\n",
    "\n",
    "inada2008t3['Notes'] = '-99'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#inada2008t3.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#inada2008t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inada2008 = pd.concat([inada2008t2,inada2008t3])\n",
    "\n",
    "#inada2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329134d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d240ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inada+2010\n",
    "#for table 2 first\n",
    "\n",
    "# No inconclusive cases missed here. They don't use 'inconclusive'; but I've included all cases of 'no lens'\n",
    "\n",
    "\n",
    "inada2010t2 = pd.read_csv('Tables/inada2010/inada2010_t2.csv', sep=',')\n",
    "\n",
    "# Excluding the no lens objects because they believe these are still likely lenses\n",
    "#inada2010t2 = inada2010t2[inada2010t2['Comment']!='No lens object']\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "inada2010t2['Name1'] = inada2010t2['Name']\n",
    "inada2010t2['Name2'] = '-99'\n",
    "#inada2010t2['z2'] = -99\n",
    "inada2010t2['z1_type'] = \"spec\"\n",
    "inada2010t2['z2_type'] = \"spec\"\n",
    "\n",
    "name_to_coords(inada2010t2,inada2010t2['Name'])\n",
    "\n",
    "inada2010t2['RA1'] = inada2010t2['RA'] \n",
    "inada2010t2['Dec1'] = inada2010t2['Dec']\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = inada2010t2['RA'], dec = inada2010t2['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2010t2['RA1_deg'] = coordconvert.ra.degree\n",
    "inada2010t2['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "inada2010t2['RA2'] = -99\n",
    "inada2010t2['Dec2'] = -99\n",
    "\n",
    "inada2010t2['RA2_deg'] = -99\n",
    "inada2010t2['Dec2_deg'] = -99\n",
    "\n",
    "\n",
    "# Adding details about the coordinates\n",
    "inada2010t2['Equinox1'] = \"J2000\"\n",
    "inada2010t2['Coordinate_waveband1'] = \"Optical\"\n",
    "inada2010t2['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "inada2010t2['Equinox2'] = '-99'\n",
    "inada2010t2['Coordinate_waveband2'] = '-99'\n",
    "inada2010t2['Coordinate_Source2'] = '-99'\n",
    "\n",
    "inada2010t2['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "inada2010t2['Brightness1'] = -100\n",
    "inada2010t2['Brightness_band1'] = -100\n",
    "inada2010t2['Brightness_type1'] = -100\n",
    "\n",
    "inada2010t2['Brightness2'] = -100\n",
    "inada2010t2['Brightness_band2'] = -100\n",
    "inada2010t2['Brightness_type2'] = -100\n",
    "\n",
    "inada2010t2['Sep'] = inada2010t2['sep_as']\n",
    "\n",
    "#inada2010t2['Sep(kpc)'] = inada2010t2['Sep']*((cosmo.arcsec_per_kpc_proper(inada2010t2['z']))**(-1))\n",
    "\n",
    "inada2010t2['dV'] = -99\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "for index, row in inada2010t2.iterrows():\n",
    "    if row['z1']<0:\n",
    "        inada2010t2.at[index, 'z1'] = -99\n",
    "        inada2010t2.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']<0:\n",
    "        inada2010t2.at[index, 'z2'] = -99\n",
    "        inada2010t2.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "for index, row in inada2010t2.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        inada2010t2.at[index, 'dV'] = (2.99e+5)*((1+inada2010t2.at[index,'z1'])**2 - (1+inada2010t2.at[index,'z2'])**2)/((1+inada2010t2.at[index,'z1'])**2+(1+inada2010t2.at[index,'z2'])**2)\n",
    "    else:\n",
    "        inada2010t2.at[index, 'dV'] = -99\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "inada2010t2['Selection Method'] = \"-99\" #DPSELs\n",
    "inada2010t2['Confirmation Method'] = \"-99\"\n",
    "inada2010t2['Paper(s)'] = \"Inada+2010\"\n",
    "inada2010t2['BibCode(s)'] = \"2010AJ....140..403I\"\n",
    "inada2010t2['DOI(s)'] = \"https://doi.org/10.1088/0004-6256/140/2/403\"\n",
    "\n",
    "inada2010t2['Notes'] = '-99'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#inada2010t2.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#inada2010t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e062d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inada+2010\n",
    "#for table 3\n",
    "\n",
    "# This was nicely formatted already. I removed all obvious projected pairs, but I've left a few pairs that \\\n",
    "# we can remove here, especially the ones that Hennawi already found (those are marked... we probably will just \\\n",
    "# throw those out....)\n",
    "\n",
    "\n",
    "inada2010t3 = pd.read_csv('Tables/inada2010/inada2010_t3.csv', sep=',')\n",
    "\n",
    "# Excluding the no lens objects because they believe these are still likely lenses\n",
    "#inada2010t3 = inada2010t3[inada2010t3['Comment']!='No lens object']\n",
    "#inada2010t3 = inada2010t3[inada2010t3['Comment']!='no lens object']\n",
    "#inada2010t3 = inada2010t3[inada2010t3['z2']>0]\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "inada2010t3['z1_type'] = \"spec\"\n",
    "inada2010t3['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords1(inada2010t3,inada2010t3['Name1'])\n",
    "name_to_coords2(inada2010t3,inada2010t3['Name2'])\n",
    "\n",
    "## Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = inada2010t3['RA1'], dec = inada2010t3['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2010t3['RA1_deg'] = coordconvert.ra.degree\n",
    "inada2010t3['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "coordconvert = SkyCoord(ra = inada2010t3['RA2'], dec = inada2010t3['Dec2'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2010t3['RA2_deg'] = coordconvert.ra.degree\n",
    "inada2010t3['Dec2_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding details about the coordinates\n",
    "inada2010t3['Equinox1'] = \"J2000\"\n",
    "inada2010t3['Coordinate_waveband1'] = \"Optical\"\n",
    "inada2010t3['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "inada2010t3['Equinox2'] = \"J2000\"\n",
    "inada2010t3['Coordinate_waveband2'] = \"Optical\"\n",
    "inada2010t3['Coordinate_Source2'] = \"SDSS\"\n",
    "\n",
    "inada2010t3['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "inada2010t3['Brightness1'] = -100\n",
    "inada2010t3['Brightness_band1'] = -100\n",
    "inada2010t3['Brightness_type1'] = -100\n",
    "\n",
    "inada2010t3['Brightness2'] = -100\n",
    "inada2010t3['Brightness_band2'] = -100\n",
    "inada2010t3['Brightness_type2'] = -100\n",
    "\n",
    "inada2010t3['Sep'] = inada2010t3['Sep_as']\n",
    "\n",
    "#inada2010t3['Sep(kpc)'] = inada2010t3['Sep']*((cosmo.arcsec_per_kpc_proper(inada2010t3['z']))**(-1))\n",
    "\n",
    "inada2010t3['dV'] = (2.99e+5)*((1+inada2010t3['z1'])**2 - (1+inada2010t3['z2'])**2)/((1+inada2010t3['z1'])**2+(1+inada2010t3['z2'])**2)\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "inada2010t3['Selection Method'] = \"-99\" #DPSELs\n",
    "inada2010t3['Confirmation Method'] = \"-99\"\n",
    "inada2010t3['Paper(s)'] = \"Inada+2010\"\n",
    "inada2010t3['BibCode(s)'] = \"2010AJ....140..403I\"\n",
    "inada2010t3['DOI(s)'] = \"https://doi.org/10.1088/0004-6256/140/2/403\"\n",
    "\n",
    "inada2010t3['Notes'] = '-99'\n",
    "\n",
    "# I fucked myself over here... I'm going to have to back and thoroughly rematch again because I wasn't being \\\n",
    "# careful here. Fuck. \n",
    "inada2010t3_forlater = inada2010t3[np.abs(inada2010t3['dV'])>10000]\n",
    "inada2010t3 = inada2010t3[np.abs(inada2010t3['dV'])<10000]\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#inada2010t3.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#inada2010t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are going to save this tabel for later. We will need to match this against the full MAC in the match \\\n",
    "# catalogs notebook. Otherwise it will be too much work to match here, then match alll the way through the other \\\n",
    "# notebook\n",
    "\n",
    "for index, row in inada2010t3_forlater.iterrows():\n",
    "    if row['z1']==0:\n",
    "        inada2010t3_forlater.at[index, 'z1'] = -99\n",
    "        inada2010t3_forlater.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']==0:\n",
    "        inada2010t3_forlater.at[index, 'z2'] = -99\n",
    "        inada2010t3_forlater.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "\n",
    "for index, row in inada2010t3_forlater.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        inada2010t3_forlater.at[index, 'dV'] = (2.99e+5)*((1+inada2010t3_forlater.at[index,'z1'])**2 - (1+inada2010t3_forlater.at[index,'z2'])**2)/((1+inada2010t3_forlater.at[index,'z1'])**2+(1+inada2010t3_forlater.at[index,'z2'])**2)\n",
    "    else:\n",
    "        inada2010t3_forlater.at[index, 'dV'] = -99\n",
    "\n",
    "\n",
    "inada2010t3_forlater.to_csv('inada2010t3_forlatermatching.csv', sep=',', index=False)\n",
    "\n",
    "#inada2010t3_forlater\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inada2010t3_forlater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c42f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "inada2010 = pd.concat([inada2010t2,inada2010t3])\n",
    "\n",
    "#inada2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457e90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a08936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching the Inada 2008 and 2010 tables here\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(inada2008,inada2010,5)\n",
    "\n",
    "#print(len(tmatches)) there are no matches!\n",
    "\n",
    "the_whills = pd.concat([inada2008,inada2010])\n",
    "\n",
    "# verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9aed80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inada+2012\n",
    "\n",
    "# No inconclusive cases to include here. All 'no lens' cases have been included.\n",
    "# I verified that I'm not missing any possible pairs from that table\n",
    "\n",
    "inada2012t3 = pd.read_csv('Tables/Inada2012/inada2012_t3.csv', sep=',')\n",
    "\n",
    "#inada2012t3 = inada2012t3[inada2012t3['Com']!='No lens object']\n",
    "#inada2012t3 = inada2012t3[inada2012t3['Com']!='no lens object']\n",
    "#inada2012t3 = inada2012t3[inada2012t3['Com']!='separation 0.7 arcsec']\n",
    "#inada2012t3 = inada2012t3[inada2012t3['z2']>0]\n",
    "\n",
    "inada2012t3['Name1'] = inada2012t3['Name']\n",
    "inada2012t3['Name2'] = '-99'\n",
    "inada2012t3['z1_type'] = \"spec\"\n",
    "inada2012t3['z2_type'] = \"spec\"\n",
    "\n",
    "for index, row in inada2012t3.iterrows():\n",
    "    if row['z1']<0:\n",
    "        inada2012t3.at[index, 'z1'] = -99\n",
    "        inada2012t3.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']<0:\n",
    "        inada2012t3.at[index, 'z2'] = -99\n",
    "        inada2012t3.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(inada2012t3,inada2012t3['Name'])\n",
    "\n",
    "inada2012t3['RA1'] = inada2012t3['RA']\n",
    "inada2012t3['Dec1'] = inada2012t3['Dec']\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = inada2012t3['RA'], dec = inada2012t3['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2012t3['RA1_deg'] = coordconvert.ra.degree\n",
    "inada2012t3['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "inada2012t3['RA2'] = -99\n",
    "inada2012t3['Dec2'] = -99\n",
    "\n",
    "inada2012t3['RA2_deg'] = -99\n",
    "inada2012t3['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "inada2012t3['Equinox1'] = \"J2000\"\n",
    "inada2012t3['Coordinate_waveband1'] = \"Optical\"\n",
    "inada2012t3['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "inada2012t3['Equinox2'] = '-99'\n",
    "inada2012t3['Coordinate_waveband2'] = '-99'\n",
    "inada2012t3['Coordinate_Source2'] = '-99'\n",
    "\n",
    "inada2012t3['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "inada2012t3['Brightness1'] = -100\n",
    "inada2012t3['Brightness_band1'] = -100\n",
    "inada2012t3['Brightness_type1'] = -100\n",
    "\n",
    "inada2012t3['Brightness2'] = -100\n",
    "inada2012t3['Brightness_band2'] = -100\n",
    "inada2012t3['Brightness_type2'] = -100\n",
    "\n",
    "inada2012t3['Sep'] = inada2012t3['theta']\n",
    "\n",
    "#inada2012t3['Sep(kpc)'] = inada2012t3['Sep']*((cosmo.arcsec_per_kpc_proper(inada2012t3['z']))**(-1))\n",
    "\n",
    "inada2012t3['dV'] = -99\n",
    "for index, row in inada2012t3.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        inada2012t3.at[index, 'dV'] = (2.99e+5)*((1+inada2012t3.at[index,'z1'])**2 - (1+inada2012t3.at[index,'z2'])**2)/((1+inada2012t3.at[index,'z1'])**2+(1+inada2012t3.at[index,'z2'])**2)\n",
    "    else:\n",
    "        inada2012t3.at[index, 'dV'] = -99\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "inada2012t3['Selection Method'] = \"-99\" #DPSELs\n",
    "inada2012t3['Confirmation Method'] = \"-99\"\n",
    "inada2012t3['Paper(s)'] = \"Inada+2012\"\n",
    "inada2012t3['BibCode(s)'] = \"2012AJ....143..119I\"\n",
    "inada2012t3['DOI(s)'] = \"https://doi.org/10.1088/0004-6256/143/5/119\"\n",
    "\n",
    "inada2012t3['Notes'] = '-99'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#inada2012t3.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#inada2012t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4223ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inada+2012\n",
    "\n",
    "# All objects that are not QSO+stars or QSOs+nonQSOs are included, except for this apparent triple image:\n",
    "#SDSSJ162033.71+121121.2,1.629,18.66,SDSSJ162032.65+121112.8,0,18.33,17.90,no lensing object\n",
    "#SDSSJ162033.71+121121.2,1.629,18.66,SDSSJ162033.47+121117.8,0,19.53,5.11,no lensing object\n",
    "\n",
    "inada2012t4 = pd.read_csv('Tables/Inada2012/inada2012_t4.csv', sep=',')\n",
    "\n",
    "#inada2012t4 = inada2012t4[inada2012t4['Com']!='No lens object']\n",
    "#inada2012t4 = inada2012t4[inada2012t4['Com']!='no lens object']\n",
    "#inada2012t4 = inada2012t4[inada2012t4['z2']>0]\n",
    "\n",
    "#inada2012t4['Name1'] = inada2012t4['Name']\n",
    "inada2012t4['z1_type'] = \"spec\"\n",
    "inada2012t4['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords1(inada2012t4,inada2012t4['Name1'])\n",
    "name_to_coords2(inada2012t4,inada2012t4['Name2'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = inada2012t4['RA1'], dec = inada2012t4['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2012t4['RA1_deg'] = coordconvert.ra.degree\n",
    "inada2012t4['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "coordconvert = SkyCoord(ra = inada2012t4['RA2'], dec = inada2012t4['Dec2'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "inada2012t4['RA2_deg'] = coordconvert.ra.degree\n",
    "inada2012t4['Dec2_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding details about the coordinates\n",
    "inada2012t4['Equinox1'] = \"J2000\"\n",
    "inada2012t4['Coordinate_waveband1'] = \"Optical\"\n",
    "inada2012t4['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "inada2012t4['Equinox2'] = \"J2000\"\n",
    "inada2012t4['Coordinate_waveband2'] = \"Optical\"\n",
    "inada2012t4['Coordinate_Source2'] = \"SDSS\"\n",
    "\n",
    "inada2012t4['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "inada2012t4['Brightness1'] = -100\n",
    "inada2012t4['Brightness_band1'] = -100\n",
    "inada2012t4['Brightness_type1'] = -100\n",
    "\n",
    "inada2012t4['Brightness2'] = -100\n",
    "inada2012t4['Brightness_band2'] = -100\n",
    "inada2012t4['Brightness_type2'] = -100\n",
    "\n",
    "inada2012t4['Sep'] = inada2012t4['theta']\n",
    "\n",
    "#inada2012t4['Sep(kpc)'] = inada2012t4['Sep']*((cosmo.arcsec_per_kpc_proper(inada2012t4['z']))**(-1))\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "inada2012t4['Selection Method'] = \"-99\" #DPSELs\n",
    "inada2012t4['Confirmation Method'] = \"-99\"\n",
    "inada2012t4['Paper(s)'] = \"Inada+2012\"\n",
    "inada2012t4['BibCode(s)'] = \"2012AJ....143..119I\"\n",
    "inada2012t4['DOI(s)'] = \"https://doi.org/10.1088/0004-6256/143/5/119\"\n",
    "\n",
    "inada2012t4['Notes'] = '-99'\n",
    "\n",
    "inada2012t4['dV'] = (2.99e+5)*((1+inada2012t4['z1'])**2 - (1+inada2012t4['z2'])**2)/((1+inada2012t4['z1'])**2+(1+inada2012t4['z2'])**2)\n",
    "inada2012t4_forlater = inada2012t4[np.abs(inada2012t4['dV'])>3000]\n",
    "inada2012t4 = inada2012t4[np.abs(inada2012t4['dV'])<3000]\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#inada2012t4.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#inada2012t4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee82f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inada2012t4_forlater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a705bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the 'lost' objects due to the stupid dV cut above. We will need to match this against the MAC in \\\n",
    "# the matching notebook\n",
    "\n",
    "for index, row in inada2012t4_forlater.iterrows():\n",
    "    if row['z1']==0:\n",
    "        inada2012t4_forlater.at[index, 'z1'] = -99\n",
    "        inada2012t4_forlater.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']==0:\n",
    "        inada2012t4_forlater.at[index, 'z2'] = -99\n",
    "        inada2012t4_forlater.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "\n",
    "for index, row in inada2012t4_forlater.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        inada2012t4_forlater.at[index, 'dV'] = (2.99e+5)*((1+inada2012t4_forlater.at[index,'z1'])**2 - (1+inada2012t4_forlater.at[index,'z2'])**2)/((1+inada2012t4_forlater.at[index,'z1'])**2+(1+inada2012t4_forlater.at[index,'z2'])**2)\n",
    "    else:\n",
    "        inada2012t4_forlater.at[index, 'dV'] = -99\n",
    "\n",
    "\n",
    "inada2012t4_forlater.to_csv('inada2012t4_forlatermatching.csv', sep=',', index=False)\n",
    "\n",
    "#inada2012t4_forlater\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inada2012 = pd.concat([inada2012t3,inada2012t4])\n",
    "\n",
    "#inada2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bffb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in Inada 2012 here\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(inada2008,inada2010,5)\n",
    "\n",
    "#print(len(tmatches)) #there are no matches!\n",
    "\n",
    "the_whills = pd.concat([the_whills,inada2012])\n",
    "\n",
    "# verified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef92026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're adding in the information from Rusu+2013\n",
    "# SDSS J132059.17+164402.59 and SDSS J132059.73+164405.6 (SDSS J1320+1644)\n",
    "\n",
    "#rusu2013 = ['SDSSJ132059.17+164402.59','SDSSJ132059.73+164405.6'] # \n",
    "#for index, row in the_whills.iterrows():\n",
    "#    if row['Name1'] in rusu2013:\n",
    "#        the_whills.at[index, 'Paper(s)'] += ' ; Rusu+2013 '\n",
    "#        the_whills.at[index, 'BibCode(s)'] += ' ; 2013ApJ...765..139R' \n",
    "#        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/765/2/139'\n",
    "#        the_whills.at[index, 'Notes']=' Rusu+2013 find this system comprises two QSO images and two galaxies between in a cross-like configuration. Rusu+ argue that this is most likely a lens rather than a QSO pair. All observed differences between the spectra can be attributed to a combination of extinction, microlensing,and intrinsic variability. They cannot indisputably rule out the binary hypothesis however.'\n",
    "#        the_whills.at[index, 'System Type'] = 'Lens / Binary Quasar'\n",
    "\n",
    "# Actually, since this isn't in the tables, I'll add this as an individual object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More+2016 here\n",
    "\n",
    "#Table 1. Summary of spectroscopic observations. The Selection column indicates the method(s) with M for morphological selection, C for colour\n",
    "#selection, and S for spectroscopic selection (see the text for details). Several candidates were selected by multiple methods. Observation indicates the\n",
    "#telescope (S-Subaru or K-Keck), the date and setup of the spectroscopic observations (see also Section 3.1). Result indicates the conclusion from the\n",
    "#follow-up spectroscopy as well as imaging observations described in Section 3.2. The zQSO are taken from the BOSS DR12 catalogue which has redshifts\n",
    "#corrected after visual inspection of the spectra. The last column has redshifts from the follow-up spectra except when marked witha which are taken from the\n",
    "#BOSS spectrum.\n",
    "\n",
    "#Table 7. Summary of confirmed quasar pairs. See Table 1 for the RA and Dec. of the brighter quasar image (A). The\n",
    "#positions and the i-band magnitudes in this table are from the SDSS data base except for J0818+0601 where we used the\n",
    "#astrometrically calibrated SOAR images.\n",
    "\n",
    "# this table includes quasar pairs that have velocity differences less than 2000 km s^-1, pairs that don't have \\\n",
    "# two spec-z's but are listed as having sdifferent SEDs, and two inconclusive pairs\n",
    "\n",
    "\n",
    "more2016 = pd.read_csv('Tables/more2016/more2016.csv', sep=',')\n",
    "\n",
    "# No distinct coordinates are provided. We need to reach out and request them.\n",
    "# Separations provided for only some targets\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "#more2016['Name2'] = more2016['Name']\n",
    "#more2016['z2'] = more2016['z']\n",
    "more2016['z1_type'] = \"spec\"\n",
    "more2016['z2_type'] = \"spec\"\n",
    "\n",
    "for index, row in more2016.iterrows():\n",
    "    if row['z1']<0:\n",
    "        more2016.at[index, 'z1'] = -99\n",
    "        more2016.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']<0:\n",
    "        more2016.at[index, 'z2'] = -99\n",
    "        more2016.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(more2016,more2016['Name'])\n",
    "\n",
    "## Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = more2016['RA1'], dec = more2016['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "more2016['RA1_deg'] = coordconvert.ra.degree\n",
    "more2016['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "## Converting the coordinates\n",
    "# had to use chatgpt to debug this bug here\n",
    "more2016['RA2_deg'] = -99\n",
    "more2016['Dec2_deg'] = -99\n",
    "\n",
    "# Iterate over rows of the DataFrame\n",
    "for idx, row in more2016.iterrows():\n",
    "    try:\n",
    "        coordconvert = SkyCoord(ra=row['RA2'], dec=row['Dec2'], frame='icrs', unit=(u.hourangle, u.deg))\n",
    "        more2016.at[idx, 'RA2_deg'] = coordconvert.ra.degree\n",
    "        more2016.at[idx, 'Dec2_deg'] = coordconvert.dec.degree\n",
    "    except:\n",
    "        # If there's an exception, just set this row's values to '-99'\n",
    "        more2016.at[idx, 'RA2_deg'] = -99\n",
    "        more2016.at[idx, 'Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "more2016['Equinox1'] = \"J2000\"\n",
    "more2016['Coordinate_waveband1'] = \"Optical\"\n",
    "more2016['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "more2016['Equinox2'] = \"J2000\"\n",
    "more2016['Coordinate_waveband2'] = \"Optical\"\n",
    "more2016['Coordinate_Source2'] = \"SDSS\"\n",
    "\n",
    "for index, row in more2016.iterrows():\n",
    "    if row['RA2_deg']<0:\n",
    "        more2016.at[index, 'Equinox2'] = '-99'\n",
    "        more2016.at[index, 'Coordinate_waveband2'] = '-99'\n",
    "        more2016.at[index, 'Coordinate_Source2'] = '-99'\n",
    "\n",
    "more2016['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "more2016['Brightness1'] = -100\n",
    "more2016['Brightness_band1'] = -100\n",
    "more2016['Brightness_type1'] = -100\n",
    "\n",
    "more2016['Brightness2'] = -100\n",
    "more2016['Brightness_band2'] = -100\n",
    "more2016['Brightness_type2'] = -100\n",
    "\n",
    "more2016['Sep'] = more2016['Sep_as']\n",
    "\n",
    "#more2016['Sep(kpc)'] = more2016['Sep']*((cosmo.arcsec_per_kpc_proper(more2016['z']))**(-1))\n",
    "\n",
    "more2016['dV'] = -99\n",
    "for index, row in more2016.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        more2016.at[index, 'dV'] = (2.99e+5)*((1+more2016.at[index,'z1'])**2 - (1+more2016.at[index,'z2'])**2)/((1+more2016.at[index,'z1'])**2+(1+more2016.at[index,'z2'])**2)\n",
    "    else:\n",
    "        more2016.at[index, 'dV'] = -99\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "more2016['Selection Method'] = \"-99\" #DPSELs\n",
    "more2016['Confirmation Method'] = \"-99\"\n",
    "more2016['Paper(s)'] = \"More+2016\"\n",
    "more2016['BibCode(s)'] = \"2016MNRAS.456.1595M\"\n",
    "more2016['DOI(s)'] = \"https://doi.org/10.1093/mnras/stv2813\"\n",
    "more2016['Notes'] = '-99'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#more2016.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#more2016\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in More2016 now...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,more2016,5)\n",
    "\n",
    "#print(len(tmatches)) #there are no matches!\n",
    "\n",
    "the_whills = pd.concat([the_whills,more2016])\n",
    "\n",
    "# verified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54a583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b41286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eftekharzadeh+2017 here\n",
    "# Sandrinelli+2018 has some notes on an object selected in this sample\n",
    "\n",
    "#Table 1. Summary of spectroscopic observations. The Selection column indicates the method(s) with M for morphological selection, C for colour\n",
    "#selection, and S for spectroscopic selection (see the text for details). Several candidates were selected by multiple methods. Observation indicates the\n",
    "#telescope (S-Subaru or K-Keck), the date and setup of the spectroscopic observations (see also Section 3.1). Result indicates the conclusion from the\n",
    "#follow-up spectroscopy as well as imaging observations described in Section 3.2. The zQSO are taken from the BOSS DR12 catalogue which has redshifts\n",
    "#corrected after visual inspection of the spectra. The last column has redshifts from the follow-up spectra except when marked witha which are taken from the\n",
    "#BOSS spectrum.\n",
    "\n",
    "#Table 7. Summary of confirmed quasar pairs. See Table 1 for the RA and Dec. of the brighter quasar image (A). The\n",
    "#positions and the i-band magnitudes in this table are from the SDSS data base except for J0818+0601 where we used the\n",
    "#astrometrically calibrated SOAR images.\n",
    "\n",
    "\n",
    "# I'm including 3 objects flagged as a lens but not classified as such (flag 4 but without a class Q lens)\n",
    "# All quasar pairs (72)\n",
    "# Alll ambiguous cases (flag 1); 45 of these not including any star+quasars\n",
    "\n",
    "eftek2017 = pd.read_csv('Tables/Eftekharzadeh2017/Eftekharzadeh2017.csv', sep=',')\n",
    "\n",
    "eftek2017['z1_type'] = \"spec\"\n",
    "eftek2017['z2_type'] = \"spec\"\n",
    "\n",
    "eftek2017['RA1_deg'] = eftek2017['RAdeg1']\n",
    "eftek2017['Dec1_deg'] = eftek2017['DEdeg1']\n",
    "\n",
    "## Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=eftek2017['RA1_deg']*u.degree, dec=eftek2017['Dec1_deg']*u.degree, frame='icrs')\n",
    "eftek2017['RA1'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "eftek2017['Dec1'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "\n",
    "# Convert RA and Dec to the desired format\n",
    "ra_format = coords.ra.to_string(unit=u.hour, sep='', precision=2, pad=True)\n",
    "dec_format = coords.dec.to_string(unit=u.deg, sep='', precision=2, alwayssign=True, pad=True)\n",
    "\n",
    "# Concatenate to form J_format using a loop\n",
    "eftek2017['Name1'] = [\"J\" + ra + dec for ra, dec in zip(ra_format, dec_format)]\n",
    "\n",
    "# Next object\n",
    "eftek2017['RA2_deg'] = eftek2017['RAdeg2']\n",
    "eftek2017['Dec2_deg'] = eftek2017['DEdeg2']\n",
    "\n",
    "## Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=eftek2017['RA2_deg']*u.degree, dec=eftek2017['Dec2_deg']*u.degree, frame='icrs')\n",
    "eftek2017['RA2'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "eftek2017['Dec2'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "\n",
    "# Convert RA and Dec to the desired format\n",
    "ra_format = coords.ra.to_string(unit=u.hour, sep='', precision=2, pad=True)\n",
    "dec_format = coords.dec.to_string(unit=u.deg, sep='', precision=2, alwayssign=True, pad=True)\n",
    "\n",
    "# Concatenate to form J_format using a loop\n",
    "eftek2017['Name2'] = [\"J\" + ra + dec for ra, dec in zip(ra_format, dec_format)]\n",
    "\n",
    "for index, row in eftek2017.iterrows():\n",
    "    if row['z1']<0:\n",
    "        eftek2017.at[index, 'z1'] = -99\n",
    "        eftek2017.at[index, 'z1_type'] = '-99'\n",
    "    if row['z2']<0:\n",
    "        eftek2017.at[index, 'z2'] = -99\n",
    "        eftek2017.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "# Adding details about the coordinates\n",
    "eftek2017['Equinox1'] = \"J2000\"\n",
    "eftek2017['Coordinate_waveband1'] = \"Optical\"\n",
    "eftek2017['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "eftek2017['Equinox2'] = \"J2000\"\n",
    "eftek2017['Coordinate_waveband2'] = \"Optical\"\n",
    "eftek2017['Coordinate_Source2'] = \"SDSS\"\n",
    "\n",
    "eftek2017['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "eftek2017['Brightness1'] = -100\n",
    "eftek2017['Brightness_band1'] = -100\n",
    "eftek2017['Brightness_type1'] = -100\n",
    "\n",
    "eftek2017['Brightness2'] = -100\n",
    "eftek2017['Brightness_band2'] = -100\n",
    "eftek2017['Brightness_type2'] = -100\n",
    "\n",
    "eftek2017['Sep'] = eftek2017['Sep_as1']\n",
    "\n",
    "#eftek2017['Sep(kpc)'] = eftek2017['Sep']*((cosmo.arcsec_per_kpc_proper(eftek2017['z']))**(-1))\n",
    "\n",
    "eftek2017['dV'] = -99\n",
    "for index, row in eftek2017.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        eftek2017.at[index, 'dV'] = (2.99e+5)*((1+eftek2017.at[index,'z1'])**2 - (1+eftek2017.at[index,'z2'])**2)/((1+eftek2017.at[index,'z1'])**2+(1+eftek2017.at[index,'z2'])**2)\n",
    "    else:\n",
    "        eftek2017.at[index, 'dV'] = -99\n",
    "        \n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "eftek2017['Selection Method'] = \"-99\" #DPSELs\n",
    "eftek2017['Confirmation Method'] = \"-99\"\n",
    "eftek2017['Paper(s)'] = \"Eftekharzadeh+2017\"\n",
    "eftek2017['BibCode(s)'] = \"2017MNRAS.468...77E\"\n",
    "eftek2017['DOI(s)'] = \"https://doi.org/10.1093/mnras/stx412\"\n",
    "eftek2017['Notes'] = '-99'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#eftek2017.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#eftek2017\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0c536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc37aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in eftek2017 now...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,eftek2017,2)\n",
    "\n",
    "#print(len(tmatches)) #there are no matches!\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    #print(\"i:\", i, \"j:\", j)\n",
    "    #print(\"Sep:\", mcgurk2015t4.at[j, 'NIRC2sep(as)'])\n",
    "    #print(\"dV:\", mcgurk2015t4.at[j, 'dV[OIII]'])\n",
    "    the_whills.at[i, 'Sep'] = eftek2017.at[j, 'Sep']\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Eftekharzadeh+2017'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2017MNRAS.468...77E' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stx412'\n",
    "    the_whills.at[i, 'z2'] = eftek2017.at[j, 'z2']\n",
    "    #the_whills.at[i, 'Notes'] += ' Fu+2011 companions within 3 arcseconds,'\n",
    "\n",
    "# dropping the matching indices from the eftek2017 table\n",
    "eftek2017.drop(idx2, axis=0, inplace=True)\n",
    "eftek2017.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#tmatches\n",
    "\n",
    "# now matching the two objects where Inada and eftek. has the objects switched\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib_2RA(the_whills,eftek2017,2)\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Sep'] = eftek2017.at[j, 'Sep']\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Eftekharzadeh+2017'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2017MNRAS.468...77E' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stx412'\n",
    "\n",
    "# dropping the matching indices from the eftek2017 table\n",
    "eftek2017.drop(idx2, axis=0, inplace=True)\n",
    "eftek2017.reset_index(drop=True, inplace=True)\n",
    "\n",
    "the_whills = pd.concat([the_whills,eftek2017])\n",
    "\n",
    "# verified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills = the_whills_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab70fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(eftek2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be89fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunique, tmatches, idx1, idx2 = match_tables_fib_2RA(the_whills,eftek2017,2)\n",
    "#\n",
    "#print(len(tmatches)) #there are no matches!\n",
    "#\n",
    "##the_whills = pd.concat([the_whills,more2016])\n",
    "#\n",
    "## there are 2 matches\n",
    "#\n",
    "#tmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b01995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Agnello+2018 here\n",
    "\n",
    "# S03320021 is listed as a contaminating QSO + QSO but only one redshift is listed.\n",
    "# We need to reach out and ask if this is a projected pair or not\n",
    "\n",
    "agnello2018 = pd.read_csv('Tables/Agnello2018/agnello2018.csv', sep=',')\n",
    "\n",
    "# No distinct coordinates are provided. We need to reach out and request them.\n",
    "# No separations are provided\n",
    "# A0326 overlaps with Schecter. They also claim there is more overlap between Schecter and their NIQs but \n",
    "# I cannot find any other matching objects to support this notion\n",
    "\n",
    "# all uncertain/unclear cases are included here along with their NIQs.\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "agnello2018['Name1'] = agnello2018['Name']\n",
    "agnello2018['Name2'] = '-99'\n",
    "agnello2018['z2'] = -99\n",
    "agnello2018['z1_type'] = \"spec\"\n",
    "agnello2018['z2_type'] = \"-99\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(agnello2018,agnello2018['Name'])\n",
    "\n",
    "## Converting the coordinates\n",
    "#coordconvert = SkyCoord(ra = agnello2018['RA'], dec = agnello2018['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "agnello2018['RA1_deg'] = agnello2018['RA(J2000)']\n",
    "agnello2018['Dec1_deg'] = agnello2018['Dec(J2000)']\n",
    "\n",
    "## Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=agnello2018['RA1_deg']*u.degree, dec=agnello2018['Dec1_deg']*u.degree, frame='icrs')\n",
    "agnello2018['RA1'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "agnello2018['Dec1'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "\n",
    "## Concatenate to form J_format using a loop\n",
    "#agnello2018['Name1'] = [\"J\" + ra + dec for ra, dec in zip(ra_format, dec_format)]\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "agnello2018['RA2'] = -99\n",
    "agnello2018['Dec2'] = -99\n",
    "\n",
    "agnello2018['RA2_deg'] = -99\n",
    "agnello2018['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "agnello2018['Equinox1'] = \"J2000\"\n",
    "agnello2018['Coordinate_waveband1'] = \"Optical\"\n",
    "agnello2018['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "agnello2018['Equinox2'] = \"-99\"\n",
    "agnello2018['Coordinate_waveband2'] = \"-99\"\n",
    "agnello2018['Coordinate_Source2'] = \"-99\"\n",
    "\n",
    "agnello2018['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "agnello2018['Brightness1'] = -100\n",
    "agnello2018['Brightness_band1'] = -100\n",
    "agnello2018['Brightness_type1'] = -100\n",
    "\n",
    "agnello2018['Brightness2'] = -100\n",
    "agnello2018['Brightness_band2'] = -100\n",
    "agnello2018['Brightness_type2'] = -100\n",
    "\n",
    "#agnello2018['Sep'] = 3 # arcseconds\n",
    "\n",
    "#agnello2018['Sep(kpc)'] = agnello2018['Sep']*((cosmo.arcsec_per_kpc_proper(agnello2018['z']))**(-1))\n",
    "\n",
    "agnello2018['dV'] = -99\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "agnello2018['Selection Method'] = \"-99\" #DPSELs\n",
    "agnello2018['Confirmation Method'] = \"-99\"\n",
    "agnello2018['Paper(s)'] = \"Agnello+2018\"\n",
    "agnello2018['BibCode(s)'] = \"2018MNRAS.475.2086A\"\n",
    "agnello2018['DOI(s)'] = \"https://doi.org/10.1093/mnras/stx3226\"\n",
    "\n",
    "agnello2018['Notes'] = 'Nearly identical quasars and no lens galaxy detected. For S0332 need to verify that this is not a dual quasar.'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#agnello2018.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#agnello2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in agnello2018 now...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,agnello2018,20)\n",
    "\n",
    "#print(len(tmatches)) #there are no matches!\n",
    "\n",
    "the_whills = pd.concat([the_whills,agnello2018])\n",
    "\n",
    "# verified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801830e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spiniello+2018\n",
    "# Distinct coordinates are not provided.\n",
    "\n",
    "spiniello2018 = pd.read_csv('Tables/spiniello2018/spiniello2018.csv', sep=',')\n",
    "\n",
    "# No distinct coordinates are provided. We need to reach out and request them.\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "# We include all objects from Table 2 and 3, except for this entry since they are confirmed to be stars:\n",
    "# KIDS0834-0139,08:34:40,-01:39:08,DIA,3.0,Not a lens. TNG Spectroscopy reveals that the two objects are stars.\n",
    "\n",
    "spiniello2018['Name1'] = spiniello2018['Name']\n",
    "spiniello2018['Name2'] = '-99'\n",
    "spiniello2018['z1'] = -99\n",
    "spiniello2018['z2'] = -99\n",
    "spiniello2018['z1_type'] = \"-99\"\n",
    "spiniello2018['z2_type'] = \"-99\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "spiniello2018['RA1'] = spiniello2018['RA']\n",
    "spiniello2018['Dec1'] = spiniello2018['Dec']\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = spiniello2018['RA1'], dec = spiniello2018['Dec1'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "spiniello2018['RA1_deg'] = coordconvert.ra.degree\n",
    "spiniello2018['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "spiniello2018['RA2'] = -99\n",
    "spiniello2018['Dec2'] = -99\n",
    "\n",
    "spiniello2018['RA2_deg'] = -99\n",
    "spiniello2018['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "spiniello2018['Equinox1'] = \"J2000\"\n",
    "spiniello2018['Coordinate_waveband1'] = \"Optical\"\n",
    "spiniello2018['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "spiniello2018['Equinox2'] = \"-99\"\n",
    "spiniello2018['Coordinate_waveband2'] = \"-99\"\n",
    "spiniello2018['Coordinate_Source2'] = \"-99\"\n",
    "\n",
    "spiniello2018['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "spiniello2018['Brightness1'] = -100\n",
    "spiniello2018['Brightness_band1'] = -100\n",
    "spiniello2018['Brightness_type1'] = -100\n",
    "\n",
    "spiniello2018['Brightness2'] = -100\n",
    "spiniello2018['Brightness_band2'] = -100\n",
    "spiniello2018['Brightness_type2'] = -100\n",
    "\n",
    "#spiniello2018['Sep'] = 3 # arcseconds\n",
    "\n",
    "#spiniello2018['Sep(kpc)'] = spiniello2018['Sep']*((cosmo.arcsec_per_kpc_proper(spiniello2018['z']))**(-1))\n",
    "\n",
    "#spiniello2018['dV'] = (2.99e+5)*((1+spiniello2018['z'])**2 - (1+spiniello2018['z2'])**2)/((1+spiniello2018['z'])**2+(1+spiniello2018['z2'])**2)\n",
    "## dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "spiniello2018['Selection Method'] = \"-99\" #DPSELs\n",
    "spiniello2018['Confirmation Method'] = \"-99\"\n",
    "spiniello2018['Paper(s)'] = \"Spiniello+2018\"\n",
    "spiniello2018['BibCode(s)'] = \"2018MNRAS.480.1163S\"\n",
    "spiniello2018['DOI(s)'] = \"https://doi.org/10.1093/mnras/sty1923\"\n",
    "\n",
    "spiniello2018['Notes'] = ' '\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#spiniello2018.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#spiniello2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding in the Spiniello2018 table...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,spiniello2018,20)\n",
    "\n",
    "#print(len(tmatches)) \n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Spiniello+2018'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2018MNRAS.480.1163S' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/sty1923'\n",
    "\n",
    "# dropping the matching indices from the spiniello2018 table\n",
    "spiniello2018.drop(idx2, axis=0, inplace=True)\n",
    "spiniello2018.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "the_whills = pd.concat([the_whills,spiniello2018])\n",
    "\n",
    "#tmatches\n",
    "\n",
    "# verified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261138b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d5b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lemon+2018\n",
    "# I do include a few objects listed a inconclusive, but not the object that is listed as quasar+star? \n",
    "# or the inconclusive pair that is labeled as likely stars\n",
    "# inconclusife pairs from table 2 aqre not ioncluded here\n",
    "\n",
    "# No distinct coordinates are provided by Lemon. We need to reach out and request them.\n",
    "\n",
    "# Here we're loading in the double-peaked emission line galaxy catalog of Wang+2009\n",
    "lemon2018 = pd.read_csv('Tables/Lemon2018/Lemon2018_t2.csv', sep=',')\n",
    "# table1.dat is a modified version of Wang's catalog in which I've added a duplicate row for each target\n",
    "# since all of these are candidate dual AGN systems\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "lemon2018['Name1'] = lemon2018['Name']\n",
    "lemon2018['Name2'] = '-99'\n",
    "lemon2018['z1'] = lemon2018['z']\n",
    "lemon2018['z2'] = lemon2018['z'] # these are NIQs with nearly identical redshifts\n",
    "lemon2018['z1_type'] = \"spec\"\n",
    "lemon2018['z2_type'] = \"spec\"\n",
    "\n",
    "# Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=lemon2018['RA']*u.degree, dec=lemon2018['Dec']*u.degree, frame='icrs')\n",
    "lemon2018['RA1'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "lemon2018['Dec1'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "lemon2018['RA1_deg'] = lemon2018['RA']\n",
    "lemon2018['Dec1_deg'] = lemon2018['Dec']\n",
    "\n",
    "lemon2018['RA2'] = -99\n",
    "lemon2018['Dec2'] = -99\n",
    "\n",
    "lemon2018['RA2_deg'] = -99\n",
    "lemon2018['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "lemon2018['Equinox1'] = \"J2000\"\n",
    "lemon2018['Coordinate_waveband1'] = \"Optical\"\n",
    "lemon2018['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "lemon2018['Equinox2'] = \"-99\"\n",
    "lemon2018['Coordinate_waveband2'] = \"-99\"\n",
    "lemon2018['Coordinate_Source2'] = \"-99\"\n",
    "\n",
    "lemon2018['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "lemon2018['Brightness1'] = -100\n",
    "lemon2018['Brightness_band1'] = -100\n",
    "lemon2018['Brightness_type1'] = -100\n",
    "\n",
    "lemon2018['Brightness2'] = -100\n",
    "lemon2018['Brightness_band2'] = -100\n",
    "lemon2018['Brightness_type2'] = -100\n",
    "\n",
    "lemon2018['Sep'] = lemon2018['Sep_as']\n",
    "\n",
    "#lemon2018['Sep(kpc)'] = lemon2018['Sep']*((cosmo.arcsec_per_kpc_proper(lemon2018['z']))**(-1))\n",
    "\n",
    "#lemon2018['dV'] = (2.99e+5)*((1+lemon2018['z'])**2 - (1+lemon2018['z2'])**2)/((1+lemon2018['z'])**2+(1+lemon2018['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "lemon2018['Selection Method'] = \"-99\" #DPSELs\n",
    "lemon2018['Confirmation Method'] = \"-99\"\n",
    "lemon2018['Paper(s)'] = \"Lemon+2018\"\n",
    "lemon2018['BibCode(s)'] = \"2018MNRAS.479.5060L\"\n",
    "lemon2018['DOI(s)'] = \"https://doi.org/10.1093/mnras/sty911\"\n",
    "\n",
    "lemon2018['Notes'] = 'Nearly identical quasars and no lens galaxy detected.'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#lemon2018.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#lemon2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910aaf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding lemon2018 here\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,lemon2018,20)\n",
    "\n",
    "print(len(tmatches)) # 1 matches\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Lemon+2018'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2018MNRAS.479.5060L' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/sty911'\n",
    "\n",
    "# dropping the matching indices from the lemon2018 table\n",
    "lemon2018.drop(idx2, axis=0, inplace=True)\n",
    "lemon2018.reset_index(drop=True, inplace=True)\n",
    "\n",
    "the_whills = pd.concat([the_whills,lemon2018])\n",
    "\n",
    "#tmatches\n",
    "\n",
    "# verified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8795dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57effc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lemon+2019\n",
    "# They quote 5 NIQs but I count only four in the table. Need to ask about missing one.\n",
    "# I include two onbjects classified simply as 'inconclusive'\n",
    "\n",
    "\n",
    "# The method of selection is shown for each candidate: WD, WT, WQ = ALLWISE double, triple, quad; uW = unWISE model fitting; MQ = Milliquas;\n",
    "\n",
    "# No distinct coordinates are provided by Lemon. We need to reach out and request them.\n",
    "\n",
    "# Here we're loading in the double-peaked emission line galaxy catalog of Wang+2009\n",
    "lemon2019 = pd.read_csv('Tables/Lemon2019/Lemon2019_t4.csv', sep=',')\n",
    "# table1.dat is a modified version of Wang's catalog in which I've added a duplicate row for each target\n",
    "# since all of these are candidate dual AGN systems\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "lemon2019['Name1'] = lemon2019['Name']\n",
    "lemon2019['Name2'] = '-99'\n",
    "lemon2019['z1'] = lemon2019['z']\n",
    "lemon2019['z2'] = lemon2019['z'] # these are NIQs or inconclusives, so we're adopting identical redshifts\n",
    "lemon2019['z1_type'] = \"spec\"\n",
    "lemon2019['z2_type'] = \"spec\"\n",
    "\n",
    "# Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=lemon2019['RA']*u.degree, dec=lemon2019['Dec']*u.degree, frame='icrs')\n",
    "lemon2019['RA1'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "lemon2019['Dec1'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "lemon2019['RA1_deg'] = lemon2019['RA']\n",
    "lemon2019['Dec1_deg'] = lemon2019['Dec']\n",
    "\n",
    "lemon2019['RA2'] = -99\n",
    "lemon2019['Dec2'] = -99\n",
    "\n",
    "lemon2019['RA2_deg'] = -99\n",
    "lemon2019['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "lemon2019['Equinox1'] = \"J2000\"\n",
    "lemon2019['Coordinate_waveband1'] = \"Optical\"\n",
    "lemon2019['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "lemon2019['Equinox2'] = \"-99\"\n",
    "lemon2019['Coordinate_waveband2'] = \"-99\"\n",
    "lemon2019['Coordinate_Source2'] = \"-99\"\n",
    "\n",
    "lemon2019['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "lemon2019['Brightness1'] = -100\n",
    "lemon2019['Brightness_band1'] = -100\n",
    "lemon2019['Brightness_type1'] = -100\n",
    "\n",
    "lemon2019['Brightness2'] = -100\n",
    "lemon2019['Brightness_band2'] = -100\n",
    "lemon2019['Brightness_type2'] = -100\n",
    "\n",
    "lemon2019['Sep'] = -99 # arcseconds\n",
    "\n",
    "#lemon2019['Sep(kpc)'] = lemon2019['Sep']*((cosmo.arcsec_per_kpc_proper(lemon2019['z']))**(-1))\n",
    "\n",
    "#lemon2019['dV'] = (2.99e+5)*((1+lemon2019['z'])**2 - (1+lemon2019['z2'])**2)/((1+lemon2019['z'])**2+(1+lemon2019['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "lemon2019['Selection Method'] = \"-99\" #DPSELs\n",
    "lemon2019['Confirmation Method'] = \"-99\"\n",
    "lemon2019['Paper(s)'] = \"Lemon+2019\"\n",
    "lemon2019['BibCode(s)'] = \"2019MNRAS.483.4242L\"\n",
    "lemon2019['DOI(s)'] = \"https://doi.org/10.1093/mnras/sty3366\"\n",
    "\n",
    "lemon2019['Notes'] = 'Nearly identical quasars and no lens galaxy detected.'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#lemon2019.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "# for J2316+0610, mark redshift type as -99\n",
    "# for J1307+0642, mark redshift type as -99\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1']=='J2316+0610':\n",
    "        the_whills.at[index, 'z1_type'] = '-99'\n",
    "        the_whills.at[index, 'z2_type'] = '-99'\n",
    "    if row['Name1']=='J1307+0642':\n",
    "        the_whills.at[index, 'z1_type'] = '-99'\n",
    "        the_whills.at[index, 'z2_type'] = '-99'\n",
    "\n",
    "#lemon2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding lemon 2019 here...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,lemon2019,20)\n",
    "\n",
    "#print(len(tmatches)) # 1 matches\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Lemon+2019'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2019MNRAS.479.5060L' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/sty911'\n",
    "\n",
    "# dropping the matching indices from the lemon2019 table\n",
    "lemon2019.drop(idx2, axis=0, inplace=True)\n",
    "lemon2019.reset_index(drop=True, inplace=True)\n",
    "\n",
    "the_whills = pd.concat([the_whills,lemon2019])\n",
    "\n",
    "#tmatches\n",
    "\n",
    "# verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffa688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now loading in the tables for Rusu+2019\n",
    "\n",
    "rusu2019t1 = pd.read_csv('Tables/Rusu2019/Rusu2019_t1.csv', sep=',') # this contains the 91 candidates they identified\n",
    "rusu2019t1b = pd.read_csv('Tables/Rusu2019/Rusu2019_t1b.csv', sep=',') # This contains previously confirmed candidate (lens? - unclear in the text) as well as rejected ones\n",
    "\n",
    "# In table 2, confirmation_flag==2 are candidates that have been ruled out becased on GAIA properties or spectroscopy\n",
    "# confirmation_flag==1 are confirmed lenses or binary quasars\n",
    "\n",
    "# Here we are removing those irrelevant rejected candidates:\n",
    "rusu2019t1b = rusu2019t1b[rusu2019t1b['Confirmation_flag']<2]\n",
    "# And now removing things mnually based on objects that were previously discovered \n",
    "# It seems that we only need to include the first entry, which is a pair of SDSS QSOs\n",
    "# It looks like all other entries are accounted for in our bibliography (I'm not going to try to match this \\\n",
    "# against my other tables; there's way too much going on here... maybe I can come back and fix this later)\n",
    "\n",
    "# In their table 1, we're including all of their candidates\n",
    "\n",
    "rusu2019t1['Name1'] = rusu2019t1['Name']\n",
    "rusu2019t1['Name2'] = '-99'\n",
    "rusu2019t1['z1'] = -99\n",
    "rusu2019t1['z2'] = -99\n",
    "rusu2019t1['z1_type'] = \"-99\"\n",
    "rusu2019t1['z2_type'] = \"-99\"\n",
    "\n",
    "# Converting the coordinates\n",
    "rusu2019t1['RA1_deg'] = rusu2019t1['RAdeg']#coordconvert.ra.degree\n",
    "rusu2019t1['Dec1_deg'] = rusu2019t1['DEdeg']#coordconvert.dec.degree\n",
    "#\n",
    "# Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=rusu2019t1['RA1_deg']*u.degree, dec=rusu2019t1['Dec1_deg']*u.degree, frame='icrs')\n",
    "rusu2019t1['RA1'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "rusu2019t1['Dec1'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "\n",
    "rusu2019t1['RA2'] = -99\n",
    "rusu2019t1['Dec2'] = -99\n",
    "\n",
    "rusu2019t1['RA2_deg'] = -99\n",
    "rusu2019t1['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "rusu2019t1['Equinox1'] = \"J2000\"\n",
    "rusu2019t1['Coordinate_waveband1'] = \"-99\"\n",
    "rusu2019t1['Coordinate_Source1'] = \"-99\"\n",
    "\n",
    "rusu2019t1['Equinox2'] = \"-99\"\n",
    "rusu2019t1['Coordinate_waveband2'] = \"-99\"\n",
    "rusu2019t1['Coordinate_Source2'] = \"-99\"\n",
    "\n",
    "rusu2019t1['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "rusu2019t1['Brightness1'] = -100\n",
    "rusu2019t1['Brightness_band1'] = -100\n",
    "rusu2019t1['Brightness_type1'] = -100\n",
    "\n",
    "rusu2019t1['Brightness2'] = -100\n",
    "rusu2019t1['Brightness_band2'] = -100\n",
    "rusu2019t1['Brightness_type2'] = -100\n",
    "\n",
    "rusu2019t1['Sep'] = rusu2019t1['Sep(as)'] # arcseconds\n",
    "\n",
    "#rusu2019t1['Sep(kpc)'] = rusu2019t1['Sep']*((cosmo.arcsec_per_kpc_proper(rusu2019t1['z']))**(-1))\n",
    "\n",
    "#rusu2019t1['dV'] = (2.99e+5)*((1+rusu2019t1['z'])**2 - (1+rusu2019t1['z2'])**2)/((1+rusu2019t1['z'])**2+(1+rusu2019t1['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "rusu2019t1['Selection Method'] = \"-99\" #DPSELs\n",
    "rusu2019t1['Confirmation Method'] = \"-99\"\n",
    "rusu2019t1['Paper(s)'] = \"Rusu+2019\"\n",
    "rusu2019t1['BibCode(s)'] = \"2019MNRAS.486.4987R\"\n",
    "rusu2019t1['DOI(s)'] = \"https://doi.org/10.1093/mnras/stz1142\"\n",
    "\n",
    "rusu2019t1['Notes'] = ' '\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#rusu2019t1.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#rusu2019t1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rusu2019t1b\n",
    "\n",
    "rusu2019t1b['Name1'] = rusu2019t1b['Name']\n",
    "rusu2019t1b['Name2'] = '-99'\n",
    "rusu2019t1b['z1'] = -99\n",
    "rusu2019t1b['z2'] = -99\n",
    "rusu2019t1b['z1_type'] = \"-99\"\n",
    "rusu2019t1b['z2_type'] = \"-99\"\n",
    "\n",
    "# Converting the coordinates\n",
    "#coordconvert = SkyCoord(ra = rusu2019t1b['RA'], dec = rusu2019t1b['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "rusu2019t1b['RA1_deg'] = rusu2019t1b['RAdeg']#coordconvert.ra.degree\n",
    "rusu2019t1b['Dec1_deg'] = rusu2019t1b['DEdeg']#coordconvert.dec.degree\n",
    "\n",
    "# Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=rusu2019t1b['RA1_deg']*u.degree, dec=rusu2019t1b['Dec1_deg']*u.degree, frame='icrs')\n",
    "rusu2019t1b['RA1'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "rusu2019t1b['Dec1'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "\n",
    "rusu2019t1b['RA2'] = -99\n",
    "rusu2019t1b['Dec2'] = -99\n",
    "\n",
    "rusu2019t1b['RA2_deg'] = -99\n",
    "rusu2019t1b['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "rusu2019t1b['Equinox1'] = \"J2000\"\n",
    "rusu2019t1b['Coordinate_waveband1'] = \"-99\"\n",
    "rusu2019t1b['Coordinate_Source1'] = \"-99\"\n",
    "\n",
    "rusu2019t1b['Equinox2'] = \"-99\"\n",
    "rusu2019t1b['Coordinate_waveband2'] = \"-99\"\n",
    "rusu2019t1b['Coordinate_Source2'] = \"-99\"\n",
    "\n",
    "rusu2019t1b['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "rusu2019t1b['Brightness1'] = -100\n",
    "rusu2019t1b['Brightness_band1'] = -100\n",
    "rusu2019t1b['Brightness_type1'] = -100\n",
    "\n",
    "rusu2019t1b['Brightness2'] = -100\n",
    "rusu2019t1b['Brightness_band2'] = -100\n",
    "rusu2019t1b['Brightness_type2'] = -100\n",
    "\n",
    "rusu2019t1b['Sep'] = rusu2019t1b['Sep(as)'] # arcseconds\n",
    "\n",
    "#rusu2019t1b['Sep(kpc)'] = rusu2019t1b['Sep']*((cosmo.arcsec_per_kpc_proper(rusu2019t1b['z']))**(-1))\n",
    "\n",
    "#rusu2019t1b['dV'] = (2.99e+5)*((1+rusu2019t1b['z'])**2 - (1+rusu2019t1b['z2'])**2)/((1+rusu2019t1b['z'])**2+(1+rusu2019t1b['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "rusu2019t1b['Selection Method'] = \"-99\" #DPSELs\n",
    "rusu2019t1b['Confirmation Method'] = \"-99\"\n",
    "rusu2019t1b['Paper(s)'] = \"Rusu+2019\"\n",
    "rusu2019t1b['BibCode(s)'] = \"2019MNRAS.486.4987R\"\n",
    "rusu2019t1b['DOI(s)'] = \"https://doi.org/10.1093/mnras/stz1142\"\n",
    "\n",
    "rusu2019t1b['Notes'] = ' '\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#rusu2019t1b.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "rusu2019t1b = rusu2019t1b.loc[[0]]\n",
    "# HERE I AM ONLY INCLUDING THE SEEMINGLY UNIQUE QSO PAIR NOR IDENTIFIED BY OTHERS BEFORE\n",
    "\n",
    "#rusu2019t1b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "rusu2019 = pd.concat([rusu2019t1,rusu2019t1b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in Rusu2019 now...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,rusu2019,20)\n",
    "\n",
    "print(len(tmatches)) # 4 matches\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Rusu+2019'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2019MNRAS.486.4987R' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stz1142'\n",
    "\n",
    "# manually adding in the separation from Rusu for the object in Spiniello2-18 that's missing one\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1']=='KIDS1217-0256':\n",
    "        the_whills.at[index, 'Sep'] = 1.700\n",
    "\n",
    "# dropping the matching indices from the rusu2019 table\n",
    "rusu2019.drop(idx2, axis=0, inplace=True)\n",
    "rusu2019.reset_index(drop=True, inplace=True)\n",
    "\n",
    "the_whills = pd.concat([the_whills,rusu2019])\n",
    "\n",
    "#tmatches\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a1d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lemon+2020\n",
    "#Quasar pairs. Selection: G1: Gaia 1, G2: Gaia 2, V: variability, C: component fitting. NIQ stands for nearly identical quasar pair.\n",
    "\n",
    "# No distinct coordinates are provided by Lemon. We need to reach out and request them.\n",
    "\n",
    "# there were no inconclusive cases included in their tables this time, so no need to include any\n",
    "\n",
    "# Here we're loading in the double-peaked emission line galaxy catalog of Wang+2009\n",
    "lemon2020 = pd.read_csv('Tables/Lemon2020/Lemon2020_t2.csv', sep=',')\n",
    "# table1.dat is a modified version of Wang's catalog in which I've added a duplicate row for each target\n",
    "# since all of these are candidate dual AGN systems\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "lemon2020['Name1'] = lemon2020['Name']\n",
    "lemon2020['Name2'] = '-99'\n",
    "lemon2020['z1'] = lemon2020['z']\n",
    "lemon2020['z2'] = lemon2020['z'] # these are nearly identical quasars with effectively identical redshifts\n",
    "lemon2020['z1_type'] = \"spec\"\n",
    "lemon2020['z2_type'] = \"spec\"\n",
    "\n",
    "# Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=lemon2020['RA(J2000)']*u.degree, dec=lemon2020['Dec(J2000)']*u.degree, frame='icrs')\n",
    "lemon2020['RA1'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "lemon2020['Dec1'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "lemon2020['RA1_deg'] = lemon2020['RA(J2000)']\n",
    "lemon2020['Dec1_deg'] = lemon2020['Dec(J2000)']\n",
    "\n",
    "lemon2020['RA2'] = -99\n",
    "lemon2020['Dec2'] = -99\n",
    "\n",
    "lemon2020['RA2_deg'] = -99\n",
    "lemon2020['Dec2_deg'] = -99\n",
    "\n",
    "# Adding details about the coordinates\n",
    "lemon2020['Equinox1'] = \"J2000\"\n",
    "lemon2020['Coordinate_waveband1'] = \"Optical\"\n",
    "lemon2020['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "lemon2020['Equinox2'] = \"-99\"\n",
    "lemon2020['Coordinate_waveband2'] = \"-99\"\n",
    "lemon2020['Coordinate_Source2'] = \"-99\"\n",
    "\n",
    "lemon2020['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "lemon2020['Brightness1'] = -100\n",
    "lemon2020['Brightness_band1'] = -100\n",
    "lemon2020['Brightness_type1'] = -100\n",
    "\n",
    "lemon2020['Brightness2'] = -100\n",
    "lemon2020['Brightness_band2'] = -100\n",
    "lemon2020['Brightness_type2'] = -100\n",
    "\n",
    "lemon2020['Sep'] = lemon2020['Sep_as']\n",
    "\n",
    "#lemon2020['Sep(kpc)'] = lemon2020['Sep']*((cosmo.arcsec_per_kpc_proper(lemon2020['z']))**(-1))\n",
    "\n",
    "#lemon2020['dV'] = (2.99e+5)*((1+lemon2020['z'])**2 - (1+lemon2020['z2'])**2)/((1+lemon2020['z'])**2+(1+lemon2020['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "lemon2020['Selection Method'] = \"-99\" #DPSELs\n",
    "lemon2020['Confirmation Method'] = \"-99\"\n",
    "lemon2020['Paper(s)'] = \"Lemon+2020\"\n",
    "lemon2020['BibCode(s)'] = \"2020MNRAS.494.3491L\"\n",
    "lemon2020['DOI(s)'] = \"https://doi.org/10.1093/mnras/staa652\"\n",
    "\n",
    "lemon2020['Notes'] = 'Nearly identical quasars and no lens galaxy detected.'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#lemon2020.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#lemon2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in Lemon 2020 here\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,lemon2020,20)\n",
    "\n",
    "#print(len(tmatches)) # 2 matches\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Lemon+2020'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2020MNRAS.494.3491L' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/staa652'\n",
    "    the_whills.at[i, 'Notes'] += '. Nearly identical quasars and no lens galaxy detected.'\n",
    "\n",
    "# I'm keeping the originally listed separations from eftek2017 and rusu2019\n",
    "\n",
    "## dropping the matching indices from the lemon2020 table\n",
    "lemon2020.drop(idx2, axis=0, inplace=True)\n",
    "lemon2020.reset_index(drop=True, inplace=True)\n",
    "\n",
    "the_whills = pd.concat([the_whills,lemon2020])\n",
    "\n",
    "#tmatches\n",
    "\n",
    "\n",
    "# we may need to come back and add in the redshifts....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're adding in information from Hutsemekers+2020 \n",
    "# This is a follow-up to the object SDSS J081830.46+060138.0 (J0818+0601)\n",
    "\n",
    "hutsemekers2020 = ['SDSS J0818+0601A'] # \n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name1'] in hutsemekers2020:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Hutsemekers+2020 '\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2020A&A...633A.101H' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1051/0004-6361/201936973'\n",
    "        the_whills.at[index, 'Notes']='Hutsemekers+ find in their spectropolarimetric obs that the BAL profiles are nearly identical between the two spectra. This strongly suppoprts the notion of a lens rather than a binary given how unlikely it is that to find two similar absorbers in the two AGNs. Microlensing appears to show two sources of continuum. An intervening absorption line system is observed around z=1 in both images but with different intensities. This absorption line system hints at the presence of a lens galaxy.'\n",
    "        the_whills.at[index, 'System Type'] = 'Binary Quasar Candidate'\n",
    "\n",
    "# we'll need to come back and use a flag of -0.5 or -1 for this target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b9259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Anguita+2018\n",
    "# Distinct coordinates are not provided.\n",
    "\n",
    "anguita2018 = pd.read_csv('Tables/Anguita2018/Anguita2018_t2.csv', sep=',')\n",
    "\n",
    "# No distinct coordinates are provided. We need to reach out and request them.\n",
    "# Verify no overlap with Schecter+2017\n",
    "\n",
    "# 0120 was also independently identified by Ostrovski+ but I can't find a paper by Ostrovski that examines this object\n",
    "# 2141 may have been looked at by Hennawi+\n",
    "\n",
    "# 0120 and 2141 show evidence in their spectra of having distinct quasar pairs \n",
    "\n",
    "# We've included their NIQs, the 'other' inconclusive cases, as well as one case of awhat they refer to as a \\\n",
    "# projected pair that might be physically associated\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "anguita2018['Name2'] = '-99'\n",
    "#anguita2018['z2'] = '-99'\n",
    "anguita2018['z1_type'] = \"spec\"\n",
    "anguita2018['z2_type'] = \"-99\"\n",
    "\n",
    "for index, row in inada2012t3.iterrows():\n",
    "    if (row['z2']==0):\n",
    "        inada2012t3.at[index, 'z2'] = -99\n",
    "\n",
    "## Converting the coordinates\n",
    "#coordconvert = SkyCoord(ra = anguita2018['RA'], dec = anguita2018['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "anguita2018['RA1'] = '-99'\n",
    "anguita2018['Dec1'] = '-99'\n",
    "anguita2018['RA1_deg'] = '-99'\n",
    "anguita2018['Dec1_deg'] = '-99'\n",
    "#\n",
    "## Adding in a second set of coordinates for the 'secondary'\n",
    "anguita2018['RA2'] = '-99'\n",
    "anguita2018['Dec2'] = '-99'\n",
    "\n",
    "anguita2018['RA2_deg'] = '-99'\n",
    "anguita2018['Dec2_deg'] = '-99'\n",
    "\n",
    "# Adding details about the coordinates\n",
    "anguita2018['Equinox1'] = \"J2000\"\n",
    "anguita2018['Coordinate_waveband1'] = \"Optical\"\n",
    "anguita2018['Coordinate_Source1'] = \"SDSS\"\n",
    "\n",
    "anguita2018['Equinox2'] = \"J2000\"\n",
    "anguita2018['Coordinate_waveband2'] = \"Optical\"\n",
    "anguita2018['Coordinate_Source2'] = \"SDSS\"\n",
    "\n",
    "anguita2018['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "anguita2018['Brightness1'] = -100\n",
    "anguita2018['Brightness_band1'] = -100\n",
    "anguita2018['Brightness_type1'] = -100\n",
    "\n",
    "anguita2018['Brightness2'] = -100\n",
    "anguita2018['Brightness_band2'] = -100\n",
    "anguita2018['Brightness_type2'] = -100\n",
    "\n",
    "anguita2018['Sep'] = anguita2018['Sep_as']\n",
    "\n",
    "#anguita2018['Sep(kpc)'] = anguita2018['Sep']*((cosmo.arcsec_per_kpc_proper(anguita2018['z']))**(-1))\n",
    "\n",
    "inada2012t3['dV'] = -99\n",
    "for index, row in inada2012t3.iterrows():\n",
    "    if (row['z1']!=-99) & (row['z2']!=-99):\n",
    "        inada2012t3.at[index, 'dV'] = (2.99e+5)*((1+inada2012t3.at[index,'z1'])**2 - (1+inada2012t3.at[index,'z2'])**2)/((1+inada2012t3.at[index,'z1'])**2+(1+inada2012t3.at[index,'z2'])**2)\n",
    "    else:\n",
    "        inada2012t3.at[index, 'dV'] = -99\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "anguita2018['Selection Method'] = \"-99\" #DPSELs\n",
    "anguita2018['Confirmation Method'] = \"-99\"\n",
    "anguita2018['Paper(s)'] = \"Anguita+2018\"\n",
    "anguita2018['BibCode(s)'] = \"2018MNRAS.480.5017A\"\n",
    "anguita2018['DOI(s)'] = \"https://doi.org/10.1093/mnras/sty2172\"\n",
    "\n",
    "anguita2018['Notes'] = 'Nearly identical quasars and no lens galaxy detected. .'\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "#anguita2018.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "#              axis=1, inplace=True)\n",
    "\n",
    "#anguita2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in the anguita2018 table now...\n",
    "\n",
    "# we can do a straight concatenation because there are no coordinates for us to match on\n",
    "\n",
    "the_whills = pd.concat([the_whills,anguita2018])\n",
    "\n",
    "# verified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a41f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing NaN values\n",
    "\n",
    "the_whills.fillna('-99', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52777c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to format the final table and remove columns we don't need\n",
    "\n",
    "the_whills.drop(labels=['level_0','index','iSDSS','i_cor','theta_SDSS','theta_iband','Comment','Ref','imag1',\\\n",
    "                        'thetaSDSS1','Comment1','imag2','thetaSDSS2','Comment2','reference','Table_flag','Name',\\\n",
    "                        'i_band_psfmag(extcorr)','sep_as','i_band_psfmag(extcorr)1','i_band_psfmag(extcorr)2',\\\n",
    "                        'Sep_as','imag','theta','Com','Selection','Result','iA','iB','Sep_as1','Obs_stat1',\\\n",
    "                        'RAdeg1','DEdeg1','gmag1','Class1','QQ?','Sep_as2','Obs_stat2','RAdeg2','DEdeg2',\\\n",
    "                        'gmag2','Class2','RA(J2000)','Dec(J2000)','mag_i','class','Methods','Grade','z',\\\n",
    "                        'Gaia_G1','Gaia_G2','RAdeg','DEdeg','Num_comp','i_mag','Sep(as)','Rank','Gmag1',\\\n",
    "                        'Gmag2','Gmag3','Confirmation_flag','g'], axis=1, inplace=True)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb69b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we're finally saving the table\n",
    "\n",
    "the_whills.to_csv('Duals_from_lens_searches.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee691d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c88c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
