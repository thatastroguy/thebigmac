{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Washington Multiple AGN (WMAGN) Catalog DR1 - Double Peaked SDSS/Optical Sources\n",
    "# Author: R. W. Pfeifle\n",
    "# Original Date Created: 10 Sept. 2020\n",
    "# Last Revision Date: 2 Oct. 2020\n",
    "\n",
    "# New Form Creation Date: 13 January 2023\n",
    "# Last Revision: 3 October 2023\n",
    "\n",
    "# Purpose: Combine various catalogs of double-peaked optically selected dual AGN candidates\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in packages for pandas, astropy, etc. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.io import ascii\n",
    "from astropy.table import Column, MaskedColumn\n",
    "from astropy.io.ascii import masked\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.cosmology import LambdaCDM \n",
    "from astroquery.simbad import Simbad\n",
    "from astroquery.sdss import SDSS\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "import os \n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.3, Ode0=0.7) #Creating our choice of cosmology here...\n",
    "\n",
    "pd.set_option('display.max_columns', 300) # Setting max number of rows per df to be the size of the df\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_coords(df,dfcol):\n",
    "    if (len(dfcol[0])) == 14:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=1) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=6) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=6, stop=13) # Stripping the RA parts\n",
    "        df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=6) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=8) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #print(dfcol.apply(len))\n",
    "    elif (len(dfcol[0])) == 19:\n",
    "        df['Coordinates'] = dfcol.str.slice(start=1) # Stripping the J\n",
    "        df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "        df['Dec_test'] = df['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "        df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "        df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "        df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "        return\n",
    "    #elif dfcol.apply(len) ==\n",
    "    else:\n",
    "        print('Error Encountered')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm rewriting our matching algorithm using the search_around_sky() function\n",
    "# It may not always be the best option, but at least for these double peaked catalogs, I think I'm going to run \n",
    "# with it\n",
    "\n",
    "def match_tables_fib(t1,t2,match_tol):\n",
    "    if 'level_0' in t1.columns:\n",
    "        t1.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t1.reset_index(drop=False, inplace=True)\n",
    "    if 'level_0' in t2.columns:\n",
    "        t2.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "    t2.reset_index(inplace=True, drop=False)\n",
    "    t1['Table_flag'] = 'Table1'\n",
    "    t2['Table_flag'] = 'Table2'\n",
    "    # First we begin by matching RA1 and Dec1 of t1 to RA1 and Dec1 of t2\n",
    "    c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "    c2 = SkyCoord(ra=t2['RA1_deg']*u.degree, dec=t2['Dec1_deg']*u.degree) # storing coordinates for table 2\n",
    "    # Adding a match tolerance here, with user input for the function\n",
    "    max_sep = match_tol * u.arcsec # The max match tolerance will be 5''\n",
    "    #idx2, d2d2, d3d2 = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "    idx1, idx2, _, _ = c2.search_around_sky(c1, max_sep) \n",
    "    # idx1 and idx2 are the indices in table 1 and table 2 which are the closest matching rows to each other\n",
    "    # Note, we should not need to cross match RA1 vs. RA2, across table because the double peaked sources only have\n",
    "    # a single set of coordinates at this point\n",
    "    # We need to make tables for t1 and t2 that do not include the matched items\n",
    "    t1unique = (t1[~t1['index'].isin(idx1)]).reset_index(drop=True)\n",
    "    t2unique = (t2[~t2['index'].isin(idx2)]).reset_index(drop=True)\n",
    "    # And then we need a table for the matches items where we ensure they are properly matching (SDSS names should \\\n",
    "    # be the same), and then remove the duplicates, store the relevant info from the second table, and concatenate \\\n",
    "    # this with the primary table\n",
    "    tmatches = pd.concat([(t1.iloc[idx1]),(t2.iloc[idx2])]).sort_values(by='Name').reset_index(drop=True)\n",
    "    tunique = pd.concat([t1unique, t2unique]).sort_values(by='Name').reset_index(drop=True)\n",
    "    #\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'Paper(s)'] += \" ; \" + t2['Paper(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'BibCode(s)'] += \" ; \" + t2['BibCode(s)'][0]\n",
    "    #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'DOI(s)'] += \" ; \" + t2['DOI(s)'][0]\n",
    "    return tunique, tmatches, idx1, idx2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunique\n",
    "\n",
    "#J013555.82+143529.7\n",
    "#J101143.92+325943.5\n",
    "#J111054.90+012936.0\n",
    "#J123524.95+060810.7\n",
    "#J131515.94+213403.6\n",
    "#J150053.92+382349.6\n",
    "#J155205.93+043317.5\n",
    "#J225420.99-005134.1\n",
    "#J230442.82-093345.3\n",
    "#J231051.95-090011.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're loading in the double-peaked emission line galaxy catalog of Wang+2009\n",
    "wang2009 = ((Table.read('Tables/Wang2009/table1.dat', readme = 'Tables/Wang2009/ReadMe', format='ascii.cds')).to_pandas()).drop(columns=['---'])\n",
    "# table1.dat is a modified version of Wang's catalog in which I've added a duplicate row for each target\n",
    "# since all of these are candidate dual AGN systems\n",
    "\n",
    "# Since the format of the catalog will require names for both components, we'll add in a column 'Name2' which for \\\n",
    "# these and similar targets will be duplicates of the first 'Name column'. Same goes for z2, etc. \n",
    "\n",
    "wang2009['Name2'] = wang2009['Name']\n",
    "wang2009['z2'] = wang2009['z']\n",
    "wang2009['z1_type'] = \"spec\"\n",
    "wang2009['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(wang2009,wang2009['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = wang2009['RA'], dec = wang2009['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "wang2009['RA1_deg'] = coordconvert.ra.degree\n",
    "wang2009['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "wang2009['RA2'] = wang2009['RA']\n",
    "wang2009['Dec2'] = wang2009['Dec']\n",
    "\n",
    "wang2009['RA2_deg'] = wang2009['RA1_deg']\n",
    "wang2009['Dec2_deg'] = wang2009['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "wang2009['Equinox'] = \"J2000\"\n",
    "wang2009['Coordinate_waveband'] = \"Optical\"\n",
    "wang2009['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "wang2009['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "wang2009['Brightness1'] = -100\n",
    "wang2009['Brightness_band1'] = -100\n",
    "wang2009['Brightness_type1'] = -100\n",
    "\n",
    "wang2009['Brightness2'] = -100\n",
    "wang2009['Brightness_band2'] = -100\n",
    "wang2009['Brightness_type2'] = -100\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "wang2009['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#wang2009['Sep(kpc)'] = wang2009['Sep']*((cosmo.arcsec_per_kpc_proper(wang2009['z']))**(-1))\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#wang2009['delta_z'] = wang2009['z']-wang2009['z2']\n",
    "wang2009['dV'] = (2.99e+5)*((1+wang2009['z'])**2 - (1+wang2009['z2'])**2)/((1+wang2009['z'])**2+(1+wang2009['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "wang2009['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "wang2009['Confirmation Method'] = \"-99\"\n",
    "wang2009['Paper(s)'] = \"Wang+2009\"\n",
    "wang2009['BibCode(s)'] = \"2009ApJ...705L..76W\"\n",
    "wang2009['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/705/1/L76\"\n",
    "\n",
    "wang2009['Notes'] = ''\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "wang2009.drop(labels=['e_z','Del1','e_Del1','Del2','e_Del2','FOIII1','e_FOIII1','FOIII2','e_FOIII2','D'],\\\n",
    "              axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wang2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the catalog from Liu+2010a now...\n",
    "liu2010 = ((Table.read('Tables/XLiu2010a/table1.dat', readme = 'Tables/XLiu2010a/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---'])\n",
    "# Note here that any cells containing '--' in Xin's table are going to be replaced by 0's\n",
    "\n",
    "liu2010['Name'] = liu2010['SDSS']\n",
    "liu2010['Name2'] = liu2010['SDSS']\n",
    "liu2010['z2'] = liu2010['z']\n",
    "liu2010['z1_type'] = \"spec\"\n",
    "liu2010['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(liu2010,liu2010['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = liu2010['RA'], dec = liu2010['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "liu2010['RA1_deg'] = coordconvert.ra.degree\n",
    "liu2010['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "liu2010['RA2'] = liu2010['RA']\n",
    "liu2010['Dec2'] = liu2010['Dec']\n",
    "\n",
    "liu2010['RA2_deg'] = liu2010['RA1_deg']\n",
    "liu2010['Dec2_deg'] = liu2010['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "liu2010['Equinox'] = \"J2000\"\n",
    "liu2010['Coordinate_waveband'] = \"Optical\"\n",
    "liu2010['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "liu2010['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "liu2010['Brightness1'] = -100\n",
    "liu2010['Brightness_band1'] = -100\n",
    "liu2010['Brightness_type1'] = -100\n",
    "\n",
    "liu2010['Brightness2'] = -100\n",
    "liu2010['Brightness_band2'] = -100\n",
    "liu2010['Brightness_type2'] = -100\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "liu2010['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "\n",
    "#liu2010['Sep(kpc)'] = liu2010['Sep']*((cosmo.arcsec_per_kpc_proper(liu2010['z']))**(-1))\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#liu2010['delta_z'] = liu2010['z']-liu2010['z2']\n",
    "liu2010['dV'] = (2.99e+5)*((1+liu2010['z'])**2 - (1+liu2010['z2'])**2)/((1+liu2010['z'])**2+(1+liu2010['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "liu2010['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "liu2010['Confirmation Method'] = \"-99\"\n",
    "liu2010['Paper(s)'] = \"Liu+2010a\"\n",
    "liu2010['BibCode(s)'] = \"2010ApJ...708..427L\"\n",
    "liu2010['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/708/1/427\"\n",
    "\n",
    "liu2010['Notes'] = ''\n",
    "# Here we're making manual adjustments based on the findings of Liu+2010b:\n",
    "\n",
    "\n",
    "# And dropping any columns that we don't need....\n",
    "liu2010.drop(labels=['SDSS','Plate','Fiber','MJD','f_SDSS','sigma','FWHM1','FWHM2','VOIII1','VOIII2','VHb1','VHb2'],\\\n",
    "              axis=1, inplace=True)\n",
    "\n",
    "#liu2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, so for whatever reason, there are 9 objects that do match across the Wang+2009 and Liu+2010 catalogs, \\\n",
    "# but astropy cannot match them together. Annoyingly, it's faster to just manually adjust the tables here after \\\n",
    "# the matching process, so that's what I'm doing below. \n",
    "\n",
    "# I'll be manually checking every matching process to ensure proper matches have been made and we are not \\\n",
    "# including duplicates\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(wang2009,liu2010,5)\n",
    "\n",
    "# These are the 9 in the tunique table that will need adjustment. I guess so long as I just remove the Wang+ \\\n",
    "# entries, we'll be fine. \n",
    "objs_liu=['J013555.82+143529.7','J101143.92+325943.5','J111054.90+012936.0','J123524.95+060810.7',\\\n",
    "      'J131515.94+213403.6','J150053.92+382349.6','J155205.93+043317.5','J225420.99-005134.1',\\\n",
    "      'J230442.82-093345.3','J231051.95-090011.9']\n",
    "objs_wang=['J013555+143529','J101143+325943','J111054+012936','J123524+060810',\\\n",
    "      'J131515+213403','J150053+382349','J155205+043317','J225420-005134',\\\n",
    "      'J230442-093345','J231051-090011']\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Table_flag']=='Table2':\n",
    "        tmatches.at[index, 'Paper(s)'] = 'Wang+2009 ; ' + tmatches.at[index, 'Paper(s)']\n",
    "        tmatches.at[index, 'BibCode(s)'] = '2009ApJ...705L..76W ; ' + tmatches.at[index, 'BibCode(s)']\n",
    "        tmatches.at[index, 'DOI(s)'] = 'https://doi.org/10.1088/0004-637X/705/1/L76 ; ' + tmatches.at[index, 'DOI(s)']\n",
    "\n",
    "# Now clipping out all Wang+2009 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Table_flag']!='Table1'].reset_index(drop=True)\n",
    "# Adding the Wang+2009 information to the Liu+2010 rows in the unique table that are actually matches\n",
    "for index, row in tunique.iterrows():\n",
    "    if row['Name'] in objs_liu:\n",
    "        tunique.at[index, 'Paper(s)'] = 'Wang+2009 ; ' + tunique.at[index, 'Paper(s)']\n",
    "        tunique.at[index, 'BibCode(s)'] = '2009ApJ...705L..76W ; ' + tunique.at[index, 'BibCode(s)']\n",
    "        tunique.at[index, 'DOI(s)'] = 'https://doi.org/10.1088/0004-637X/705/1/L76 ; ' + tunique.at[index, 'DOI(s)']\n",
    "# Clipping out the Wang+2009 rows from the unique table that are actually matches \n",
    "tunique = tunique[~tunique['Name'].isin(objs_wang)].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "the_whills.drop(labels=['index'], axis=1, inplace=True) #'level_0'\n",
    "\n",
    "#the_whills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i,j in zip(idx1,idx2):\n",
    "#    print(i,j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alright, so for whatever reason, there are 9 objects that do match across the Wang+2009 and Liu+2010 catalogs, \\\n",
    "## but astropy cannot match them together. Annoyingly, it's faster to just manually adjust the tables here after \\\n",
    "## the matching process, so that's what I'm doing below. \n",
    "#\n",
    "## I'll be manually checking every matching process to ensure proper matches have been made and we are not \\\n",
    "## including duplicates\n",
    "#\n",
    "#tunique, tmatches, idx1, idx2 = match_tables_fib(wang2009,liu2010,5)\n",
    "#\n",
    "## These are the 9 in the tunique table that will need adjustment. I guess so long as I just remove the Wang+ \\\n",
    "## entries, we'll be fine. \n",
    "#objs_liu=['J013555.82+143529.7','J101143.92+325943.5','J111054.90+012936.0','J123524.95+060810.7',\\\n",
    "#      'J131515.94+213403.6','J150053.92+382349.6','J155205.93+043317.5','J225420.99-005134.1',\\\n",
    "#      'J230442.82-093345.3','J231051.95-090011.9']\n",
    "#objs_wang=['J013555+143529','J101143+325943','J111054+012936','J123524+060810',\\\n",
    "#      'J131515+213403','J150053+382349','J155205+043317','J225420-005134',\\\n",
    "#      'J230442-093345','J231051-090011']\n",
    "#\n",
    "## Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "#for index, row in tmatches.iterrows():\n",
    "#    if row['Name']=='Liu+2010':\n",
    "#        tmatches.at[index, 'Paper(s)'] = 'Wang+2009 ; ' + tmatches.at[index, 'Paper(s)']\n",
    "#        tmatches.at[index, 'BibCode(s)'] = '2009ApJ...705L..76W ; ' + tmatches.at[index, 'BibCode(s)']\n",
    "#        tmatches.at[index, 'DOI(s)'] = 'https://doi.org/10.1088/0004-637X/705/1/L76 ; ' + tmatches.at[index, 'DOI(s)']\n",
    "#\n",
    "## Now clipping out all Wang+2009 rows from the matches table\n",
    "#tmatches = tmatches[tmatches['Paper(s)']!='Wang+2009'].reset_index(drop=True)\n",
    "## Adding the Wang+2009 information to the Liu+2010 rows in the unique table that are actually matches\n",
    "#for index, row in tunique.iterrows():\n",
    "#    if row['Name'] in objs_liu:\n",
    "#        tunique.at[index, 'Paper(s)'] = 'Wang+2009 ; ' + tunique.at[index, 'Paper(s)']\n",
    "#        tunique.at[index, 'BibCode(s)'] = '2009ApJ...705L..76W ; ' + tunique.at[index, 'BibCode(s)']\n",
    "#        tunique.at[index, 'DOI(s)'] = 'https://doi.org/10.1088/0004-637X/705/1/L76 ; ' + tunique.at[index, 'DOI(s)']\n",
    "## Clipping out the Wang+2009 rows from the unique table that are actually matches \n",
    "#tunique = tunique[~tunique['Name'].isin(objs_wang)].reset_index(drop=True)\n",
    "#\n",
    "## Concatenating everything together to generate a master table here\n",
    "#the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "#the_whills.drop(labels=['index'], axis=1, inplace=True) #'level_0'\n",
    "#\n",
    "##the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading in the catalog from Smith+2010 now...\n",
    "smith2010 = ((Table.read('Tables/Smith2010/table1.dat', readme = 'Tables/Smith2010/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---'])\n",
    "# Note here that any cells containing '--' in Xin's table are going to be replaced by 0's\n",
    "\n",
    "smith2010['Name'] = smith2010['SDSS']\n",
    "smith2010['Name2'] = smith2010['SDSS']\n",
    "smith2010['z2'] = smith2010['z']\n",
    "smith2010['z1_type'] = \"spec\"\n",
    "smith2010['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(smith2010,smith2010['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = smith2010['RA'], dec = smith2010['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "smith2010['RA1_deg'] = coordconvert.ra.degree\n",
    "smith2010['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "smith2010['RA2'] = smith2010['RA']\n",
    "smith2010['Dec2'] = smith2010['Dec']\n",
    "\n",
    "smith2010['RA2_deg'] = smith2010['RA1_deg']\n",
    "smith2010['Dec2_deg'] = smith2010['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "smith2010['Equinox'] = \"J2000\"\n",
    "smith2010['Coordinate_waveband'] = \"Optical\"\n",
    "smith2010['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "smith2010['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "smith2010['Brightness1'] = -100\n",
    "smith2010['Brightness_band1'] = -100\n",
    "smith2010['Brightness_type1'] = -100\n",
    "\n",
    "smith2010['Brightness2'] = -100\n",
    "smith2010['Brightness_band2'] = -100\n",
    "smith2010['Brightness_type2'] = -100\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "smith2010['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#smith2010['Sep(kpc)'] = smith2010['Sep']*((cosmo.arcsec_per_kpc_proper(smith2010['z']))**(-1))\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#smith2010['delta_z'] = smith2010['z']-smith2010['z2']\n",
    "smith2010['dV'] = smith2010['Vel']\n",
    "# smith+2010 included velocity measurements\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "smith2010['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "smith2010['Confirmation Method'] = \"-99\"\n",
    "smith2010['Paper(s)'] = \"Smith+2010\"\n",
    "smith2010['BibCode(s)'] = \"2010ApJ...716..866S\"\n",
    "smith2010['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/716/1/866\"\n",
    "smith2010['Notes'] = ''\n",
    "\n",
    "#smith2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang+2016 examined a subset of the Smith+ objects, looking for evidence of statistically smaller \\\n",
    "# virial black hole masses in the binary candidates compared to single AGNs, but find larger masses instead.\n",
    "# They disfavor these as binary candidates, but they do not list coordinates or designations.  They only lsit\n",
    "# plate and fiber IDs. So I cannot match onto these unless I manually check for the IDs. It did not add much \\ \n",
    "# so I am not adding it in for now.\n",
    "\n",
    "#for index, row in thewhills.iterrows():\n",
    "#    if row['Name'] in objs:\n",
    "#        the_whills.at[index, 'Paper(s)'] += ' ; Zhang+2016'\n",
    "#        the_whills.at[index, 'BibCode(s)'] += ' ; 2016MNRAS.457.3878Z' \n",
    "#        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stw210'\n",
    "#        #the_whills.at[index, 'Notes']='AGNs confirmed via X-rays and reanalyses of optical spectroscopy.'\n",
    "#        #the_whills.at[index, 'Confirmation Method'] = 'X-ray Imaging / X-ray Spectroscopy / Optical Spectroscopy'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're matching the_whills against the Smith+2010 catalog\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,smith2010,5)\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Smith+2010 rows here in the matches table...\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Table_flag']!='Table2':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Smith+2010'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2010ApJ...716..866S' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/716/1/866'\n",
    "#\n",
    "##!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "## We need to add in some commands that take the velocity column from the Smith+ rows and writes them into the \\\n",
    "## final row containing all relevant information\n",
    "##!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "\n",
    "# Now clipping out all Smith+2010 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Paper(s)']!='Smith+2010'].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "the_whills.drop(labels=['index','Table_flag'], axis=1, inplace=True) #'level_0',\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(the_whills['Name'][88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(the_whills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add in Smith+2012 here...\n",
    "\n",
    "# Wait, we didn't defer to Smith+2010 for names, so tomorrow just use the RA and Dec matching commands\n",
    "ep_objs = ['J101241.20+215556.0','J113105.07+610405.1','J131018.47+250329.5','J144105.64+180507.9',\\\n",
    "           'J151518.29+551535.3','J153231.80+420342.7','J082857.99+074255.7','J123605.45-014119.1',\\\n",
    "           'J124928.36+353926.8','J133226.34+060627.3','J134415.75+331719.1','J144157.24+094859.1',\\\n",
    "           'J171544.02+600835.4','J081542.53+063522.9','J090615.92+121845.6','J091649.41+000031.5',\\\n",
    "           'J120343.22+283557.8','J121911.16+042905.9','J124813.82+362423.6',\\\n",
    "           'J130724.08+460400.9','J133455.24+612042.1','J145110.04+490813.5','J145408.36+240521.3',\\\n",
    "           'J153423.19+540809.0','J084049.46+272704.7',\\\n",
    "           'J120526.04+321314.6','J140500.14+073014.1',\\\n",
    "           'J140816.02+015528.3','J151842.95+244026.0','J210449.13-000919.1']\n",
    "\n",
    "smith2012 = pd.DataFrame(data=ep_objs,columns=['SDSS'])\n",
    "name_to_coords(smith2012,smith2012['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = smith2012['RA'], dec = smith2012['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "smith2012['RA1_deg'] = coordconvert.ra.degree\n",
    "smith2012['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,smith2012,5)\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Smith+2010 rows here in the matches table...\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Table_flag']!='Table2':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Smith+2012'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2012ApJ...752...63S' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/752/1/63'\n",
    "\n",
    "# Now clipping out all Smith+2010 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "the_whills.drop(labels=['index','Table_flag'], axis=1, inplace=True) #'level_0',\n",
    "\n",
    "#  Verified: There are 30 matching objects\n",
    "the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(the_whills['Name'].str[:7].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "print([item for item, count in collections.Counter(the_whills['Name'].str[:7].to_list()).items() if count > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = the_whills\n",
    "#test = test[(test['Paper(s)']!='Liu+2010a') & (test['Paper(s)']!='Wang+2009') & (test['Paper(s)']!='Smith+2010') ]\n",
    "#\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the duplicte entries that for some reason do not have matches. 10 matches; removing these, we will \\\n",
    "# recovder the correct number of 340 souble peaked objects across these three catalogs\n",
    "#'J085416+502631' and 'J085416.76+502632.0' --> Wang+ and Liu+\n",
    "#'J085841+104122' and 'J085841.76+104122.1' --> Wang+ and Liu+\n",
    "#'J112659+294442' and 'J112659.54+294442.8' --> Wang+ and Liu+\n",
    "#'J161006+210735' and 'J161006.42+210735.1' --> Wang+ and Liu+\n",
    "#\n",
    "#'J080218' and 'J080218.65+304622.7' --> Wang+ and Liu+\n",
    "#'J082107' and 'J082107.89+502115.8' --> Wang+ and Liu+\n",
    "#'J094427' and 'J094427.59+144717.1' --> Wang+ and Liu+\n",
    "#'J110821+591851' and 'J110821.81+591852.0' --> Wang+ and Liu+\n",
    "#'J111042' and 'J111042.36+030033.8' --> Wang+ and Liu+\n",
    "#'J132547' and 'J132547.95+545019.5' --> Wang+ and Liu+ \n",
    "\n",
    "\n",
    "# Here we are manually updating the bib information and defering to Liu+ since these non-matches are an issue \\\n",
    "# between Liu+ and Wang+\n",
    "\n",
    "# First, we're dropping the rows from Wang+\n",
    "\n",
    "\n",
    "# Next, we're updating the doi information...\n",
    "\n",
    "\n",
    "# These are the 9 in the tunique table that will need adjustment. I guess so long as I just remove the Wang+ \\\n",
    "# entries, we'll be fine. \n",
    "\n",
    "print(len(the_whills))\n",
    "\n",
    "duplist = ['J080218.65+304622.7','J082107.89+502115.8','J094427.59+144717.1','J110821.81+591852.0',\\\n",
    "           'J111042.36+030033.8','J132547.95+545019.5','J085416.76+502632.0','J085841.76+104122.1',\\\n",
    "           'J112659.54+294442.8','J161006.42+210735.1']\n",
    "droplist = ['J085416+502631','J085841+104122','J112659+294442','J161006+210735','J080218+304622',\\\n",
    "            'J082107+502115','J094427+144717','J110821+591851','J111042+030033','J132547+545019']\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in duplist:\n",
    "        the_whills.at[index, 'Paper(s)'] = 'Wang+2009 ; ' + the_whills.at[index, 'Paper(s)']\n",
    "        the_whills.at[index, 'BibCode(s)'] = '2009ApJ...705L..76W ; ' + the_whills.at[index, 'BibCode(s)']\n",
    "        the_whills.at[index, 'DOI(s)'] = 'https://doi.org/10.1088/0004-637X/705/1/L76 ; ' + the_whills.at[index, 'DOI(s)']\n",
    "\n",
    "# Now clipping out all Wang+2009 rows from the matches table\n",
    "the_whills = the_whills[~the_whills['Name'].isin(droplist)].reset_index(drop=True)\n",
    "\n",
    "print(len(the_whills))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills['Name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in Tingay & Wayth+2011\n",
    "\n",
    "# As listed in Tingay+ table 1\n",
    "#objs = ['J000249+004504','J095833-005118','J105653+331945','J110957+020138','J115249+190300',\\\n",
    "#        'J150452+321414','J151659+051751','J151709+335324','J152606+414014','J155619+094855',\\\n",
    "#        'J160024+264035']\n",
    "\n",
    "# Made this list by manually checking my matched list because the Liu+ namings overwrite the Wang+ names\n",
    "\n",
    "objs =  ['J000249.07+004504.8','J095833.20-005118.6','J105653+331945','J110957.14+020138.6','J115249.33+190300.3',\\\n",
    "         'J150452+321414','J151659.24+051751.5','J151709.21+335324.7','J152606+414014','J155619.30+094855.6',\\\n",
    "         'J160024+264035']\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Tingay+2011'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2011AJ....141..174T' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-6256/141/6/174'\n",
    "        the_whills.at[index, 'Notes'] += ' Tingay+ find no evidence of double radio cores in VLBA imaging.'\n",
    "\n",
    "# All of these have been manually adjusted and do match with the table.\n",
    "# The weird issue with J000249 appears to have been solved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're adding in the information from Rosario's works... \n",
    "\n",
    "# First is J151709.21+335324.7, which Rosario+2010 notes as J151709.20+335324.7\n",
    "# This object is in both the smith and liu catalogs\n",
    "\n",
    "objs =  ['J151709.21+335324.7']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Rosario+2010'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2010ApJ...716..131R' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/716/1/131'\n",
    "        the_whills.at[index, 'Notes'] += ' Rosario+ find that the radio jet and the emission-line regions are coaligned and the emission line spectrum is consistent with ionization by strong shocks. The profiles show a mirrior symmetry consistent with acceleration due to a bipolar outflow. There is also no evidence for a merger. The double-peak profiles are most likely due to jet-ISM interactions and not dual AGNs.'\n",
    "\n",
    "# Now for the second object that they discussed in the appendix: J112939.77+605742.5, which they refer to as\n",
    "# J112939.78+605742.6\n",
    "\n",
    "objs =  ['J112939.77+605742.5']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Rosario+2010'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2010ApJ...716..131R' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/716/1/131'\n",
    "        the_whills.at[index, 'Notes'] += ' Rosario+ find that a compact triple radio structure where the central source is cospatial with the galaxy nucleus. They identify this as the core of the jet. to jet-ISM interactions and not dual AGNs. The size of the radio source is smaller than in J1517; due to the compactness they cannot unambiguously associate the emission line gas and the jets but the double peaked emission lines in this system are again likely to be due to jet-ISM interactions.'\n",
    "\n",
    "\n",
    "# Naming was manually adjusted to match that used in the OG double-peaked catalogs\n",
    "# I have verified that the matching works\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills['Paper(s)'][280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now for Rosario+2011 which looked at near-IR AO Keck imaging for 12 double peaked AGNs\n",
    "\n",
    "objs = ['J153231.80+420342.7','J091649.41+000031.5','J161027.41+130806.8','J081542.53+063522.9',\\\n",
    "        'J095207.62+255257.2','J130724.08+460400.9','J020011.52-093126.1','J140923.51-012430.5',\\\n",
    "        'J124859.72-025730.7','J154107.81+203608.8','J121911.16+042905.9','J072554.42+374436.9']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True', row['Name'])\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Rosario+2011'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2011ApJ...739...44R' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/739/1/44'\n",
    "        #the_whills.at[index, 'Notes'] += ' '\n",
    "\n",
    "# Doubles here\n",
    "the_whills.loc[the_whills['Name']=='J161027.41+130806.8', 'Sep'] = 2.35\n",
    "the_whills.loc[the_whills['Name']=='J095207.62+255257.2', 'Sep'] = 1.00\n",
    "the_whills.loc[the_whills['Name']=='J130724.08+460400.9', 'Sep'] = 2.37\n",
    "the_whills.loc[the_whills['Name']=='J020011.52-093126.1', 'Sep'] = 1.17\n",
    "the_whills.loc[the_whills['Name']=='J124859.72-025730.7', 'Sep'] = 0.53\n",
    "the_whills.loc[the_whills['Name']=='J154107.81+203608.8', 'Sep'] = 2.00\n",
    "\n",
    "the_whills.loc[the_whills['Name']=='J161027.41+130806.8', 'Notes'] += ' Double structure observed in Keck AO near-IR imaging.'\n",
    "the_whills.loc[the_whills['Name']=='J095207.62+255257.2', 'Notes'] += ' Double structure observed in Keck AO near-IR imaging. Rosario+ consider it a merger.'\n",
    "the_whills.loc[the_whills['Name']=='J130724.08+460400.9', 'Notes'] += ' Double structure observed in Keck AO near-IR imaging. Rosario+ consider it a merger.'\n",
    "the_whills.loc[the_whills['Name']=='J020011.52-093126.1', 'Notes'] += ' Double structure observed in Keck AO near-IR imaging. Rosario+ consider it a merger. Outflow may be responsible for observed line profiles given the low level of ionization in both peaks (suggesting they both come from a common ionized region).'\n",
    "the_whills.loc[the_whills['Name']=='J124859.72-025730.7', 'Notes'] += ' Double structure observed in Keck AO near-IR imaging. Rosario+ consider it a merger.'\n",
    "the_whills.loc[the_whills['Name']=='J154107.81+203608.8', 'Notes'] += ' Double structure observed in Keck AO near-IR imaging.'\n",
    "\n",
    "the_whills.loc[the_whills['Name']=='J153231.80+420342.7', 'Notes'] += ' Single structure observed in Keck AO near-IR imaging.'\n",
    "the_whills.loc[the_whills['Name']=='J091649.41+000031.5', 'Notes'] += ' Single structure observed in Keck AO near-IR imaging.'\n",
    "the_whills.loc[the_whills['Name']=='J081542.53+063522.9', 'Notes'] += ' Single structure observed in Keck AO near-IR imaging.'\n",
    "the_whills.loc[the_whills['Name']=='J140923.51-012430.5', 'Notes'] += ' Single structure observed in Keck AO near-IR imaging.'\n",
    "the_whills.loc[the_whills['Name']=='J121911.16+042905.9', 'Notes'] += ' Single structure observed in Keck AO near-IR imaging.'\n",
    "the_whills.loc[the_whills['Name']=='J072554.42+374436.9', 'Notes'] += ' Single structure observed in Keck AO near-IR imaging.'\n",
    "\n",
    "\n",
    "# Verified that the matching process works here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in Shen+2011 information now...\n",
    "\n",
    "shen2011 = pd.read_csv('Tables/Shen2011/Shen2011.csv', sep=',')\n",
    "\n",
    "print(len(shen2011['Name'].to_list()))\n",
    "\n",
    "# objs = ['J153231.80+420342.7','J091649.41+000031.5','J161027.41+130806.8','J081542.53+063522.9','J095207.62+255257.2','J130724.08+460400.9','J020011.52−093126.1','J140923.51−012430.5','J124859.72−025730.7','J154107.81+203608.8','J121911.16+042905.9','J072554.42+374436.9']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in shen2011['Name'].to_list():\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Shen+2011'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2011ApJ...735...48S' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/735/1/48'\n",
    "        the_whills.at[index, 'Notes'] += ' '\n",
    "\n",
    "# Iterate through and add notes for objects resolved and not resolved into two nuclei in near-IR imaging\n",
    "# Also include angular/spectral separations\n",
    "\n",
    "for index, row in the_whills.iterrows():\n",
    "    # Check if the 'Name' exists in shen2011 dataframe\n",
    "    if row['Name'] in shen2011['Name'].to_list():\n",
    "        # Find the corresponding value from shen2011 dataframe\n",
    "        if shen2011.loc[shen2011['Name'] == row['Name'], 'Category'].iloc[0]=='NLR kinematics':\n",
    "            # Add to the notes in the_whills dataframe\n",
    "            the_whills.at[index, 'Notes'] += ' Shen+2011 attribute the double-peaked lines to NLR kinematics. Only a single nucleus is resolved in near-IR imaging and the spatial offset observed between the two velocity components of the emission lines is larger than >0.6''. If the double-peaks were due to two NLRs from dual AGNs there should be two nuclei visible in the imaging.'\n",
    "            the_whills.at[index, 'System Type'] = 'Likely Single AGN'\n",
    "        elif shen2011.loc[shen2011['Name'] == row['Name'], 'Category'].iloc[0]=='Ambiguous':\n",
    "            # Add to the notes in the_whills dataframe\n",
    "            the_whills.at[index, 'Notes'] += ' Shen+2011 find only a single resolved nucleus in near-IR imaging but the velocity separation of the emission lines are spatially offset by <0.4'' which is below the resolution limit of the near-IR imaging. These could be single AGNs or closer dual AGNs. Angular separation is derived from the separation observed between the velocity separated lines in the slit spectrum. '\n",
    "            the_whills.at[index, 'Sep'] = shen2011.loc[shen2011['Name'] == row['Name'], 'Spec_offset_as'].iloc[0]\n",
    "        elif shen2011.loc[shen2011['Name'] == row['Name'], 'Category'].iloc[0]=='Binary AGN':\n",
    "            # Add to the notes in the_whills dataframe\n",
    "            the_whills.at[index, 'Notes'] += ' Shen+2011 identify two nuclei in near-IR imaging that coincide with the two velocity components of the narrow emission lines. Shen+ classify these as binary AGNs.'\n",
    "            the_whills.at[index, 'Sep'] = shen2011.loc[shen2011['Name'] == row['Name'], 'PANIC_offset_as'].iloc[0]\n",
    "        else:\n",
    "            Print('Encountered a problem!')\n",
    "            \n",
    "# Also note in the 'Notes' that the separation is given by \"the two velocity components of the narrow line emission,\n",
    "# measured from the emission peaks of the two velocity components in the slit spectrum\" as per Shen+2011\n",
    "\n",
    "\n",
    "# But I have verified that the matching process works properly\n",
    "\n",
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills.to_csv('the_whills_beforege2012.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ge2012 = ((Table.read('Tables/Ge2012/table3.dat', readme = 'Tables/Ge2012/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---'])\n",
    "\n",
    "# There seems to be minimal matches across the pp-AGN table and the dual core tables. I'm going to skip matching \\\n",
    "# these tables together since it won't add much info at all.\n",
    "######################\n",
    "#ge2012t6 = ((Table.read('Ge2012/table6.dat', readme = 'Ge2012/ReadMe', format='ascii.cds')).to_pandas())\n",
    "#ge2012t7 = ((Table.read('Ge2012/table7.dat', readme = 'Ge2012/ReadMe', format='ascii.cds')).to_pandas())\n",
    "#ge = ge2012t5.set_index('SDSS')\n",
    "#ge = ge.join(ge2012t6.set_index('SDSS'), rsuffix = 2)\n",
    "#ge = ge.join(ge2012t7.set_index('SDSS'), rsuffix = 3)\n",
    "######################\n",
    "\n",
    "# Here we're choosing only the Type Is and Type IIs from Ge+2012\n",
    "ge2012 = ge2012[(ge2012['T']==1) | (ge2012['T']==2)]\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(ge2012,ge2012['SDSS'])\n",
    "\n",
    "ge2012['Name'] = ge2012['SDSS']\n",
    "ge2012['Name2'] = ge2012['SDSS']\n",
    "ge2012['z1'] = ge2012['z']\n",
    "ge2012['z2'] = ge2012['z']\n",
    "ge2012['z1_type'] = \"spec\"\n",
    "ge2012['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(ge2012,ge2012['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = ge2012['RA'], dec = ge2012['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "ge2012['RA1_deg'] = coordconvert.ra.degree\n",
    "ge2012['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "ge2012['RA2'] = ge2012['RA']\n",
    "ge2012['Dec2'] = ge2012['Dec']\n",
    "\n",
    "ge2012['RA2_deg'] = ge2012['RA1_deg']\n",
    "ge2012['Dec2_deg'] = ge2012['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "ge2012['Equinox'] = \"J2000\"\n",
    "ge2012['Coordinate_waveband'] = \"Optical\"\n",
    "ge2012['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "ge2012['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "ge2012['Brightness1'] = ge2012['rmag']\n",
    "ge2012['Brightness_band1'] = \"SDSS r-band\"\n",
    "ge2012['Brightness_type1'] = \"mag\"\n",
    "\n",
    "ge2012['Brightness2'] = ge2012['rmag']\n",
    "ge2012['Brightness_band2'] = \"SDSS r-band\"\n",
    "ge2012['Brightness_type2'] = \"mag\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "ge2012['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#ge2012['Sep(kpc)'] = ge2012['Sep']*((cosmo.arcsec_per_kpc_proper(ge2012['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#ge2012['delta_z'] = ge2012['z']-ge2012['z2']\n",
    "ge2012['dV'] = (2.99e+5)*((1+ge2012['z'])**2 - (1+ge2012['z2'])**2)/((1+ge2012['z'])**2+(1+ge2012['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "ge2012['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "ge2012['Confirmation Method'] = \"-99\"\n",
    "ge2012['Paper(s)'] = \"Ge+2012\"\n",
    "ge2012['BibCode(s)'] = \"2012ApJS..201...31G\"\n",
    "ge2012['DOI(s)'] = \"https://doi:10.1088/0067-0049/201/2/31\"\n",
    "ge2012['Notes'] = ''\n",
    "\n",
    "# Drop the rows\n",
    "ge2012.drop_duplicates(subset='Name', keep='last', inplace=True)\n",
    "\n",
    "rows_to_drop = ge2012[(ge2012['Name'] == 'J153753.64-005720.7') | (ge2012['Name'] == 'J142438.01-010547.3')].index\n",
    "ge2012.drop(rows_to_drop, inplace=True)\n",
    "ge2012.reset_index(drop='True', inplace=True)\n",
    "\n",
    "\n",
    "#ge2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ge2012['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we're matching the_whills against the Ge+2012 catalog\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,ge2012,18)\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Paper(s)']!='Ge+2012':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Ge+2012'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2012ApJS..201...31G' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi:10.1088/0067-0049/201/2/31'\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "# We need to add in some commands that take the velocity column from the Ge+ rows and writes them into the \\\n",
    "# final row containing all relevant information\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "\n",
    "# Now clipping out all Smith+2010 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Paper(s)']!='Ge+2012'].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "the_whills.drop(labels=['index'], axis=1, inplace=True)\n",
    "\n",
    "#the_whills\n",
    "\n",
    "# These are the duplicate rows when adding/matching Ge+ to the_whills\n",
    "# I'm not sure why, but these objects are listed twice in Ge+'s table\n",
    "\n",
    "# Match found: J110053.07+053017.0 and J110053.07+053017.0\n",
    "\n",
    "# Match found: J142438.01-010547.2 and J142438.01-010547.3\n",
    "\n",
    "# Match found: J153753.63-005720.6 and J153753.64-005720.7\n",
    "\n",
    "# duplicates have been removed\n",
    "# this matching process is verified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(names):\n",
    "    name_dict = {}\n",
    "    matches = []\n",
    "\n",
    "    for name in names:\n",
    "        first_6_chars = name[:7]\n",
    "        if first_6_chars in name_dict:\n",
    "            matches.append((name_dict[first_6_chars], name))\n",
    "        else:\n",
    "            name_dict[first_6_chars] = name\n",
    "\n",
    "    return matches\n",
    "\n",
    "names = the_whills['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here now that we're loaded in the double peaked catalogs, we'll add in some individual targets\n",
    "\n",
    "# First, Liu+2010b\n",
    "liu2010objs = ['J110851.04+065901.4','J113126.08-020459.2','J114642.47+511029.6','J133226.34+060627.4']\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in liu2010objs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Liu+2010b'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2010ApJ...715L..30L' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/2041-8205/715/1/L30'\n",
    "        the_whills.at[index, 'Notes']='NIR nuclei and double OIII emitting regions are spatially coincident. Angular separations quoted are the separations between the OIII components.'\n",
    "        the_whills.at[index, 'Confirmation Method'] = 'NIR Imaging / Optical Spectroscopy'\n",
    "\n",
    "the_whills.loc[the_whills['Name']=='J110851.04+065901.4', 'Sep'] = 0.9\n",
    "the_whills.loc[the_whills['Name']=='J113126.08-020459.2', 'Sep'] = 0.6\n",
    "the_whills.loc[the_whills['Name']=='J114642.47+511029.6', 'Sep'] = 2.5\n",
    "the_whills.loc[the_whills['Name']=='J133226.34+060627.4', 'Sep'] = 1.6\n",
    "\n",
    "the_whills.loc[the_whills['Name']=='J110851.04+065901.4', 'System Type'] = 'Dual AGN'\n",
    "the_whills.loc[the_whills['Name']=='J114642.47+511029.6', 'System Type'] = 'Dual AGN'\n",
    "\n",
    "#the_whills\n",
    "\n",
    "\n",
    "# Verified that this matching process works properly.\n",
    "# Although I've just noticed that Smith+2012 shows up twice... odd. Maybe I matched twice by accident.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(the_whills['Paper(s)'][223])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adding in formation from Nandi+2012 and Nandi+2017 for the misalgined/doube-peaked DDRG J1328\n",
    "\n",
    "nandiobjs = ['J132848.46+275227.8']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in nandiobjs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Nandi+2012 ; Nandi+2017'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2012BASI...40..121N ; 2017MNRAS.467L..56N' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.48550/arXiv.1208.1941 ; https://doi.org/10.1093/mnrasl/slw256'\n",
    "        the_whills.at[index, 'Notes'] += 'Nandi+12 flagged this as a misaligned DDRG and Nandi+2017 re-examined this in light of the double-peaked emission lines. They consider it a candidate binary. Inner structure is centered on optical host but misaligned with outer  structure by about 30 deg. Fot every double-peaked line they find different intensities for each component. The different component sshow similar BPT classifications/positions.  They argue this is evidence against  a rotating disc of jet-cloud interaction. The outer double shows steeper spectral indices than the inner double indicating two epochs of jet activity. '\n",
    "        the_whills.loc[the_whills['Name']=='J132848.46+275227.8', 'System Type'] = 'Dual AGN / Binary AGN'\n",
    "\n",
    "\n",
    "# Verified that this matching process works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is for Fu+2011\n",
    "\n",
    "fu2011d = pd.read_csv('Tables/Fu2011a/fu2011_doubles.csv', sep=',')\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = fu2011d['RA'], dec = fu2011d['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "fu2011d['RA1_deg'] = coordconvert.ra.degree\n",
    "fu2011d['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "#fu2011d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will add a matching code that matches based on RA and DEC instead of by name because Fu+ did not \\\n",
    "# adopt the same naming convention as any previous work\n",
    "\n",
    "# Here we're matching for the pairs in Fu+2011\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,fu2011d,3)\n",
    "\n",
    "#len(tmatches)\n",
    "\n",
    "# This finds 16 matches, which is exactly what we'd expect (there are 16 doubles in the table from Fu+2011)\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    #print(\"i:\", i, \"j:\", j)\n",
    "    #print(\"Sep:\", mcgurk2015t4.at[j, 'NIRC2sep(as)'])\n",
    "    #print(\"dV:\", mcgurk2015t4.at[j, 'dV[OIII]'])\n",
    "    the_whills.at[i, 'Sep'] = fu2011d.at[j, 'Sep_as']\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Fu+2011'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2011ApJ...733..103F' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/733/2/103'\n",
    "    the_whills.at[i, 'dV'] = fu2011d.at[j, 'dV']\n",
    "    the_whills.at[i, 'Notes'] += ' Fu+2011 companions within 3 arcseconds,'\n",
    "\n",
    "# verified that this matching process works properly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is for Fu+2011\n",
    "\n",
    "fu2011s = pd.read_csv('Tables/Fu2011a/fu2011_singles.csv', sep=',')\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = fu2011s['RA'], dec = fu2011s['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "fu2011s['RA1_deg'] = coordconvert.ra.degree\n",
    "fu2011s['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "\n",
    "#fu2011s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will add a matching code that matches based on RA and DEC instead of by name because Fu+ did not \\\n",
    "# adopt the same naming convention as any previous work\n",
    "\n",
    "# Here we're matching for the singles in Fu+2011\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,fu2011s,12)\n",
    "\n",
    "# There are 34 objects in Fu+2011 singles table, so the tmatches table should end up with 68 objects total\n",
    "# We end up only with 34 when matching with like 5''. When we match by 12'', we do recover the missing two objects\n",
    "\n",
    "# We are good to proceed, as I have visually verified that these matches are correct\n",
    "\n",
    "#len(tmatches)\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    #print(\"i:\", i, \"j:\", j)\n",
    "    #print(\"Sep:\", mcgurk2015t4.at[j, 'NIRC2sep(as)'])\n",
    "    #print(\"dV:\", mcgurk2015t4.at[j, 'dV[OIII]'])\n",
    "    #the_whills.at[i, 'Sep'] = fu2011s.at[j, 'NIRC2sep(as)']\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Fu+2011'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2011ApJ...733..103F' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/733/2/103'\n",
    "    the_whills.at[i, 'dV'] = fu2011s.at[j, 'dV']\n",
    "    the_whills.at[i, 'Notes'] += ' Fu+2011 found no companions within 3 arcseconds,'\n",
    "\n",
    "# verified that this matching process works properly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're loading in the tables from Fu+2012:\n",
    "\n",
    "fu2012t2 = pd.read_csv('Tables/Fu2012/Fu2012_t2.csv', sep=',')\n",
    "# Table 3 includes objects that have a companion within 3'' of the SDSS fiber position\n",
    "fu2012t3 = pd.read_csv('Tables/Fu2012/Fu2012_t3.csv', sep=',')\n",
    "# Table 4 includes objecs that DO NOT have a companion within the 3'' fiber position, and therfore the double-\\\n",
    "# peaked lines in these objects cannot be due to kpc-duals because we're not observing two NLRs\n",
    "fu2012t4 = pd.read_csv('Tables/Fu2012/Fu2012_t4.csv', sep=',')\n",
    "\n",
    "# From caption for Table 2:\n",
    "#Objects are grouped according to the origin of the double-peaked [O iii] lines. Column 1: J2000 designation. Type-1s are indicated by stars. Column\n",
    "#2: redshift. Column 3: velocity splitting between [O iii] λ5007 components: ΔV/c = [(1 + zr )2/(1 + zb)2 − 1]/[(1 + zr )2/(1 + zb)2 + 1], where zr and zb are\n",
    "#the redshifts of the redshifted and blueshifted [O iii] components, respectively. Columns 4 and 5: [O iii] λ5007 luminosity in log(L ) for the blueshifted (b)\n",
    "#and redshifted (r) line, corrected for Galactic extinction. Columns 6 and 7: [Oiii] λ5007 FWHMs, corrected for the σ = 65 km s−1 instrumental broadening.\n",
    "#Column 8: Kellermann et al. (1989) radio loudness, R = Fν,5 GHz/Fν,4400A. Columns 9 and 10: projected angular separation (arcsec) and physical separation\n",
    "#(kpc) between the main components in a merging system. Column 11: secondary classification (Section 3). Column 12: classification from Shen et al. (2011a).\n",
    "\n",
    "fu2012t2['Name'] = fu2012t2['SDSS Name']\n",
    "fu2012t2['Name2'] = fu2012t2['SDSS Name']\n",
    "#fu2012t2['z1'] = fu2012t2['z']\n",
    "#fu2012t2['z2'] = fu2012t2['z']\n",
    "#fu2012t2['z1_type'] = \"spec\"\n",
    "#fu2012t2['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(fu2012t2,fu2012t2['SDSS Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = fu2012t2['RA'], dec = fu2012t2['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "fu2012t2['RA1_deg'] = coordconvert.ra.degree\n",
    "fu2012t2['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "fu2012t2['RA2'] = fu2012t2['RA']\n",
    "fu2012t2['Dec2'] = fu2012t2['Dec']\n",
    "\n",
    "fu2012t2['RA2_deg'] = fu2012t2['RA1_deg']\n",
    "fu2012t2['Dec2_deg'] = fu2012t2['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "fu2012t2['Equinox'] = \"J2000\"\n",
    "fu2012t2['Coordinate_waveband'] = \"Optical\"\n",
    "fu2012t2['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "fu2012t2['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "fu2012t2['Brightness1'] = \"-99\"\n",
    "fu2012t2['Brightness_band1'] = \"-99\"\n",
    "fu2012t2['Brightness_type1'] = \"-99\"\n",
    "\n",
    "fu2012t2['Brightness2'] = \"-99\"\n",
    "fu2012t2['Brightness_band2'] = \"-99\"\n",
    "fu2012t2['Brightness_type2'] = \"-99\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "fu2012t2['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#fu2012t2['Sep(kpc)'] = fu2012t2['Sep']*((cosmo.arcsec_per_kpc_proper(fu2012t2['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#fu2012t2['delta_z'] = fu2012t2['z1']-fu2012t2['z2']\n",
    "#fu2012t2['dV'] = (2.99e+5)*((1+fu2012t2['z1'])**2 - (1+fu2012t2['z2'])**2)/((1+fu2012t2['z1'])**2+(1+fu2012t2['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "fu2012t2['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "fu2012t2['Confirmation Method'] = \"-99\"\n",
    "fu2012t2['Paper(s)'] = \"Fu+2012\"\n",
    "fu2012t2['BibCode(s)'] = \"2012ApJ...745...67F\"\n",
    "fu2012t2['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/745/1/67\"\n",
    "\n",
    "#fu2012t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,fu2012t2,5)\n",
    "\n",
    "print(len(tmatches))\n",
    "\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for i, j in zip(idx1, idx2):\n",
    "    #print(\"i:\", i, \"j:\", j)\n",
    "    #print(\"Sep:\", mcgurk2015t4.at[j, 'NIRC2sep(as)'])\n",
    "    #print(\"dV:\", mcgurk2015t4.at[j, 'dV[OIII]'])\n",
    "    the_whills.at[i, 'Sep'] = fu2012t2.at[j, 'Deltatheta']\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Fu+2012'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2012ApJ...745...67F' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/745/1/67'\n",
    "    the_whills.at[i, 'dV'] = fu2012t2.at[j, 'DeltaV']\n",
    "    the_whills.at[i, 'Notes'] += ' Fu+2011 argue the origin of the double-peaked lines is ' + str(fu2012t2.at[j, 'Primary Class'])\n",
    "    if pd.notna(fu2012t2.at[j, 'Secondary Class']):\n",
    "        the_whills.at[i, 'Notes'] += 'Fu+2011 also include a secondary class: ' + str(fu2012t2.at[j, 'Secondary Class'])\n",
    "\n",
    "# all match within 5''\n",
    "\n",
    "# and I have now visually verified that we have the correct number of matches and that the \\\n",
    "# correct objects have been matched\n",
    "\n",
    "#the_whills\n",
    "\n",
    "# verified that this process works and adds in the notes as intended.\n",
    "# ******\n",
    "# However we still need to go back and add additonal notes/classifications based on Fu+2011 in-text descriptions\n",
    "# ******\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fu2012t3['Name'] = fu2012t3['SDSSName']\n",
    "fu2012t3['Name2'] = fu2012t3['SDSSName']\n",
    "#fu2012t3['z1'] = fu2012t3['z']\n",
    "#fu2012t3['z2'] = fu2012t3['z']\n",
    "#fu2012t3['z1_type'] = \"spec\"\n",
    "#fu2012t3['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(fu2012t3,fu2012t3['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = fu2012t3['RA'], dec = fu2012t3['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "fu2012t3['RA1_deg'] = coordconvert.ra.degree\n",
    "fu2012t3['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "fu2012t3['RA2'] = fu2012t3['RA']\n",
    "fu2012t3['Dec2'] = fu2012t3['Dec']\n",
    "\n",
    "fu2012t3['RA2_deg'] = fu2012t3['RA1_deg']\n",
    "fu2012t3['Dec2_deg'] = fu2012t3['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "fu2012t3['Equinox'] = \"J2000\"\n",
    "fu2012t3['Coordinate_waveband'] = \"Optical\"\n",
    "fu2012t3['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "fu2012t3['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "fu2012t3['Brightness1'] = \"-99\"\n",
    "fu2012t3['Brightness_band1'] = \"-99\"\n",
    "fu2012t3['Brightness_type1'] = \"-99\"\n",
    "\n",
    "fu2012t3['Brightness2'] = \"-99\"\n",
    "fu2012t3['Brightness_band2'] = \"-99\"\n",
    "fu2012t3['Brightness_type2'] = \"-99\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "fu2012t3['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#fu2012t3['Sep(kpc)'] = fu2012t3['Sep']*((cosmo.arcsec_per_kpc_proper(fu2012t3['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#fu2012t3['delta_z'] = fu2012t3['z1']-fu2012t3['z2']\n",
    "#fu2012t3['dV'] = (2.99e+5)*((1+fu2012t3['z1'])**2 - (1+fu2012t3['z2'])**2)/((1+fu2012t3['z1'])**2+(1+fu2012t3['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "fu2012t3['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "fu2012t3['Confirmation Method'] = \"-99\"\n",
    "fu2012t3['Paper(s)'] = \"Fu+2012\"\n",
    "fu2012t3['BibCode(s)'] = \"2012ApJ...745...67F\"\n",
    "fu2012t3['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/745/1/67\"\n",
    "\n",
    "#fu2012t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,fu2012t3,5)\n",
    "\n",
    "print(len(tmatches))\n",
    "\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Fu+2012'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2012ApJ...745...67F' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/745/1/67'\n",
    "        the_whills.at[index, 'Notes'] += ' Companion(s) within 3 arcseconds.'\n",
    "\n",
    "\n",
    "# all match within 5''\n",
    "\n",
    "# and I have now visually verified that we have the correct number of matches and that the \\\n",
    "# correct objects have been matched\n",
    "\n",
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmatches = tmatches.sort_values(by=['z'])\n",
    "#tmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fu2012t4['Name'] = fu2012t4['SDSS']\n",
    "fu2012t4['Name2'] = fu2012t4['SDSS']\n",
    "#fu2012t4['z1'] = fu2012t4['z']\n",
    "#fu2012t4['z2'] = fu2012t4['z']\n",
    "#fu2012t4['z1_type'] = \"spec\"\n",
    "#fu2012t4['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(fu2012t4,fu2012t4['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = fu2012t4['RA'], dec = fu2012t4['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "fu2012t4['RA1_deg'] = coordconvert.ra.degree\n",
    "fu2012t4['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "fu2012t4['RA2'] = fu2012t4['RA']\n",
    "fu2012t4['Dec2'] = fu2012t4['Dec']\n",
    "\n",
    "fu2012t4['RA2_deg'] = fu2012t4['RA1_deg']\n",
    "fu2012t4['Dec2_deg'] = fu2012t4['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "fu2012t4['Equinox'] = \"J2000\"\n",
    "fu2012t4['Coordinate_waveband'] = \"Optical\"\n",
    "fu2012t4['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "fu2012t4['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "fu2012t4['Brightness1'] = \"-99\"\n",
    "fu2012t4['Brightness_band1'] = \"-99\"\n",
    "fu2012t4['Brightness_type1'] = \"-99\"\n",
    "\n",
    "fu2012t4['Brightness2'] = \"-99\"\n",
    "fu2012t4['Brightness_band2'] = \"-99\"\n",
    "fu2012t4['Brightness_type2'] = \"-99\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "fu2012t4['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#fu2012t4['Sep(kpc)'] = fu2012t4['Sep']*((cosmo.arcsec_per_kpc_proper(fu2012t4['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#fu2012t4['delta_z'] = fu2012t4['z1']-fu2012t4['z2']\n",
    "#fu2012t4['dV'] = (2.99e+5)*((1+fu2012t4['z1'])**2 - (1+fu2012t4['z2'])**2)/((1+fu2012t4['z1'])**2+(1+fu2012t4['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "fu2012t4['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "fu2012t4['Confirmation Method'] = \"-99\"\n",
    "fu2012t4['Paper(s)'] = \"Fu+2012\"\n",
    "fu2012t4['BibCode(s)'] = \"2012ApJ...745...67F\"\n",
    "fu2012t4['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/745/1/67\"\n",
    "\n",
    "#fu2012t4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,fu2012t4,14)\n",
    "\n",
    "print(len(tmatches))\n",
    "\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Fu+2012'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2012ApJ...745...67F' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/745/1/67'\n",
    "        the_whills.at[index, 'Notes'] += ' No companion within 3 arcseconds.'\n",
    "\n",
    "\n",
    "# we have to go out to 14'' in match tolerance \\\n",
    "# to get the last object (72 match within 10'')\n",
    "\n",
    "# and I have now visually verified that we have the correct number of matches and that the \\\n",
    "# correct objects have been matched\n",
    "\n",
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_matches(names):\n",
    "#     name_dict = {}\n",
    "#     matches = []\n",
    "\n",
    "#     for name in names:\n",
    "#         first_6_chars = name[:7]\n",
    "#         if first_6_chars in name_dict:\n",
    "#             matches.append((name_dict[first_6_chars], name))\n",
    "#         else:\n",
    "#             name_dict[first_6_chars] = name\n",
    "\n",
    "#     return matches\n",
    "\n",
    "# names = the_whills['Name'].to_list()\n",
    "# matched_pairs = find_matches(names)\n",
    "\n",
    "# for pair in matched_pairs:\n",
    "#     print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "    \n",
    "# tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,fu2012t4,14)\n",
    "\n",
    "# print(len(tmatches))\n",
    "\n",
    "\n",
    "# # Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "# for index, row in the_whills.iterrows():\n",
    "#     if index in idx1:\n",
    "#         the_whills.at[index, 'Paper(s)'] += ' ; Fu+2012'\n",
    "#         the_whills.at[index, 'BibCode(s)'] += ' ; 2012ApJ...745...67F' \n",
    "#         the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/745/1/67'\n",
    "#         the_whills.at[index, 'Notes'] += ' No companion within 3 arcseconds.'\n",
    "\n",
    "\n",
    "# # we have to go out to 14'' in match tolerance \\\n",
    "# # to get the last object (72 match within 10'')\n",
    "\n",
    "# # and I have now visually verified that we have the correct number of matches and that the \\\n",
    "# # correct objects have been matched\n",
    "\n",
    "# #the_whills\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're loading in the double-peaked emission line galaxy catalog of Comerford2012\n",
    "comerford2012t1 = ((Table.read('Tables/Comerford2012/table1.dat', readme = 'Tables/Comerford2012/ReadMe', format='ascii.cds')).to_pandas()).drop(columns=['---', 'T/I','Date1','---_1','Date2','PA1','PA2','Exp1','---_2','Exp2'])\n",
    "comerford2012t2 = ((Table.read('Tables/Comerford2012/table2.dat', readme = 'Tables/Comerford2012/ReadMe', format='ascii.cds')).to_pandas()).drop(columns=['n_SName','T','Em','PA','e_PA','dPA','ell','e_dXang','dXkpc','e_dXkpc','e_dV'])\n",
    "\n",
    "comerford2012 = (pd.concat([comerford2012t1,comerford2012t2], axis=1)).drop(columns=['SName'])\n",
    "\n",
    "\n",
    "#References. (1) Liu et al. 2010b; (2) Wang et al. 2009; (3) Fu et al. 2012; (4) Shen et al. 2011; (5) Tingay & Wayth 2011; (6) Smith et al. 2010;\n",
    "#(7) Rosario et al. 2011; (8) Fu et al. 2011a; (9) McGurk et al. 2011; (10) Rosario et al. 2010; (11) Comerford et al. 2011.\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(comerford2012,comerford2012['SDSS'])\n",
    "\n",
    "comerford2012['Name'] = comerford2012['SDSS']\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = comerford2012['RA'], dec = comerford2012['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "comerford2012['RA1_deg'] = coordconvert.ra.degree\n",
    "comerford2012['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "comerford2012['RA2'] = comerford2012['RA']\n",
    "comerford2012['Dec2'] = comerford2012['Dec']\n",
    "\n",
    "comerford2012['RA2_deg'] = comerford2012['RA1_deg']\n",
    "comerford2012['Dec2_deg'] = comerford2012['Dec1_deg']\n",
    "\n",
    "\n",
    "#comerford2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we're matching comerford+2012\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,comerford2012,15)\n",
    "\n",
    "print(len(tmatches))\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    #print(\"i:\", i, \"j:\", j)\n",
    "    #print(\"Sep:\", mcgurk2015t4.at[j, 'NIRC2sep(as)'])\n",
    "    #print(\"dV:\", mcgurk2015t4.at[j, 'dV[OIII]'])\n",
    "    the_whills.at[i, 'Sep'] = comerford2012.at[j, 'dXang']\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Comerford+2012'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2012ApJ...753...42C' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/753/1/42'\n",
    "    the_whills.at[i, 'dV'] = comerford2012.at[j, 'dV']\n",
    "    the_whills.at[i, 'Notes'] += ' Comerford+ examined long slit spectra of this target. Two spatially distinct emission peaks observed.'\n",
    "\n",
    "# comerford+ consider 14 objects as being promising dual AGN candidates:\n",
    "dualcand = ['J084049.47+272704.8','J095207.62+255257.2','J160524.59+152233.5',\\\n",
    "            'J163316.03+262716.3','J210449.13-000919.1','J225510.12-081234.4',\\\n",
    "            'J230442.82-093345.3','J011659.59-102539.1','J080418.23+305157.2',\\\n",
    "            'J093024.84+343057.3','J102325.57+324348.4','J144804.17+182537.9',\\\n",
    "            'J162939.58+240856.0','J225420.99-005134.1']\n",
    "dualcand = pd.DataFrame(dualcand, columns=['SDSS'])\n",
    "\n",
    "name_to_coords(dualcand,dualcand['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = dualcand['RA'], dec = dualcand['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "dualcand['RA1_deg'] = coordconvert.ra.degree\n",
    "dualcand['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,dualcand,15)\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Notes'] += ' Comerford+2012 consider this a strong dual AGN candidate. Double optical emission line peaks observed oriented along the plane of the galaxy.'\n",
    "\n",
    "# we have to use an exceptionally large search size (15''), but i visually verified that the matching is working\n",
    "\n",
    "#tmatches = tmatches.sort_values(by=['Name'])\n",
    "#tmatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dualcand = ['J095207.62+255257.2','J115523.74+150756.9','J123915.40+531414.6']\n",
    "dualcand = pd.DataFrame(dualcand, columns=['SDSS'])\n",
    "\n",
    "name_to_coords(dualcand,dualcand['SDSS'])\n",
    "\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = dualcand['RA'], dec = dualcand['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "dualcand['RA1_deg'] = coordconvert.ra.degree\n",
    "dualcand['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,dualcand,15)\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Notes'] += ' Comerford+2012 consider this a very strong dual AGN candidate. The spatial separations and positional angles of the double emission components likely coincide with the observed double stellar nuclei.'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills.to_csv('the_whills_uptoComerford2013.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're loading in the double-peaked emission line galaxy catalog of Comerford2013\n",
    "comerford2013 = ((Table.read('Tables/Comerford2013/table1.dat', readme = 'Tables/Comerford2013/ReadMe', format='ascii.cds')).to_pandas()).drop(columns=['---','HbVoff','e_HbVoff','O3Voff','e_O3Voff','HaVoff','e_HaVoff','O3/Hb','e_O3/Hb','N2/Ha','e_N2/Ha','S2/Ha','e_S2/Ha','O1/Ha','e_O1/Ha','Class','n_Class'])\n",
    "# table1.dat is a modified version of Wang's catalog in which I've added a duplicate row for each target\n",
    "# since all of these are candidate dual AGN systems\n",
    "\n",
    "# Comerford2013 looked at 173 Type 2 AGNs in Deep2 and found only two double-peaked dual AGN candidates \\\n",
    "# and found five offset AGNs. We are retaining only the double peaked duals.s\n",
    "\n",
    "comerford2013 = comerford2013[(comerford2013['NDWFS'] == 'J143208.27+353255.5') | (comerford2013['NDWFS'] == 'J143359.71+351020.5')]\n",
    "\n",
    "comerford2013['Coordinates'] = comerford2013['NDWFS'].str.slice(start=1)\n",
    "comerford2013['RA_test'] = comerford2013['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "comerford2013['Dec_test'] = comerford2013['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "comerford2013['RA'] = comerford2013['RA_test'].str.slice(start=0, stop=2)+\":\"+comerford2013['RA_test'].str.slice(start=2, stop=4)+\":\"+comerford2013['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "comerford2013['Dec'] = comerford2013['Dec_test'].str.slice(start=0, stop=3)+\":\"+comerford2013['Dec_test'].str.slice(start=3, stop=5)+\":\"+comerford2013['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "comerford2013.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "\n",
    "\n",
    "comerford2013['Name'] = comerford2013['NDWFS']\n",
    "comerford2013['Name2'] = comerford2013['NDWFS']\n",
    "comerford2013['z1'] = comerford2013['z']\n",
    "comerford2013['z2'] = comerford2013['z']\n",
    "comerford2013['z1_type'] = \"spec\"\n",
    "comerford2013['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "#name_to_coords(comerford2013,comerford2013['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = comerford2013['RA'], dec = comerford2013['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "comerford2013['RA1_deg'] = coordconvert.ra.degree\n",
    "comerford2013['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "comerford2013['RA2'] = comerford2013['RA']\n",
    "comerford2013['Dec2'] = comerford2013['Dec']\n",
    "\n",
    "comerford2013['RA2_deg'] = comerford2013['RA1_deg']\n",
    "comerford2013['Dec2_deg'] = comerford2013['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "comerford2013['Equinox'] = \"J2000\"\n",
    "comerford2013['Coordinate_waveband'] = \"Optical\"\n",
    "comerford2013['Coordinate_Source'] = \"AGES\"\n",
    "\n",
    "comerford2013['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "comerford2013['Brightness1'] = \"-99\"\n",
    "comerford2013['Brightness_band1'] = \"-99\"\n",
    "comerford2013['Brightness_type1'] = \"-99\"\n",
    "\n",
    "comerford2013['Brightness2'] = \"-99\"\n",
    "comerford2013['Brightness_band2'] = \"-99\"\n",
    "comerford2013['Brightness_type2'] = \"-99\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "comerford2013['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#comerford2013['Sep(kpc)'] = comerford2013['Sep']*((cosmo.arcsec_per_kpc_proper(comerford2013['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "comerford2013['delta_z'] = comerford2013['z1']-comerford2013['z2']\n",
    "comerford2013['dV'] = (2.99e+5)*((1+comerford2013['z1'])**2 - (1+comerford2013['z2'])**2)/((1+comerford2013['z1'])**2+(1+comerford2013['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "comerford2013['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "comerford2013['Confirmation Method'] = \"-99\"\n",
    "comerford2013['Paper(s)'] = \"Comerford+2013\"\n",
    "comerford2013['BibCode(s)'] = \"2013ApJ...777...64C\"\n",
    "comerford2013['DOI(s)'] = \"https://doi:10.1088/0004-637X/777/1/64\"\n",
    "\n",
    "comerford2013 = comerford2013.iloc[::2].reset_index(drop=True)\n",
    "# Note: for some reason, if we don't drop the index here, the matching process ahead will not work and one of the matching objects between Ge+ and Comerford+ will be considered both unique and a match.\n",
    "#comerford2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comerford2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're matching the_whills against the Comerford2013 catalog\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,comerford2013,5)\n",
    "\n",
    "#tmatches\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Paper(s)']!='Comerford+2013':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Comerford+2013'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2013ApJ...777...64C' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi:10.1088/0004-637X/777/1/64'\n",
    "        tmatches.at[index, 'Notes'] += ' Ge+ selected this as a double-peaked source in SDSS. Comerford+ selected this as a double-peaked dual candidate in AGES.'\n",
    "        \n",
    "# Now clipping out all Comerford+2013 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Paper(s)']!='Comerford+2013'].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "the_whills.drop(labels=['level_0','index'], axis=1, inplace=True)\n",
    "#tunique\n",
    "#the_whills\n",
    "\n",
    "# Verified that this matchinkg process works properly, and I debugged a previous issue related to the indexing/matching for the Comerford+ table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're adding in the information from An+2013 which examines J151656.59+183021.5, a double-peaked AGN \\\n",
    "# identified in Smith+2010\n",
    "\n",
    "an2013obs = ['J151656.59+183021.5']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in an2013obs:\n",
    "        print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; An+2013'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2013MNRAS.433.1161A' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stt801'\n",
    "        the_whills.at[index, 'Notes'] += ' An+ refers to this as 3C 316. An+ presents eMERLIN VLA and EVN imaging of 3C 316 and reveals three major components (eastern and central and western components) that are further resolved into discrete jet components in an S-shaped line in the EVN image. None of these could be unambiguously identified as an AGN core. These resemble the knots of jets. They argue the radio structure is consistent with a single radio emitting AGN but they cannot rule out a kiloparsec radio-quiet companion.'\n",
    "\n",
    "# Verified that this matching process works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're loading in the double-peaked emission line galaxy catalog of Barrows2013\n",
    "#barrows2013t1 = ((Table.read('Barrows2013/table1.dat', readme = 'Barrows2013/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---','HbVoff','e_HbVoff','O3Voff','e_O3Voff','HaVoff','e_HaVoff','O3/Hb','e_O3/Hb','N2/Ha','e_N2/Ha','S2/Ha','e_S2/Ha','O1/Ha','e_O1/Ha','Class','n_Class'])\n",
    "#barrows2013t2 = ((Table.read('Barrows2013/table2.dat', readme = 'Barrows2013/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---','HbVoff','e_HbVoff','O3Voff','e_O3Voff','HaVoff','e_HaVoff','O3/Hb','e_O3/Hb','N2/Ha','e_N2/Ha','S2/Ha','e_S2/Ha','O1/Ha','e_O1/Ha','Class','n_Class'])\n",
    "barrows2013 = ((Table.read('Tables/Barrows2013/table3.dat', readme = 'Tables/Barrows2013/ReadMe', format='ascii.cds')).to_pandas()).drop(columns=['e_zsdss','zmgii','e_zmgii','FWHMfeii','FWHMmgii','e_FWHMmgii','fedd'])\n",
    "# table 1 and 2 specifically look at Ne V and Ne III double peaked sources (with some overlap)\n",
    "# table 3 includes the full sample\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(barrows2013,barrows2013['SDSS'])\n",
    "\n",
    "barrows2013['Name'] = barrows2013['SDSS']\n",
    "barrows2013['Name2'] = barrows2013['SDSS']\n",
    "barrows2013['z1'] = barrows2013['zsdss']\n",
    "barrows2013['z2'] = barrows2013['zsdss']\n",
    "barrows2013['z1_type'] = \"spec\"\n",
    "barrows2013['z2_type'] = \"spec\"\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = barrows2013['RA'], dec = barrows2013['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "barrows2013['RA1_deg'] = coordconvert.ra.degree\n",
    "barrows2013['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "barrows2013['RA2'] = barrows2013['RA']\n",
    "barrows2013['Dec2'] = barrows2013['Dec']\n",
    "\n",
    "barrows2013['RA2_deg'] = barrows2013['RA1_deg']\n",
    "barrows2013['Dec2_deg'] = barrows2013['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "barrows2013['Equinox'] = \"J2000\"\n",
    "barrows2013['Coordinate_waveband'] = \"Optical\"\n",
    "barrows2013['Coordinate_Source'] = \"AGES\"\n",
    "\n",
    "barrows2013['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "barrows2013['Brightness1'] = \"-99\"\n",
    "barrows2013['Brightness_band1'] = \"-99\"\n",
    "barrows2013['Brightness_type1'] = \"-99\"\n",
    "\n",
    "barrows2013['Brightness2'] = \"-99\"\n",
    "barrows2013['Brightness_band2'] = \"-99\"\n",
    "barrows2013['Brightness_type2'] = \"-99\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "barrows2013['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#barrows2013['Sep(kpc)'] = barrows2013['Sep']*((cosmo.arcsec_per_kpc_proper(barrows2013['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#barrows2013['delta_z'] = barrows2013['z1']-barrows2013['z2']\n",
    "barrows2013['dV'] = (2.99e+5)*((1+barrows2013['z1'])**2 - (1+barrows2013['z2'])**2)/((1+barrows2013['z1'])**2+(1+barrows2013['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "barrows2013['Selection Method'] = \"Double-Peaked Spectroscopic Emission Lines\" #DPSELs\n",
    "barrows2013['Confirmation Method'] = \"-99\"\n",
    "barrows2013['Paper(s)'] = \"Barrows+2013\"\n",
    "barrows2013['BibCode(s)'] = \"2013ApJ...769...95B\"\n",
    "barrows2013['DOI(s)'] = \"https://doi:10.1088/0004-637X/769/2/95\"\n",
    "barrows2013['Notes'] = \" \"\n",
    "\n",
    "#barrows2013\n",
    "\n",
    "# Here we're matching the_whills against the Barrows+2013 catalog\n",
    "#tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,barrows2013,5)\n",
    "#tmatches\n",
    "# There are no matches between Barrows+2013 and the_whills\n",
    "\n",
    "# Run next cell to concatenate Barrows+ with the main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills = pd.concat([the_whills,barrows2013])\n",
    "\n",
    "# Verified that this matching/concatenation process is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm going to add in the information from Liu+2013, Shangguan+2016, and \\\n",
    "# Barrows+2016, and +2018, and Bondi+2016\n",
    "\n",
    "# Here now that we're loaded in the double peaked catalogs, we'll add in some individual targets\n",
    "\n",
    "# First, Liu+2010b\n",
    "liu2010objs = ['J110851.04+065901.4','J113126.08-020459.2','J114642.47+511029.6','J133226.34+060627.4']\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in liu2010objs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Liu+2013 ; Shangguan+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2013ApJ...762..110L ; 2016ApJ...823...50S' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/762/2/110 ; https://doi.org/10.3847/0004-637X/823/1/50'\n",
    "        #the_whills.at[index, 'System Type']='Dual AGN'\n",
    "        #the_whills.at[index, 'Notes']='NIR nuclei and double OIII emitting regions are spatially coincident. Angular separations quoted are the separations between the OIII components.'\n",
    "        #the_whills.at[index, 'Confirmation Method'] = 'NIR Imaging / Optical Spectroscopy'\n",
    "        \n",
    "the_whills.loc[the_whills['Name']=='J110851.04+065901.4', 'System Type'] = 'Dual AGN'\n",
    "the_whills.loc[the_whills['Name']=='J114642.47+511029.6', 'System Type'] = 'Dual AGN'\n",
    "\n",
    "\n",
    "# Adding in additional info from Bondi+2016 here...\n",
    "bondi2016 = ['J110851.04+065901.4','J113126.08-020459.2']\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in bondi2016:\n",
    "        #print('True2')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Bondi+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016A&A...588A.102B' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1051/0004-6361/201528021'\n",
    "        the_whills.at[index, 'Notes'] += ' '\n",
    "\n",
    "the_whills.loc[the_whills['Name']=='J110851.04+065901.4', 'Notes'] += \" One single compact source is reported by Bondi+ in EVN observations.\"\n",
    "the_whills.loc[the_whills['Name']=='J113126.08-020459.2', 'Notes'] += \" Bondi+ report that the VLBI observation show no compact cores but the VLA imaging reveals a possible core.\"\n",
    "        \n",
    "# Now for Barrows+2016 # Borrows+ does not add much here beyond the literature besides also selecting this as an offset AGN candidate\n",
    "barrowsobjs = ['J110851.04+065901.4'] # Barrows+2016 has is listed as J110851.04+065901.5\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in barrowsobjs:\n",
    "        #print('True3')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Barrows+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...829...37B' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/829/1/37'\n",
    "\n",
    "# Now for Barrows+2016,2018\n",
    "barrowsobjs = ['J123420.14+475155.9'] # Barrows lists it as SDSSJ123420.14+471555.86\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in barrowsobjs:\n",
    "        #print('True4')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Barrows+2016 ; Barrows+2018'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...829...37B ; 2018ApJ...869..154B' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/829/1/37 ; https://doi.org/10.3847/1538-4357/aaedb6'\n",
    "        the_whills.at[index, 'Notes'] += ' Barrows+2018 notes that the one stellar core is the bluest of stellar cores within their sample while and the other is reddened.'\n",
    "\n",
    "#Also in Barrows 2016 (but not 2017 and 2018): J110851.04+065901.4\n",
    "#Nevin+ does not overlap with these follow-up works but does overlap with the original double peak catalogs\n",
    "#Liu+2018 will have its own cell because it examines 18 objects \n",
    "#Barrows  2016 and 2018 overlap for the object not  included in Liu+ --> this will have a separate entry in the manual table\n",
    "#Barrows+2016,+2018 J1234 overlaps with Ge+2012 --> this we'll have a cell here for adding bib info\n",
    "\n",
    "#******* THIS IS A NOTE TO COME BACK TO SO THAT BARROWS IS FULLY PROCESSED*****\n",
    "#Barrows+2016 also overlaps with Liu+2011 catalog\n",
    "\n",
    "#the_whills.loc[the_whills['Name']=='J110851.04+065901.4', 'Sep'] = 0.9\n",
    "#the_whills.loc[the_whills['Name']=='J113126.08-020459.2', 'Sep'] = 0.6\n",
    "#the_whills.loc[the_whills['Name']=='J114642.47+511029.6', 'Sep'] = 2.5\n",
    "#the_whills.loc[the_whills['Name']=='J133226.34+060627.4', 'Sep'] = 1.6\n",
    "\n",
    "# Barrows+2016 has a typo in the name of J123420.14 in their manuscript\n",
    "\n",
    "\n",
    "# Verified that the matching process here works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Here we're adding in the results from Comerford+2015\n",
    "# We adjusted the naming from comerford+ for J014209.01−005050.0\n",
    "objs = ['J014209-005049','J075223.35+273643.1','J084135.09+010156.2','J085416.76+502632.0',\\\n",
    "        'J095207.62+255257.2','J100654.20+464717.2','J112659.54+294442.8','J123915.40+531414.6',\\\n",
    "        'J132231.86+263159.1','J135646.11+102609.1','J144804.17+182537.9','J160436.21+500958.1']\n",
    "#print(len(objs))\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True', row['Name'])\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Comerford+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...806..219C' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/806/2/219'\n",
    "        #the_whills.at[index, 'Notes']='AGNs confirmed via X-rays and reanalyses of optical spectroscopy.'\n",
    "        #the_whills.at[index, 'Confirmation Method'] = 'X-ray Imaging / X-ray Spectroscopy / Optical Spectroscopy'\n",
    "\n",
    "#the_whills.at[index, 'System Type']='Dual AGN'\n",
    "\n",
    "objs = ['J112659.54+294442.8']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True1', row['Name'])\n",
    "        the_whills.at[index, 'System Type'] = 'Dual AGN'\n",
    "        the_whills.at[index, 'Notes'] += 'Comerford+ claim a dual AGN in this system. It is an extremely minor merger (460:1) where the secondary is detected at 2.3sigma.'\n",
    "        #the_whills.at[index, 'Notes'] += 'AGNs confirmed via X-rays and reanalyses of optical spectroscopy.'\n",
    "        the_whills.at[index, 'Confirmation Method'] = 'X-ray Imaging / X-ray Spectroscopy / Optical Spectroscopy'\n",
    "\n",
    "objs = ['J084135.09+010156.2','J095207.62+255257.2','J123915.40+531414.6','J132231.86+263159.1','J135646.11+102609.1']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True2', row['Name'])\n",
    "        the_whills.at[index, 'System Type']='Dual AGN Candidate/Offset AGN Candidate'\n",
    "        the_whills.at[index, 'Notes'] += ' Comerford+ classify these as dual AGN candidates/Offset AGN candidates based on Chandra and HST imaging. These systems have two [OIII] emission components with the same spatial separation and orientation as the two stellar bulges (within 3sigma) but two coincident X-ray sources are not detected above 2sigma.'\n",
    "\n",
    "objs = ['J014209-005049','J075223.35+273643.1','J085416.76+502632.0','J100654.20+464717.2','J144804.17+182537.9','J160436.21+500958.1']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True3', row['Name'])\n",
    "        the_whills.at[index, 'System Type']='Single AGN'\n",
    "        the_whills.at[index, 'Notes'] += ' Comerford+2015 conclude these are likely single AGNs.'\n",
    "\n",
    "objs = ['J095207.62+255257.2']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #print('True4', row['Name'])\n",
    "        the_whills.at[index, 'System Type']='Dual AGN'\n",
    "        the_whills.at[index, 'Notes'] += ' McGurk+2011 classified this as a Type 1 + type II pair based on extracted spectra.'\n",
    "        # --> we may not need to actually add this note. We may have to just drop the target from the tables \\\n",
    "        # here and add it back in via an individual object listing\n",
    "\n",
    "# Did not originally match. The former is because of the naming convention, the latter simply wasn't in the table\n",
    "#J014209.01−005050.0\n",
    "#J084135.09+010156.2\n",
    "\n",
    "#\n",
    "# We need to figure out why J084135.09+010156.2 does not show up\n",
    "# I looked manually through the Wang+ Liu+ and Smith+ tables and I do not see J0841+0101 anywhere...\n",
    "# I think it's not in those tables because it isn't actually a double-peaked source? I;m not \\\n",
    "# entirely sure why it was in Comerford+'s paper, but I think we're going to remove it from here and load \\\n",
    "# it in manually in the individual targets table\n",
    "\n",
    "\n",
    "# Verified that the matching process works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're addiing in the Gabanyi+ results that focus on four objects drawn from Comerford+2012\n",
    "\n",
    "# SDSS J102325.57+324348.4 J1023+3243\n",
    "# SDSS J115523.74+150756.9 J1155+1507\n",
    "# SDSS J210449.13–000919.1 J2104–0009\n",
    "# SDSS J23044.82–093345.3 J2304–0933\n",
    "\n",
    "\n",
    "gabobjs = ['J210449.13-000919.1','J230442.82-093345.3'] # there was a typo in Gabanyi's writing of J230442\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in gabobjs:\n",
    "        #print('True1')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Gabanyi+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...826..106G' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/826/2/106'\n",
    "        the_whills.at[index, 'Notes'] += ' Gabanyi+ find single radio sources in this object and that the position angles of the radio structure and double optical emission components are consistent and support a jet-driven scenario for the double peaked emission lines (though the radio jets influencing the lines are on much larger scales than mapped by Gabanyi+).'\n",
    "\n",
    "\n",
    "gabobjs = ['J115523.74+150756.9'] # \n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in gabobjs:\n",
    "        #print('True2')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Gabanyi+2016 '\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...826..106G' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/826/2/106'\n",
    "        the_whills.at[index, 'Notes'] += ' Gabanyi+ find the position angles of the radio emission and optical emission are nearly perpendicular and are therefore unrelated. Only one source is detected in the radio imaging.'\n",
    "\n",
    "gabobjs = ['J102325+324348'] # Gabanyi+ uses J102325.57+324348.4 for naming. I'm going to rename the listing in my table\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in gabobjs:\n",
    "        #print('True3')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Gabanyi+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...826..106G' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/826/2/106'\n",
    "        the_whills.at[index, 'Notes'] += ' Gabanyi+ do not detect a source in this system but Muller-Sanchez+ do detect two components.'\n",
    "        the_whills.at[index, 'Name'] = 'J102325.57+324348.4'\n",
    "\n",
    "\n",
    "# Verified that this matching process works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Nevin+2016\n",
    "nevin2016 = ((Table.read('Tables/nevin2016/table1.dat', readme = 'Tables/nevin2016/ReadMe', format='ascii.cds')).to_pandas())\n",
    "\n",
    "ogobjs = ['J101835.77+512753.1'] # Nevin+2016 has is listed as J101835.77+512753.0\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in ogobjs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Nevin+2016'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2016ApJ...832...67N' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/0004-637X/832/1/67'\n",
    "        the_whills.at[index, 'Notes']+=' Nevin+ classifies this system as Rotationally domination + disturbance'\n",
    "\n",
    "# Verified that this matching process works\n",
    "\n",
    "# ***********\n",
    "# We probably should go back and just add the bib info for any other matching objects between Nevin+ and the main \\\n",
    "# tables. \n",
    "# ***********\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nevin2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Liu+2018\n",
    "#\n",
    "# CONSIDER GOING BACK AND ADDING IN INFO ON WHICH ARE UNRESOLVE AND WHICH LIKELY HAVE JETS\n",
    "#\n",
    "\n",
    "# First the galaxies detected at 8.4 GHz\n",
    "liu2010obs = ['J091201.68+532036.6','J113721.36+612001.2','J124358.36-005845.4','J135251.22+654113.2','J231051.95-090011.9','J233313.17+004911.8']\n",
    "#print(len(liu2010obs))\n",
    "liu2010obs = pd.DataFrame(data=liu2010obs,columns=['SDSS'])\n",
    "name_to_coords(liu2010obs,liu2010obs['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = liu2010obs['RA'], dec = liu2010obs['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "liu2010obs['RA1_deg'] = coordconvert.ra.degree\n",
    "liu2010obs['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,liu2010obs,5)\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Table_flag']!='Table2':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Liu+2018'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2018ApJ...854..169L' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aaab47'\n",
    "        tmatches.at[index, 'Notes'] += 'Detected in follow-up 8.4 GHz VLBA imaging but unambiguous sub-kpc dual AGNs are not identified.'\n",
    "\n",
    "# Now clipping out all Liu+2018 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "the_whills.drop(labels=['index','Table_flag'], axis=1, inplace=True) #'level_0',\n",
    "\n",
    "###for index, row in the_whills.iterrows():\n",
    "###    if row['Name'] in liu2010objs:\n",
    "###        print('True1')\n",
    "###        #the_whills.at[index, 'Paper(s)'] += ' ; Liu+2018 '\n",
    "###        #the_whills.at[index, 'BibCode(s)'] += ' ; 2018ApJ...854..169L' \n",
    "###        #the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aaab47 ; '\n",
    "###        #the_whills.at[index, 'Notes']='Detected in follow-up 8.4 GHz VLBA imaging but unambiguous sub-kpc dual AGNs are not identified.'\n",
    "###\n",
    "\n",
    "# Next the galaxies NOT detected at 8.4 GHz\n",
    "liu2010obs = ['J000911.58-003654.7','J073849.75+315611.9','J080337.32+392633.1','J085841.76+104122.1','J110851.04+065901.4','J135646.11+102609.1','J171544.05+600835.7']\n",
    "#print(len(liu2010obs))\n",
    "liu2010obs = pd.DataFrame(data=liu2010obs,columns=['SDSS'])\n",
    "name_to_coords(liu2010obs,liu2010obs['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = liu2010obs['RA'], dec = liu2010obs['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "liu2010obs['RA1_deg'] = coordconvert.ra.degree\n",
    "liu2010obs['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,liu2010obs,5)\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Table_flag']!='Table2':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Liu+2018'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2018ApJ...854..169L' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aaab47'\n",
    "        tmatches.at[index, 'Notes'] += 'Detected in follow-up 8.4 GHz VLBA imaging but unambiguous sub-kpc dual AGNs are not identified.'\n",
    "\n",
    "#print(len(tmatches))\n",
    "# Now clipping out all Liu+2018 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "the_whills.drop(labels=['index','Table_flag'], axis=1, inplace=True) #'level_0',\n",
    "\n",
    "#for index, row in the_whills.iterrows():\n",
    "#    if row['Name'] in liu2010objs:\n",
    "#        print('True2')\n",
    "#        #the_whills.at[index, 'Paper(s)'] += ' ; Liu+2018 '\n",
    "#        #the_whills.at[index, 'BibCode(s)'] += ' ; 2018ApJ...854..169L' \n",
    "#        #the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aaab47 ; '\n",
    "#        #the_whills.at[index, 'Notes']='Not detected at 8.4 GHz in follow-up VLBA imaging.'\n",
    "\n",
    "#the_whills\n",
    "\n",
    "\n",
    "# Verified that this matching process works. Everything seems okay. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is for Shi+2014\n",
    "\n",
    "# I created this catalog manually since Shi+2014 did not publish their catalog separately on CDS\n",
    "shi2014 = pd.read_csv('Tables/Shi2014/shi2014.csv', sep=',')\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(shi2014,shi2014['Designation'])\n",
    "\n",
    "shi2014['Name'] = shi2014['Designation']\n",
    "shi2014['Name2'] = shi2014['Designation']\n",
    "shi2014['z1'] = shi2014['z']\n",
    "shi2014['z2'] = shi2014['z']\n",
    "shi2014['z1_type'] = \"spec\"\n",
    "shi2014['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(shi2014,shi2014['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = shi2014['RA'], dec = shi2014['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "shi2014['RA1_deg'] = coordconvert.ra.degree\n",
    "shi2014['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "shi2014['RA2'] = shi2014['RA']\n",
    "shi2014['Dec2'] = shi2014['Dec']\n",
    "\n",
    "shi2014['RA2_deg'] = shi2014['RA1_deg']\n",
    "shi2014['Dec2_deg'] = shi2014['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "shi2014['Equinox'] = \"J2000\"\n",
    "shi2014['Coordinate_waveband'] = \"Optical\"\n",
    "shi2014['Coordinate_Source'] = \"LAMOST\"\n",
    "\n",
    "shi2014['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "shi2014['Brightness1'] = shi2014['r']\n",
    "shi2014['Brightness_band1'] = \"LAMOST r-band\"\n",
    "shi2014['Brightness_type1'] = \"mag\"\n",
    "\n",
    "shi2014['Brightness2'] = shi2014['r']\n",
    "shi2014['Brightness_band2'] = \"LAMOST r-band\"\n",
    "shi2014['Brightness_type2'] = \"mag\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "shi2014['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#shi2014['Sep(kpc)'] = shi2014['Sep']*((cosmo.arcsec_per_kpc_proper(shi2014['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#shi2014['delta_z'] = shi2014['z']-shi2014['z2']\n",
    "shi2014['dV'] = (2.99e+5)*((1+shi2014['z'])**2 - (1+shi2014['z2'])**2)/((1+shi2014['z'])**2+(1+shi2014['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "shi2014['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "shi2014['Confirmation Method'] = \"-99\"\n",
    "shi2014['Paper(s)'] = \"Shi+2014\"\n",
    "shi2014['BibCode(s)'] = \"2014RAA....14.1234S\"\n",
    "shi2014['DOI(s)'] = \"https://doi:10.1088/1674-4527/14/10/003\"\n",
    "shi2014['Notes'] = ''\n",
    "\n",
    "#shi2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,shi2014,15)\n",
    "\n",
    "print(len(tmatches))\n",
    "# There are 4 matches up to this point between the Shi+2014 catalog and the_whills\n",
    "\n",
    "#tmatches\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Paper(s)']!='Shi+2014':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Shi+2014'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2014RAA....14.1234S' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi:10.1088/1674-4527/14/10/003'\n",
    "        tmatches.at[index, 'Notes'] += ' Shi+ also selected this in LAMOST.'\n",
    "        \n",
    "# Now clipping out all Comerford+2013 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Paper(s)']!='Shi+2014'].reset_index(drop=True)\n",
    "\n",
    "# Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "#the_whills.drop(labels=['level_0','index'], axis=1, inplace=True)\n",
    "\n",
    "# verified that this matching process works and that we get all of the matches we should be getting out to 15''!\n",
    "\n",
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(the_whills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for McGurk+2015\n",
    "\n",
    "mcgurk2015 = pd.read_csv('Tables/McGurk2015/McGurk2015.csv', sep=',')\n",
    "mcgurk2015t2 = pd.read_csv('Tables/McGurk2015/McGurk2015t2.csv', sep=',')\n",
    "mcgurk2015t3 = pd.read_csv('Tables/McGurk2015/McGurk2015t3.csv', sep=',')\n",
    "mcgurk2015t4 = pd.read_csv('Tables/McGurk2015/McGurk2015t4.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(names):\n",
    "    name_dict = {}\n",
    "    matches = []\n",
    "\n",
    "    for name in names:\n",
    "        first_6_chars = name[:7]\n",
    "        if first_6_chars in name_dict:\n",
    "            matches.append((name_dict[first_6_chars], name))\n",
    "        else:\n",
    "            name_dict[first_6_chars] = name\n",
    "\n",
    "    return matches\n",
    "\n",
    "#names = mcgurk2015t4['Name'].to_list()\n",
    "#matched_pairs = find_matches(names)\n",
    "#\n",
    "#for pair in matched_pairs:\n",
    "#    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = the_whills['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mcgurk2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = mcgurk2015t2['RA'], dec = mcgurk2015t2['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "mcgurk2015t2['RA1_deg'] = coordconvert.ra.degree\n",
    "mcgurk2015t2['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "\n",
    "#print(len(mcgurk2015t2))\n",
    "#mcgurk2015t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = mcgurk2015t3['RA'], dec = mcgurk2015t3['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "mcgurk2015t3['RA1_deg'] = coordconvert.ra.degree\n",
    "mcgurk2015t3['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "#len(mcgurk2015t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = mcgurk2015t4['RA'], dec = mcgurk2015t4['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "mcgurk2015t4['RA1_deg'] = coordconvert.ra.degree\n",
    "mcgurk2015t4['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "\n",
    "#len(mcgurk2015t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here we'll be matching the tables from McGurk to the overarching catalogs\n",
    "\n",
    "the_whills.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,mcgurk2015t2,10)\n",
    "\n",
    "print(len(tmatches))\n",
    "\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; McGurk+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...811...14M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/811/1/14'\n",
    "        the_whills.at[index, 'Notes'] += ' Companion within 3 arcseconds.'\n",
    "        \n",
    "## Now clipping out all Comerford+2013 rows from the matches table\n",
    "#tmatches = tmatches[tmatches['Paper(s)']!='Shi+2014'].reset_index(drop=True)\n",
    "#\n",
    "## Concatenating everything together to generate a master table here\n",
    "#the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "##the_whills.drop(labels=['level_0','index'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# we have to go out to 10'' in match tolerance \\\n",
    "# to get the last object (the rest match within 5'')\n",
    "\n",
    "# I stripped the duplicate rows out of the McGurk table (and just add additional info to a main row)\n",
    "\n",
    "# and I have now visually verified that we have the correct number of matches and that the \\\n",
    "# correct objects have been matched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here we'll be matching the tables from McGurk to the overarching catalogs\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,mcgurk2015t3,13)\n",
    "\n",
    "print(len(tmatches))\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; McGurk+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...811...14M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/811/1/14'\n",
    "        the_whills.at[index, 'Notes'] += ' No companion within 3 arcseconds.'\n",
    "\n",
    "\n",
    "# I've visually checked all of the matches since we have to go out to 13'' in match \\\n",
    "# tolerance to get the last object \n",
    "# verified that this process works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now here we'll be matching the tables from McGurk to the overarching catalogs\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,mcgurk2015t4,5)\n",
    "\n",
    "#len(tmatches)\n",
    "\n",
    "# These all match within 5'', just need to verify that the matching process works at the end\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    #print(\"i:\", i, \"j:\", j)\n",
    "    #print(\"Sep:\", mcgurk2015t4.at[j, 'NIRC2sep(as)'])\n",
    "    #print(\"dV:\", mcgurk2015t4.at[j, 'dV[OIII]'])\n",
    "    the_whills.at[i, 'Sep'] = mcgurk2015t4.at[j, 'NIRC2sep(as)']\n",
    "    the_whills.at[i, 'dV'] = mcgurk2015t4.at[j, 'dV[OIII]']\n",
    "    the_whills.at[i, 'Notes'] += mcgurk2015t4.at[j, 'Notes']\n",
    "\n",
    "# verified that this matching process works properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now loading in the information from Muller-Sanchez+2015:\n",
    "\n",
    "ms2015t1 = pd.read_csv('Tables/Mullersanchez2015/Mullersanchezt1.csv')\n",
    "ms2015t7 = pd.read_csv('Tables/Mullersanchez2015/Mullersanchezt7.csv')\n",
    "\n",
    "ms2015 = pd.concat([ms2015t1,ms2015t7], axis=1)\n",
    "ms2015t4 = pd.read_csv('Tables/Mullersanchez2015/Mullersanchezt4.csv') # These are the confirmed dual AGNs\n",
    "#ms2015t5 = pd.read_csv('Tables/Mullersanchez2015/Mullersanchezt5.csv') # These are the ambiguous cases\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = ms2015['RA'], dec = ms2015['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "ms2015['RA1_deg'] = coordconvert.ra.degree\n",
    "ms2015['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "ms2015['RA2'] = ms2015['RA']\n",
    "ms2015['Dec2'] = ms2015['Dec']\n",
    "\n",
    "ms2015['RA2_deg'] = ms2015['RA1_deg']\n",
    "ms2015['Dec2_deg'] = ms2015['Dec1_deg']\n",
    "\n",
    "\n",
    "#ms2015['Confirmation Method'] = \"-99\"\n",
    "ms2015['Paper(s)'] = \"Muller-Sanchez+2015\"\n",
    "ms2015['BibCode(s)'] = \"2015ApJ...813..103M\"\n",
    "ms2015['DOI(s)'] = \"https://doi.org/10.1088/0004-637X/813/2/103\"\n",
    "\n",
    "#ms2015\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's where we'll be matching the ms2015 tables back against the the_whills and filling in information as \n",
    "\n",
    "ms2015temp = ms2015[ms2015['Origin_Double_Peaks']=='radio jet-driven outflow'].reset_index(drop=True)\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,ms2015temp,5)\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Muller-Sanchez+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...813..103M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/813/2/103'\n",
    "        the_whills.at[index, 'System Type'] = 'Likely Single AGN'\n",
    "        the_whills.at[index, 'Notes'] += 'Muller-Sanchez+2015 find that the origin of the double-peaked lines is likely an jet-driven outflow. '\n",
    "        #tmatches.at[index, 'Sep'] =\n",
    "#print(len(tmatches))\n",
    "#tmatches\n",
    "\n",
    "#for i in the_whills['Paper(s)']:\n",
    "#    if 'Muller-Sanchez+2015' in i:\n",
    "#        print('True')\n",
    "\n",
    "ms2015temp = ms2015[ms2015['Origin_Double_Peaks']=='rotating disk'].reset_index(drop=True)\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,ms2015temp,5)\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Muller-Sanchez+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...813..103M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/813/2/103'\n",
    "        the_whills.at[index, 'System Type'] = 'Likely Single AGN'\n",
    "        the_whills.at[index, 'Notes'] += 'Muller-Sanchez+2015 find that the origin of the double-peaked lines is likely a rotating disk.'\n",
    "        #tmatches.at[index, 'Sep'] =\n",
    "\n",
    "#print(len(tmatches))\n",
    "#for i in the_whills['Paper(s)']:\n",
    "#    if 'Muller-Sanchez+2015' in i:\n",
    "#        print('True')\n",
    "\n",
    "ms2015temp = ms2015[ms2015['Origin_Double_Peaks']=='AGN-driven outflow'].reset_index(drop=True)\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,ms2015temp,15)\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Muller-Sanchez+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...813..103M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/813/2/103'\n",
    "        the_whills.at[index, 'System Type'] = 'Likely Single AGN'\n",
    "        the_whills.at[index, 'Notes'] += 'Muller-Sanchez+2015 find that the origin of the double-peaked lines is likely an AGN driven outflow.'\n",
    "        #tmatches.at[index, 'Sep'] =\n",
    "# we have to go up to 15'' match tolerance to grab the missing object\n",
    "#for i in the_whills['Paper(s)']:\n",
    "#    if 'Muller-Sanchez+2015' in i:\n",
    "#        print('True')       \n",
    "\n",
    "ms2015temp = ms2015[ms2015['Origin_Double_Peaks']=='dual AGNs'].reset_index(drop=True)\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,ms2015temp,10)\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Muller-Sanchez+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...813..103M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/813/2/103'\n",
    "        the_whills.at[index, 'System Type'] = 'Dual AGN'\n",
    "        the_whills.at[index, 'Notes'] += ' '\n",
    "\n",
    "#for i in the_whills['Paper(s)']:\n",
    "#    if 'Muller-Sanchez+2015' in i:\n",
    "#        print('True')  \n",
    "\n",
    "ms2015temp = ms2015[ms2015['Origin_Double_Peaks']=='ambiguous'].reset_index(drop=True)\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,ms2015temp,5)\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if index in idx1:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Muller-Sanchez+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...813..103M' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/813/2/103'\n",
    "        the_whills.at[index, 'Notes'] += 'Muller-Sanchez+2015 find that the origin of the double-peaked emission is still ambiguous.'\n",
    "\n",
    "#for i in the_whills['Paper(s)']:\n",
    "#    if 'Muller-Sanchez+2015' in i:\n",
    "#        print('True')  \n",
    "\n",
    "\n",
    "# verified that all of this matching now works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here w're just adding in some bib info and notes from Villforth+2015\n",
    "\n",
    "#vill2015 = ['J095207.6+255257','J115106.7+471157','J150243.1+111557','J171544.0+600835'] --> this Villforth's origina listing\n",
    "vill2015 = ['J095207.62+255257.2','J115106.69+471157.7','J150243.09+111557.3',] #--> these are the matching objects from the original tables\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in vill2015:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Villforth+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015AJ....149...92V' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-6256/149/3/92'\n",
    "        the_whills.at[index, 'Notes']+=' Villforth+ report clear merger features. Broad predomimantly blue [O III] wings observed. These high velocity outflows are observed only in the campact <5 kpc central regions. Given the speeds the outflows are likely driven by the AGN(s). The galaxies are redder in their central regions.The NLR is extended on sizes up to 50-100 kpc. Clear clumps in the NLRs are apparent.'\n",
    "\n",
    "\n",
    "vill2015 = ['J171544.05+600835.7']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in vill2015:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Villforth+2015'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2015AJ....149...92V' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-6256/149/3/92'\n",
    "        the_whills.at[index, 'Notes']+='Villorth+ report that there are no strong merger features in this system. They rule out that the dual AGN dentified by Comerford+ is the cause of the double-peaked emissioon lines.'\n",
    "\n",
    "# Verified that the matching process here works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills.to_csv('the_whills_beforeLyu2016.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the catalog from Lyu+2016 now...\n",
    "lyu2016 = ((Table.read('Tables/lyu2016/table1.dat', readme = 'Tables/lyu2016/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---'])\n",
    "# Note here that any cells containing '--' in table are going to be replaced by 0's\n",
    "\n",
    "lyu2016['Name'] = lyu2016['SDSS']\n",
    "lyu2016['Name2'] = lyu2016['SDSS']\n",
    "lyu2016['z2'] = lyu2016['z']\n",
    "lyu2016['z1_type'] = \"spec\"\n",
    "lyu2016['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(lyu2016,lyu2016['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = lyu2016['RA'], dec = lyu2016['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "lyu2016['RA1_deg'] = coordconvert.ra.degree\n",
    "lyu2016['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "lyu2016['RA2'] = lyu2016['RA']\n",
    "lyu2016['Dec2'] = lyu2016['Dec']\n",
    "\n",
    "lyu2016['RA2_deg'] = lyu2016['RA1_deg']\n",
    "lyu2016['Dec2_deg'] = lyu2016['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "lyu2016['Equinox'] = \"J2000\"\n",
    "lyu2016['Coordinate_waveband'] = \"Optical\"\n",
    "lyu2016['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "lyu2016['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "lyu2016['Brightness1'] = -100\n",
    "lyu2016['Brightness_band1'] = -100\n",
    "lyu2016['Brightness_type1'] = -100\n",
    "\n",
    "lyu2016['Brightness2'] = -100\n",
    "lyu2016['Brightness_band2'] = -100\n",
    "lyu2016['Brightness_type2'] = -100\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "lyu2016['Sep'] = 2 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 2'' diameter of the SDSS \\\n",
    "# **BOSS** fiber as an upper limit\n",
    "\n",
    "#lyu2016['Sep(kpc)'] = lyu2016['Sep']*((cosmo.arcsec_per_kpc_proper(lyu2016['z']))**(-1))\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#lyu2016['delta_z'] = lyu2016['z']-lyu2016['z2']\n",
    "lyu2016['dV'] = (2.99e+5)*((1+lyu2016['z'])**2 - (1+lyu2016['z2'])**2)/((1+lyu2016['z'])**2+(1+lyu2016['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "lyu2016['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "lyu2016['Confirmation Method'] = \"-99\"\n",
    "lyu2016['Paper(s)'] = \"Lyu+2016\"\n",
    "lyu2016['BibCode(s)'] = \"2016MNRAS.463...24L\"\n",
    "lyu2016['DOI(s)'] = \"https://doi.org/10.1093/mnras/stw1945\"\n",
    "\n",
    "lyu2016['Notes'] = ''\n",
    "\n",
    "lyu2016.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = lyu2016['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,lyu2016,15)\n",
    "\n",
    "print(len(tmatches))\n",
    "# There are 2 matches up to this point between the_whills\n",
    "# J023658.06+024217.9\n",
    "# J131642.90+175332.5\n",
    "\n",
    "#tmatches\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in tmatches.iterrows():\n",
    "    if row['Table_flag']!='Table2':\n",
    "        tmatches.at[index, 'Paper(s)'] += ' ; Lyu+2016'\n",
    "        tmatches.at[index, 'BibCode(s)'] += ' ; 2016MNRAS.463...24L' \n",
    "        tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stw1945'\n",
    "        tmatches.at[index, 'Notes'] += ' Lyu+2016 also selected this object with BOSS.'\n",
    "        \n",
    "# Now clipping out all Comerford+2013 rows from the matches table\n",
    "tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "#\n",
    "## Concatenating everything together to generate a master table here\n",
    "the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "#the_whills.drop(labels=['level_0','index'], axis=1, inplace=True)\n",
    "\n",
    "# this matching process 'works' but the two matches end up making duplicate entries for some reason\n",
    "# I do not understand why the matching process is not working properly here, where it worked perfectly for every other\n",
    "# cell previously\n",
    "\n",
    "# this cell is verified now\n",
    "\n",
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I'm adding some code to manually strip out the duplicate two rows\n",
    "\n",
    "rows_to_drop = the_whills[(the_whills['Name'] == 'J023658.06+024217.9') & (the_whills['Paper(s)'] == 'Shi+2014')].index\n",
    "\n",
    "# Drop the rows\n",
    "the_whills.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "rows_to_drop = the_whills[(the_whills['Name'] == 'J131642.90+175332.5') & (the_whills['Paper(s)'] == 'Smith+2010')].index\n",
    "\n",
    "# Drop the rows\n",
    "the_whills.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "the_whills.reset_index(drop=True, inplace=True)\n",
    "# this cell is verified and removes the unusual duplicates from before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills.to_csv('the_whills_afterLyu_beforeyuan.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_matches(names):\n",
    "    name_dict = {}\n",
    "    matches = []\n",
    "\n",
    "    for name in names:\n",
    "        first_6_chars = name[:7]\n",
    "        if first_6_chars in name_dict:\n",
    "            matches.append((name_dict[first_6_chars], name))\n",
    "        else:\n",
    "            name_dict[first_6_chars] = name\n",
    "\n",
    "    return matches\n",
    "\n",
    "names = the_whills['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from astropy import units as u\n",
    "# from astropy.coordinates import SkyCoord\n",
    "\n",
    "# def angular_separation_arcsec(coord1, coord2):\n",
    "#     \"\"\"\n",
    "#     Calculate angular separation between two sexagesimal coordinates.\n",
    "    \n",
    "#     Parameters:\n",
    "#     coord1, coord2: tuples containing RA and Dec in the format ('13:52:07.73', '+05:25:55.8')\n",
    "    \n",
    "#     Returns:\n",
    "#     Angular separation in arcseconds.\n",
    "#     \"\"\"\n",
    "#     c1 = SkyCoord(coord1[0], coord1[1], unit=(u.hourangle, u.deg))\n",
    "#     c2 = SkyCoord(coord2[0], coord2[1], unit=(u.hourangle, u.deg))\n",
    "#     sep = c1.separation(c2)\n",
    "#     return sep.arcsecond\n",
    "\n",
    "# # Example usage:\n",
    "# coord1 = ('13:52:07.0', '+05:25:55.0')\n",
    "# coord2 = ('13:52:07.73', '+05:25:55.8')\n",
    "# print(f\"Angular separation: {angular_separation(coord1, coord2)} arcseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, row in the_whills.iterrows():\n",
    "#    if index in idx1:\n",
    "#        the_whills.at[index, 'Paper(s)'] += ' ; Muller-Sanchez+2015'\n",
    "#        the_whills.at[index, 'BibCode(s)'] += ' ; 2015ApJ...813..103M' \n",
    "#        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1088/0004-637X/813/2/103'\n",
    "#        the_whills.at[index, 'System Type'] = 'Likely Single AGN'\n",
    "#        the_whills.at[index, 'Notes'] += 'Muller-Sanchez+2015 find that the origin of the double-peaked lines is likely a rotating disk.'\n",
    "#        #tmatches.at[index, 'Sep'] =\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills = pd.read_csv('the_whills_afterLyu_beforeyuan.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Yuan+2016 (and by extension, the Reyes+ and Zakamska+ tables)\n",
    "from astropy.io import ascii\n",
    "\n",
    "yuan2016t1 = (ascii.read('Tables/yuan2016/Type2s.tbl').to_pandas())#.drop(columns=['---'])\n",
    "yuan2016t2 = (ascii.read('Tables/yuan2016/Type2s_kinematics.tbl').to_pandas())#.drop(columns=['---'])\n",
    "\n",
    "yuan2016t2 = pd.merge(yuan2016t1, yuan2016t2, on=['plate', 'fiber', 'mjd'], how='inner')\n",
    "\n",
    "#yuan2016t2.loc[yuan2016t2['Unique']!='unique', 'Name'] = yuan2016['Unique']\n",
    "\n",
    "yuan2016t3 = (ascii.read('Tables/yuan2016/Reyes_kinematics.tbl').to_pandas())#.drop(columns=['---'])\n",
    "\n",
    "yuan2016t2 = yuan2016t2[yuan2016t2['dp']!=0]\n",
    "print(len(yuan2016t2['unique']!='unique'))\n",
    "yuan2016t3 = yuan2016t3[yuan2016t3['dp']!=0]\n",
    "#I need to go through and somehow get the SDSS BOSS designations\n",
    "\n",
    "yuan2016t3 = yuan2016t3.reset_index(drop=True)\n",
    "yuan2016t2 = yuan2016t2.reset_index(drop=True)\n",
    "\n",
    "print(len(yuan2016t2))\n",
    "print(len(yuan2016t3))\n",
    "\n",
    "yuan2016 = pd.concat([yuan2016t2,yuan2016t3])\n",
    "\n",
    "# Convert Ra and Dec from degrees to sexagesimal format\n",
    "coords = SkyCoord(ra=yuan2016['ra']*u.degree, dec=yuan2016['dec']*u.degree, frame='icrs')\n",
    "yuan2016['ra_sexagesimal'] = coords.ra.to_string(u.hour, sep=':', precision=2)\n",
    "yuan2016['dec_sexagesimal'] = coords.dec.to_string(u.deg, sep=':', precision=2)\n",
    "\n",
    "# Convert RA and Dec to the desired format\n",
    "ra_format = coords.ra.to_string(unit=u.hour, sep='', precision=2, pad=True)\n",
    "dec_format = coords.dec.to_string(unit=u.deg, sep='', precision=2, alwayssign=True, pad=True)\n",
    "\n",
    "# Concatenate to form J_format using a loop\n",
    "yuan2016['J_format'] = [\"J\" + ra + dec for ra, dec in zip(ra_format, dec_format)]\n",
    "\n",
    "\n",
    "# here we're droppping the duplicate columns from yuan's tables:\n",
    "yuan2016.drop_duplicates(subset='J_format', keep='last', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuan2016['Name'] = yuan2016['J_format'] # str(yuan2016['plate']+yuan2016['fiber']+yuan2016['mjd'])\n",
    "yuan2016['Name2'] = yuan2016['J_format'] # str(yuan2016['plate']+yuan2016['fiber']+yuan2016['mjd'])\n",
    "yuan2016['z2'] = yuan2016['z']\n",
    "yuan2016['z1_type'] = \"spec\"\n",
    "yuan2016['z2_type'] = \"spec\"\n",
    "\n",
    "# Converting the coordinates\n",
    "#coordconvert = SkyCoord(ra = yuan2016['RA'], dec = yuan2016['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "yuan2016['RA1_deg'] = yuan2016['ra']#coordconvert.ra.degree\n",
    "yuan2016['Dec1_deg'] = yuan2016['dec']#coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "yuan2016['RA2'] = yuan2016['ra']\n",
    "yuan2016['Dec2'] = yuan2016['dec']\n",
    "\n",
    "yuan2016['RA2_deg'] = yuan2016['RA1_deg']\n",
    "yuan2016['Dec2_deg'] = yuan2016['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "yuan2016['Equinox'] = \"J2000\"\n",
    "yuan2016['Coordinate_waveband'] = \"Optical\"\n",
    "yuan2016['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "yuan2016['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "yuan2016['Brightness1'] = -100\n",
    "yuan2016['Brightness_band1'] = -100\n",
    "yuan2016['Brightness_type1'] = -100\n",
    "\n",
    "yuan2016['Brightness2'] = -100\n",
    "yuan2016['Brightness_band2'] = -100\n",
    "yuan2016['Brightness_type2'] = -100\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "yuan2016['Sep'] = 2 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 2'' diameter of the SDSS \\\n",
    "# **BOSS** fiber as an upper limit\n",
    "\n",
    "#yuan2016['Sep(kpc)'] = yuan2016['Sep']*((cosmo.arcsec_per_kpc_proper(yuan2016['z']))**(-1))\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#yuan2016['delta_z'] = yuan2016['z']-yuan2016['z2']\n",
    "yuan2016['dV'] = (2.99e+5)*((1+yuan2016['z'])**2 - (1+yuan2016['z2'])**2)/((1+yuan2016['z'])**2+(1+yuan2016['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "yuan2016['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "yuan2016['Confirmation Method'] = \"-99\"\n",
    "yuan2016['Paper(s)'] = \"Yuan+2016\"\n",
    "yuan2016['BibCode(s)'] = \"2016MNRAS.462.1603Y\"\n",
    "yuan2016['DOI(s)'] = \"https://doi.org/10.1093/mnras/stw1747\"\n",
    "\n",
    "# Make sure to go back and add the citations for the papers they ask to be cited in addition to Yuan+.\n",
    "\n",
    "yuan2016['Notes'] = ''\n",
    "\n",
    "# here we're dropping some duplicates that weren't exactly identical in Yuan's tables\n",
    "rows_to_drop = yuan2016[(yuan2016['Name'] == 'J010607.06+000927.47') | (yuan2016['Name'] == 'J090247.04+012028.39') | (yuan2016['Name'] == 'J090247.04+012028.39') | (yuan2016['Name'] == 'J092152.56+515348.12') | (yuan2016['Name'] == 'J112319.20+302825.32')].index\n",
    "\n",
    "# Drop the rows\n",
    "yuan2016.drop(rows_to_drop, inplace=True)\n",
    "yuan2016.reset_index(drop='True', inplace=True)\n",
    "# verified that this drops the unusual duplicates\n",
    "\n",
    "\n",
    "#yuan2016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = yuan2016['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yuan2016.to_csv('yuantest.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(the_whills['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills.reset_index(drop=True, inplace=True)\n",
    "#yuan2016.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,yuan2016,10)\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Yuan+2016'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2016MNRAS.462.1603Y' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stw1747'\n",
    "\n",
    "print(len(idx2))\n",
    "# dropping the matching indices from the yuan2016 table\n",
    "yuan2016.drop(idx2, axis=0, inplace=True)\n",
    "yuan2016.reset_index(drop=True, inplace=True)\n",
    "\n",
    "the_whills = pd.concat([the_whills,yuan2016])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills = the_whills_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,yuan2016,10)\n",
    "#\n",
    "#print(len(idx2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = the_whills['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,yuan2016,10)\n",
    "\n",
    "# # There are a lot of matches between Lyu+ and Yuan+\n",
    "# #tmatches\n",
    "# # Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "# for index, row in tmatches.iterrows():\n",
    "#     if row['Paper(s)']!='Yuan+2016':\n",
    "#         tmatches.at[index, 'Paper(s)'] += ' ; Yuan+2016'\n",
    "#         tmatches.at[index, 'BibCode(s)'] += ' ; 2016MNRAS.462.1603Y' \n",
    "#         tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stw1747'\n",
    "#         tmatches.at[index, 'Notes'] += ' Yuan+ also selected this target.'\n",
    "        \n",
    "# # Now clipping out all Comerford+2013 rows from the matches table\n",
    "# tmatches = tmatches[tmatches['Paper(s)']!='Yuan+2016'].reset_index(drop=True)\n",
    "\n",
    "# # Create a boolean mask where each value is True if the 'Name' in tunique is also in tmatches\n",
    "# mask = tunique['Name'].isin(tmatches['Name']) #shamelessly asked chatgpt to quickly write this because I was too lazy to go find my old script\n",
    "# tunique = tunique[(~mask) & (tunique['Paper(s)']=='Yuan+2016')]\n",
    "# print(len(tmatches))\n",
    "# print(len(tunique))\n",
    "\n",
    "# ### Concatenating everything together to generate a master table here\n",
    "# the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "# ###the_whills.drop(labels=['level_0','index'], axis=1, inplace=True)\n",
    "# print(len(tmatches))\n",
    "\n",
    "# # verified that this now appears to be functioning properly\n",
    "# # I had to add some code that strips out objects from tunique that are actually within tmatches...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = the_whills['Name'].to_list()\n",
    "# matched_pairs = find_matches(names)\n",
    "\n",
    "# for pair in matched_pairs:\n",
    "#     print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the_whills['Name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the_whills['Name'].to_csv('check_yuan_dups.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(the_whills['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_matches(names):\n",
    "#     name_dict = {}\n",
    "#     matches = []\n",
    "\n",
    "#     for name in names:\n",
    "#         first_6_chars = name[:7]\n",
    "#         if first_6_chars in name_dict:\n",
    "#             matches.append((name_dict[first_6_chars], name))\n",
    "#         else:\n",
    "#             name_dict[first_6_chars] = name\n",
    "\n",
    "#     return matches\n",
    "\n",
    "# names = the_whills['Name'].to_list()\n",
    "# matched_pairs = find_matches(names)\n",
    "\n",
    "# for pair in matched_pairs:\n",
    "#     print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're adding the citation information and relevant designations/other info from individual papers that \\ \n",
    "# have discuss the double-peaked sources\n",
    "\n",
    "# First Liu+2018  (dual AGN candidate in a dwarf galaxy)\n",
    "liu2018obs = ['J092455.24+051052.0']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in liu2018obs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'dV'] = 437 #km s^-1 --> This is the from the OIII lines but is consistent with the Hbeta lines within the uncertainties \n",
    "        the_whills.at[index, 'Sep'] = 0.4\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Liu+2018 '\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2018ApJ...862...29L' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aac9cb'\n",
    "        the_whills.at[index, 'Notes'] += 'Liu+ resolve two stellar bulges and two NLRs in this system using HST. The HST data disfavor a biconical outflow scenario and they favor a dual AGN scenario; given the separation they cannot rule out a single AGN ionizing both nuclei. Liu+ finds the separation to be slightly smaller for the OIII emitting regions than the host stellar bulges.'\n",
    "\n",
    "# Now Severgnini+2018 and Liu+2020\n",
    "severobs = ['J085512+642345']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in severobs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Severgnini+2018 ; Liu+2020'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2018MNRAS.479.3804S ; 2020ApJ...896..122L' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aaab47 ; https://doi.org/10.3847/1538-4357/ab952d '\n",
    "        the_whills.at[index, 'Notes'] += 'Severgnini rule out strong radio jets but cannot rule out faint radio jets. They note that the velocity offsets are fully consistent with the rotation velocities measured in nearby galaxies and therefore the offsets could be explained by kinematics in a single NLR. They detect X-ray flux variability (periodicity of ~25 months) and argue it is intrinsic flux variations rather than varying obscuration. They also report a tentive double-peaked iron line due either to rational in an accretiondisk or a binary AGN. Under the binary hypothesis the double peaked optical lines would be due to rotationin the galaxy and not the binary. By measuring the power spectrum of this object Liu+2020 rejected the 25 month periodicity that Severgnini found. Severgnini did not fit the power spectrum.'\n",
    "\n",
    "# And now we need to add in the new information from Serafinelli+2020\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in severobs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Serafinelli+2020'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2020ApJ...902...10S' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/abb3c3'\n",
    "        the_whills.at[index, 'Notes'] += ' This object was excluded from the Serafinelli+ due to the g-test selection criteria. Serafinelli+ show the power spectrum density is consistent with white noise but when they perform a weighted sinusoid fit they find a period of ~26.3 months. Whenthey consider the 123 month data they find a period of 26.0 months and claim to reject the null at 3.4sigma. This period is consistent with that proposed by Severgnini+.'\n",
    "\n",
    "# Verified that the matching process works here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(the_whills[the_whills['Name']=='J092455.24+051052.0']['Paper(s)'])\n",
    "#print(the_whills[the_whills['Name']=='J085512+642345']['Paper(s)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# When we match/discuss J0841+0101, make sure to mention that Keel flagged that as having two AGN\n",
    "# Also bring up Keel+2019 when discuss J1354, which was selected by Liu+2011\n",
    "#\n",
    "# Keel+2019,2019MNRAS.483.4847K,https://doi.org/10.1093/mnras/sty3332\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here adding in citations for Rubinur+ and Das+\n",
    "# These papers discussed the double-peaked AGN J120320+131931 identified by Wang+09\n",
    "\n",
    "rubobs = ['J120320+131931']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in rubobs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Rubinur+2017 ; Das+2018'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2017MNRAS.465.4772R ; 2018BSRSL..87..299D' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stw2981 ; https://ui.adsabs.harvard.edu/abs/2018BSRSL..87..299D/abstract'\n",
    "        the_whills.at[index, 'Notes'] += ' Rubinur+ report distinct radio hot spots on either side of the nucleus.\\\n",
    "        They detect an S-shaped morphology 6 GHz as well as at 8.5 GHz and 11.5 GHz. They argue this object is\\\n",
    "        a CSS/CSO. They argue the observed jet precession can be explained by a binary a dual or a single AGN\\\n",
    "        with a warped acretinon disk. Das+ report the detection of an S-shaped radio morphology at both 8.5\\\n",
    "        and 11.5 GHz. They argue for a precessing jet that could be due to a binary a dual or a single AGN \\\n",
    "        with tilted accretion disk. It is unclear if the results discussed in Das+ are actually the results \\\n",
    "        of Rubinur+.'\n",
    "\n",
    "#### EVIDENTLY MCGURK+2015 AND FU+12 ALSO LOOKED AT THIS!\n",
    "\n",
    "\n",
    "# Now to add the relevant info from Rubinur+2018:\n",
    "rubobs = ['J120320+131931']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in rubobs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Rubinur+2018'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2018JApA...39....8R' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1007/s12036-018-9512-y'\n",
    "        the_whills.at[index, 'Notes'] += 'Rubinur+2018 resolve the two components previously reported in the radio structure of this object and discover they are hot spots of an S-shaped radio jet. They conclude this is likely a COS but do not rule out a gravitationally bound binary to explain the precession. '\n",
    "\n",
    "rubobs = ['J161708.95+222628.0']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in rubobs:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Rubinur+2018'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2018JApA...39....8R' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1007/s12036-018-9512-y'\n",
    "        the_whills.at[index, 'Notes'] = ' '\n",
    "        the_whills.at[index, 'Notes'] += 'Rubinur+2018 find this is a merging system with two optical nuclei and two radio sources coincident with teh nuclei at both 6 and 15 GHz in EVLA imaging. Separation is ~5.6 kpc. Primary is consistent in optical with an AGN but secondary is consistent with SF. The radio emission in secondary is consistent with SF. '\n",
    "\n",
    "# Rubinur+2018 do not provide any further details about their objects....\n",
    "\n",
    "# Verified that the matching process here works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(the_whills[the_whills['Name']=='J120320+131931']['Paper(s)'])\n",
    "#print(the_whills[the_whills['Name']=='J161708.95+222628.0']['Paper(s)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are loading in the information from Comerford+2018\n",
    "\n",
    "# WE NEED TO GO BACK IN AND ADD THE ADDITIONAL INFORMATION FOR THE 8 PROMISING DUAL CANDIDATES NOT INCLUDED IN\\\n",
    "# A CDS TABLE\n",
    "comerford2018t1 = ((Table.read('Tables/Comerford2018/table1.dat', readme = 'Tables/Comerford2018/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---'])\n",
    "comerford2018t2 = ((Table.read('Tables/Comerford2018/table2.dat', readme = 'Tables/Comerford2018/ReadMe', format='ascii.cds')).to_pandas())#.drop(columns=['---'])\n",
    "\n",
    "\n",
    "secondary_objs = comerford2018t2.iloc[1::2]#.reset_index(drop=True)\n",
    "# Get the indices of rows in every_other_df\n",
    "secondary_objs_indices = secondary_objs.index\n",
    "# Drop rows from original_df\n",
    "comerford2018t2 = comerford2018t2.drop(secondary_objs_indices).reset_index(drop=True)\n",
    "secondary_objs = secondary_objs.reset_index(drop=True)\n",
    "# Renaming columns for the secondary objects\n",
    "secondary_objs.rename(columns={'Name':'Name_2', 'PAobs':'PAobs_2', 'NR':'NR_2', 'Vr':'Vr_2', 'E_Vr':'E_Vr_2',\\\n",
    "                               'e_Vr':'e_Vr_2', 'sigma1':'sigma1_2', 'E_sigma1':'E_sigma1_2',\\\n",
    "                               'e_sigma1':'e_sigma1_2', 'sigma2':'sigma2_2', 'E_sigma2':'E_sigma2_2',\\\n",
    "                               'e_sigma2':'e_sigma2_2', 'PAgal':'PAgal_2', 'PAO3':'PAO3_2', 'e_PAO3':'e_PAO3_2',\\\n",
    "                               'A':'A_2', 'Class':'Class_2'}, inplace=True)\n",
    "comerford2018t2.rename(columns={'Name':'Name_1', 'PAobs':'PAobs_1', 'NR':'NR_1', 'Vr':'Vr_1', 'E_Vr':'E_Vr_1',\\\n",
    "                         'e_Vr':'e_Vr_1', 'sigma1':'sigma1_1', 'E_sigma1':'E_sigma1_1', 'e_sigma1':'e_sigma1_1',\\\n",
    "                         'sigma2':'sigma2_1', 'E_sigma2':'E_sigma2_1', 'e_sigma2':'e_sigma2_1', 'PAgal':'PAgal_1',\\\n",
    "                         'PAO3':'PAO3_1', 'e_PAO3':'e_PAO3_1', 'A':'A_1', 'Class':'Class_1'}, inplace=True)\n",
    "\n",
    "comerford2018t2 = pd.concat([comerford2018t2,secondary_objs], axis=1).reset_index(drop=True)\n",
    "comerford2018 = pd.concat([comerford2018t1,comerford2018t2], axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "comerford2018['Name'] = comerford2018['SDSS']\n",
    "comerford2018['Name2'] = comerford2018['SDSS']\n",
    "#comerford2018['z1'] = comerford2018['z']\n",
    "#comerford2018['z2'] = comerford2018['z']\n",
    "#comerford2018['z1_type'] = \"spec\"\n",
    "#comerford2018['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(comerford2018,comerford2018['SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = comerford2018['RA'], dec = comerford2018['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "comerford2018['RA1_deg'] = coordconvert.ra.degree\n",
    "comerford2018['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "comerford2018['RA2'] = comerford2018['RA']\n",
    "comerford2018['Dec2'] = comerford2018['Dec']\n",
    "\n",
    "comerford2018['RA2_deg'] = comerford2018['RA1_deg']\n",
    "comerford2018['Dec2_deg'] = comerford2018['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "comerford2018['Equinox'] = \"J2000\"\n",
    "comerford2018['Coordinate_waveband'] = \"Optical\"\n",
    "comerford2018['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "comerford2018['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "comerford2018['Brightness1'] = \"-99\"\n",
    "comerford2018['Brightness_band1'] = \"-99\"\n",
    "comerford2018['Brightness_type1'] = \"-99\"\n",
    "\n",
    "comerford2018['Brightness2'] = \"-99\"\n",
    "comerford2018['Brightness_band2'] = \"-99\"\n",
    "comerford2018['Brightness_type2'] = \"-99\"\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "comerford2018['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 3'' diameter of the SDSS \\\n",
    "# fiber as an upper limit\n",
    "\n",
    "#*******\n",
    "#******************the cosmo arcsec to kpc command needs to be fixed! It is deprecated apparently!\n",
    "#comerford2018['Sep(kpc)'] = comerford2018['Sep']*((cosmo.arcsec_per_kpc_proper(comerford2018['z']))**(-1))\n",
    "#************\n",
    "#*******\n",
    "\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#comerford2018['delta_z'] = comerford2018['z1']-comerford2018['z2']\n",
    "#comerford2018['dV'] = (2.99e+5)*((1+comerford2018['z1'])**2 - (1+comerford2018['z2'])**2)/((1+comerford2018['z1'])**2+(1+comerford2018['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "comerford2018['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "comerford2018['Confirmation Method'] = \"-99\"\n",
    "comerford2018['Paper(s)'] = \"Comerford+2018\"\n",
    "comerford2018['BibCode(s)'] = \"2018ApJ...867...66C\"\n",
    "comerford2018['DOI(s)'] = \"https://doi.org/10.3847/1538-4357/aae2b4\"\n",
    "\n",
    "#comerford2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comerford2018t3 = pd.read_csv('Tables/Comerford2018/comerford2018t3.csv', sep=',')\n",
    "comerford2018t4 = pd.read_csv('Tables/Comerford2018/comerford2018t4.csv', sep=',')\n",
    "\n",
    "#comerford2018 = pd.concat([comerford2018,comerford2018t3], axis=1, names=['Name1','SDSS'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = comerford2018t3['RA'], dec = comerford2018t3['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "comerford2018t3['RA1_deg'] = coordconvert.ra.degree\n",
    "comerford2018t3['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "\n",
    "#comerford2018t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implementing a matching process to see how many in the Comerford+2018 sample are in the original table...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,comerford2018,14)\n",
    "\n",
    "# we should not need to go up 14'' to match everything, but evidently that's the only way to get it to work\n",
    "# (5'' gets 180, but we should see 190 matches, 95 for the overarching table and 95 from Comerford+2018)\n",
    "# Using 14'', we get everything, and I've visually verified that the target names match\n",
    "\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "#for index, row in tmatches.iterrows():\n",
    "#    if row['Table_flag']!='Table2':\n",
    "#        tmatches.at[index, 'Paper(s)'] += ' ; Comerford+2018'\n",
    "#        tmatches.at[index, 'BibCode(s)'] += ' ; 2018ApJ...867...66C' \n",
    "#        tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aae2b4'\n",
    "#        tmatches.at[index, 'Notes'] += ' Comerford+2018 studied the kinematics that give rise to the double-peaks.'\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Comerford+2018'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2018ApJ...867...66C' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/aae2b4'\n",
    "    the_whills.at[i, 'Notes'] += ' Comerford+2018 studied the kinematics that give rise to the double-peaks.'\n",
    "\n",
    "## Now clipping out all Comerford+2013 rows from the matches table\n",
    "#tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "##\n",
    "### Concatenating everything together to generate a master table here\n",
    "#the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "###the_whills.drop(labels=['level_0','index'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# verified that this process works\n",
    "\n",
    "# *****\n",
    "# BUT WE NEED TO COME BACK AND ADD IN ANGULAR SEPARATION AND OTHER INFO FROM COMERFORD'S TABLE!!!\n",
    "# *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = the_whills['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implementing a matching process to see how many in the Comerford+2018 sample are in the original table...\n",
    "\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,comerford2018t3,5)\n",
    "\n",
    "# we should not need to go up 14'' to match everything, but evidently that's the only way to get it to work\n",
    "# (5'' gets 180, but we should see 190 matches, 95 for the overarching table and 95 from Comerford+2018)\n",
    "# Using 14'', we get everything, and I've visually verified that the target names match\n",
    "\n",
    "len(tmatches)\n",
    "\n",
    "## Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "#for index, row in tmatches.iterrows():\n",
    "#    if row['Table_flag']!='Table2':\n",
    "#        tmatches.at[index, 'Notes'] += ' Comerford+2018 found a companion within 30 kpc that also shows optical evidence of being an AGN. This is therefore a stronger case for dual AGN but not due at all to the double-peaked emission lines.'\n",
    "    \n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Notes'] += ' Comerford+2018 found a companion within 30 kpc that also shows optical evidence of being an AGN. This is therefore a stronger case for dual AGN but not due at all to the double-peaked emission lines.'\n",
    "\n",
    "\n",
    "## Now clipping out all Comerford+2013 rows from the matches table\n",
    "#tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "##\n",
    "### Concatenating everything together to generate a master table here\n",
    "#the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# verified that this process works\n",
    "\n",
    "# *****\n",
    "# BUT WE NEED TO COME BACK AND ADD IN ANGULAR SEPARATION AND OTHER INFO FROM COMERFORD'S TABLE!!!\n",
    "# *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tmatches['Name'])#.to_list()\n",
    "\n",
    "# J000656.85+154847.9 missing in matches but it is in the the_whills... not sure why we can't match\n",
    "# and it superficially looks like we got all matches because J112659.54+294442.8 gets matches twice for some reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Wang 2019\n",
    "\n",
    "# Wang+2019 did not publish their catalog in their paper or as a separate CDS catalog. I will request it for \\\n",
    "# inclusion in the next veraion but I am not delaying the curent version for that.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we're loading in RUbinur+2019\n",
    "# Rubinur+2019 did NOT include the previously used designations from the prior works and instead used IAU \\\n",
    "# designations. For this reason we'll need to match by RA and Dec\n",
    "\n",
    "rubinur2019 = pd.read_csv('Tables/Rubinur2019/Rubinur2019.csv', sep=',')\n",
    "\n",
    "\n",
    "coordconvert = SkyCoord(ra = rubinur2019['RA'], dec = rubinur2019['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "rubinur2019['RA1_deg'] = coordconvert.ra.degree\n",
    "rubinur2019['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "rubinur2019['RA2'] = rubinur2019['RA']\n",
    "rubinur2019['Dec2'] = rubinur2019['Dec']\n",
    "\n",
    "rubinur2019['RA2_deg'] = rubinur2019['RA1_deg']\n",
    "rubinur2019['Dec2_deg'] = rubinur2019['Dec1_deg']\n",
    "\n",
    "\n",
    "#rubinur2019['Confirmation Method'] = \"-99\"\n",
    "rubinur2019['Paper(s)'] = \"Rubinur+2019\"\n",
    "rubinur2019['BibCode(s)'] = \"2019MNRAS.484.4933R\"\n",
    "rubinur2019['DOI(s)'] = \"https://doi.org/10.1093/mnras/stz334\"\n",
    "\n",
    "##rubobs = ['J120320+131931']\n",
    "#for index, row in the_whills.iterrows():\n",
    "#    if row['Name'] in rubobs:\n",
    "#        #print('True')\n",
    "#        the_whills.at[index, 'Paper(s)'] += ' ; Rubinur+2018 '\n",
    "#        the_whills.at[index, 'BibCode(s)'] += ' ; 2018JApA...39....8R ' \n",
    "#        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1007/s12036-018-9512-y'\n",
    "#        the_whills.at[index, 'Notes'] += 'Rubinur+2018 resolve the two components previously reported in the radio structure of this object and discover they are hot spots of an S-shaped radio jet. They conclude this is likely a COS but do not rule out a gravitationally bound binary to explain the precession. '\n",
    "#\n",
    "\n",
    "# Rubinur+ reports on two cmpact radio sources in the following systems:\n",
    "# SDSS J100602.13+071130.9 --> They argue this is a dual based on the two radio sources and the BPT line ratios \n",
    "# SDSS J135558.08+001530.6 --> dual AGN candidate but the second core does not coincide with an optical nucleus. Swcond nucleus might be SF.\n",
    "# 2MASX J16170895+2226279 --> dual candidate\n",
    "\n",
    "# Z-shaped radio morphology here: SDSS J110215.68+290725.2\n",
    "# Two steep spectrum components in this system: 2MASX J14454130+3341080. Could be dual or a single core-jet\n",
    "# Two X-ray sources  in Chandra but only one radio source in  2MASX J09120164+5320369\n",
    "\n",
    "# The remainder were single detecteds, or in the case  of 2MASX J15001769+1051100, not detected.\n",
    "\n",
    "# Rubinur+ did not include distinct coordinates to the radio sources \n",
    "\n",
    "rubinur2019\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,rubinur2019,10)\n",
    "\n",
    "# This now matches all objects from Ge+12 that I'm including\n",
    "# Three objects from Rubinur were emphatically NOT double-peaked AGNs despite them claiming to select double-\\\n",
    "# peaked AGNs from Ge+. One was a SF-SF double, another was ambiguous, and the third was a Type II-SF double.\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Sep'] = rubinur2019.at[j, 'Radio_core_separation_as']\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Rubinur+2019'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2019MNRAS.484.4933R' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stz334'\n",
    "    the_whills.at[i, 'Notes'] += str(rubinur2019.at[j, 'Notes'])\n",
    "\n",
    "the_whills.loc[the_whills['Name']=='J100602.14+071131.0', 'System Type'] = 'Dual AGN'    \n",
    "\n",
    "#len(tmatches)\n",
    "\n",
    "#the_whills\n",
    "\n",
    "# verified that this matching process works and the correct notes are added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in the double peaked sources from Kim+2020\n",
    "\n",
    "kim2020 = pd.read_csv('Tables/Kim2020/Kim_2020.csv', sep=',')\n",
    "\n",
    "# THERE IS ONE OVERLAPPIING SOURCE BETWEEN THIS SAMPLE AND SMITH+2010\n",
    "\n",
    "# MAKE SURE TO ADD THESE NOTES IN DOWN BELOW!!!!!\n",
    "# Kim report double radio cores in J122313.21+540906.5 and possibly in J141041.50+223337.0\n",
    "#J 080544.13+113040.2, J123557.86+582122.9, and J125016.21+045745.0 show double nuclei with 3''\n",
    "# J123915.40+531414.6 shows a double optical nucleus and companion-like object to the northeast\n",
    "\n",
    "kim2020['Name'] = kim2020['Name(SDSS)']\n",
    "kim2020['Name2'] = kim2020['Name(SDSS)']\n",
    "kim2020['z2'] = kim2020['z']\n",
    "kim2020['z1_type'] = \"spec\"\n",
    "kim2020['z2_type'] = \"spec\"\n",
    "\n",
    "# Now converting the naming convention to RA and Dec and adding some informative columns\n",
    "name_to_coords(kim2020,kim2020['Name'])\n",
    "\n",
    "# Converting the coordinates\n",
    "coordconvert = SkyCoord(ra = kim2020['RA'], dec = kim2020['Dec'], frame='icrs', unit = (u.hourangle, u.deg))\n",
    "kim2020['RA1_deg'] = coordconvert.ra.degree\n",
    "kim2020['Dec1_deg'] = coordconvert.dec.degree\n",
    "\n",
    "# Adding in a second set of coordinates for the 'secondary'\n",
    "kim2020['RA2'] = kim2020['RA']\n",
    "kim2020['Dec2'] = kim2020['Dec']\n",
    "\n",
    "kim2020['RA2_deg'] = kim2020['RA1_deg']\n",
    "kim2020['Dec2_deg'] = kim2020['Dec1_deg']\n",
    "\n",
    "# Adding details about the coordinates\n",
    "kim2020['Equinox'] = \"J2000\"\n",
    "kim2020['Coordinate_waveband'] = \"Optical\"\n",
    "kim2020['Coordinate_Source'] = \"SDSS\"\n",
    "\n",
    "kim2020['System Type'] = 'Dual AGN Candidate'\n",
    "\n",
    "# Adding in some columns that we'll population via a Simbad or Ned search later\n",
    "kim2020['Brightness1'] = -100\n",
    "kim2020['Brightness_band1'] = -100\n",
    "kim2020['Brightness_type1'] = -100\n",
    "\n",
    "kim2020['Brightness2'] = -100\n",
    "kim2020['Brightness_band2'] = -100\n",
    "kim2020['Brightness_type2'] = -100\n",
    "\n",
    "# Adding in a column to denote the system separation as '-1' which I will take in this case to mean that it is \\\n",
    "# of order ~1 kpc or less, but is not currently determined.\n",
    "kim2020['Sep'] = 3 # arcseconds\n",
    "# Since these are candidates and we do not have a measure of separation, we'll use the 2'' diameter of the SDSS \\\n",
    "# **BOSS** fiber as an upper limit\n",
    "\n",
    "#kim2020['Sep(kpc)'] = kim2020['Sep']*((cosmo.arcsec_per_kpc_proper(kim2020['z']))**(-1))\n",
    "\n",
    "# For the projected separation, we'll use the upper limit of 3'' to calculate an upper limit in units of kpc\n",
    "#kim2020['delta_z'] = kim2020['z']-kim2020['z2']\n",
    "#kim2020['dV'] = (2.99e+5)*((1+kim2020['z'])**2 - (1+kim2020['z2'])**2)/((1+kim2020['z'])**2+(1+kim2020['z2'])**2)\n",
    "# dV will be zero until we include follow-up observations that show separate redshifts\n",
    "\n",
    "# Adding information about the paper and the selection method\n",
    "kim2020['Selection Method'] = \"Double-Peaked Optical Spectroscopic Emission Lines\" #DPSELs\n",
    "kim2020['Confirmation Method'] = \"-99\"\n",
    "kim2020['Paper(s)'] = \"Kim+2020\"\n",
    "kim2020['BibCode(s)'] = \"2020ApJ...904...23K\"\n",
    "kim2020['DOI(s)'] = \"https://doi.org/10.3847/1538-4357/abb9a0\"\n",
    "\n",
    "kim2020['Notes'] = ''\n",
    "#kim2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we're matching the kim2020 table with the_whills\n",
    "tunique, tmatches, idx1, idx2 = match_tables_fib(the_whills,kim2020,5)\n",
    "\n",
    "print(len(tmatches))\n",
    "\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "#for index, row in tmatches.iterrows():\n",
    "#    if row['Table_flag']!='Table2':\n",
    "#        tmatches.at[index, 'Paper(s)'] += ' ; Kim+2020'\n",
    "#        tmatches.at[index, 'BibCode(s)'] += ' ; 2020ApJ...904...23K' \n",
    "#        tmatches.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/abb9a0'\n",
    "#        tmatches.at[index, 'Notes'] += ' Kim+2020 reselected this based on double-peaked lines and compared to spectra from known dual AGNs. Also looked for companions.'\n",
    "\n",
    "for i, j in zip(idx1, idx2):\n",
    "    the_whills.at[i, 'Paper(s)'] += ' ; Kim+2020'\n",
    "    the_whills.at[i, 'BibCode(s)'] += ' ; 2020ApJ...904...23K' \n",
    "    the_whills.at[i, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/abb9a0'\n",
    "    the_whills.at[i, 'Notes'] += ' Kim+2020 reselected this based on double-peaked lines and compared to spectra from known dual AGNs. Also looked for companions.'\n",
    "\n",
    "kim2020.drop(idx2, axis=0, inplace=True)\n",
    "kim2020.reset_index(drop=True, inplace=True)\n",
    "\n",
    "the_whills = pd.concat([the_whills,kim2020])\n",
    "\n",
    "\n",
    "\n",
    "## Now clipping out all Comerford+2013 rows from the matches table\n",
    "#tmatches = tmatches[tmatches['Table_flag']!='Table2'].reset_index(drop=True)\n",
    "##\n",
    "#\n",
    "## now adding in code to strip out objects that are for some reason listed in tunique but should only be tmatches....\n",
    "#mask = tunique['Name'].isin(tmatches['Name']) #shamelessly asked chatgpt to quickly write this because I was too lazy to go find my old script\n",
    "#tunique = tunique[~mask]\n",
    "#print(len(tmatches))\n",
    "#print(len(tunique))\n",
    "#\n",
    "### Concatenating everything together to generate a master table here\n",
    "#the_whills = pd.concat([tmatches,tunique]).sort_values(by='Name').reset_index(drop=True)\n",
    "###the_whills.drop(labels=['level_0','index'], axis=1, inplace=True)\n",
    "\n",
    "# \n",
    "\n",
    "# However, Kim+2020 claim that there should be an overlap of 8 objects between them and Smith+2010 \\\n",
    "# (so there should be 69 unique objects to Kim+). However, I only find 7 matches (so 70 unique candidates). \\\n",
    "# Even if I go out to 20'', I do not find the missing object. I think this is a typo in their manuscript, \\\n",
    "# as when I match the_whills and Kim+2020, and then I check for duplicate names using the first 6-7 characters \\\n",
    "# python finds no duplicate entries. We may want to consider reaching out. The 8th match is with Ge+2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = the_whills['Name'].to_list()\n",
    "matched_pairs = find_matches(names)\n",
    "\n",
    "for pair in matched_pairs:\n",
    "    print(f\"Match found: {pair[0]} and {pair[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we're adding in the results from Foord+2020\n",
    "\n",
    "objs = ['J014209-005049','J075223.35+273643.1','J084135.09+010156.2','J085416.76+502632\\\n",
    ".0','J095207.62+255257.2','J100654.20+464717.2','J112659.54+294442.8','J123915.40+531414.6',\\\n",
    "        'J132231.86+263159.1','J135646.11+102609.1','J144804.17+182537.9','J160436.21+500958.1']\n",
    "# Foord+ listed J014209-005049 as J014209.01-005050.0\n",
    "# Adding the DOI, author, and bibcode info to all of the Liu+2010 rows here in the matches table...\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Foord+2020'\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2020ApJ...892...29F' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.3847/1538-4357/ab72fa'\n",
    "        #the_whills.at[index, 'Notes']='AGNs confirmed via X-rays and reanalyses of optical spectroscopy.'\n",
    "        #the_whills.at[index, 'Confirmation Method'] = 'X-ray Imaging / X-ray Spectroscopy / Optical Spectroscopy'\n",
    "\n",
    "#the_whills.at[index, 'System Type']='Dual AGN'\n",
    "\n",
    "objs = ['J112659.54+294442.8']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        #the_whills.at[index, 'System Type']='Dual AGN'\n",
    "        the_whills.at[index, 'Notes']+=' Foord+ confirm the claim by Comerford+ that this is a dual AGN. BAYMAX favors a dual X-ray source model.'\n",
    "\n",
    "objs = ['J084135.09+010156.2']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        the_whills.at[index, 'Notes']+=' Foord+2020 conclude that this is likely a single resolved point source.'\n",
    "\n",
    "objs = ['J075223.35+273643.1','J144804.17+182537.9','J135646.11+102609.1']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        the_whills.at[index, 'Notes']+=' Foord+2020 find that this source has strong BF values in favor of a dual X-ray point source model.'\n",
    "\n",
    "objs = ['J075223.35+273643.1']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        the_whills.at[index, 'Notes']+=' Specifically the dual point source model is only favored when using non-informative priors However they cannot strongly conclude that this system consistutes a dual AGN despite a strong indication in the X-rays.'\n",
    "\n",
    "objs = ['J144804.17+182537.9']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        the_whills.at[index, 'Notes']+=' Though a dual point source model is favored with informative and non-informative priors the luminosity is below their AGN criterion and the X-ray spectrum of the secondary is very soft.'\n",
    "\n",
    "objs = ['J014209-005049','J084135.09+010156.2','J095207.62+255257.2','J123915.40+531414.6',\\\n",
    "        'J132231.86+263159.1','J014209.01−005050.0','J085416.76+502632.0','J100654.20+464717.2',\\\n",
    "        'J160436.21+500958.1']\n",
    "for index, row in the_whills.iterrows():\n",
    "    if row['Name'] in objs:\n",
    "        the_whills.at[index, 'Notes']+=' Foord+2020 find that this source has BF values that favor a single point source model.'\n",
    "\n",
    "\n",
    "# verified that all of this matching works properly\n",
    "# However, it is missing the entry for J0841+0101 because J0841+0101 does not yet exists in this table. \\\n",
    "# J0841+0101 was added to Comerford+'s sample in 2015 (I think)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, row in the_whills.iterrows():\n",
    "#    if 'Foord+2020' in row['Paper(s)']:\n",
    "#        print(row['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're adding in just some doi information for Song+2020\n",
    "\n",
    "for index, row in the_whills.iterrows():\n",
    "    if 'Smith+2010' in row['Paper(s)']:\n",
    "        #print('True')\n",
    "        the_whills.at[index, 'Paper(s)'] += ' ; Song+2020 '\n",
    "        the_whills.at[index, 'BibCode(s)'] += ' ; 2020MNRAS.491.4023S ' \n",
    "        the_whills.at[index, 'DOI(s)'] += ' ; https://doi.org/10.1093/mnras/stz3354'\n",
    "        the_whills.at[index, 'Notes'] += ' Song+2020 examined a subsample of the double-peaked objects in Smith+2010 but did not include a table specifying the objects. We include a reference to all of them for now. Song+ disfavor a binary hypothesis.'\n",
    "\n",
    "# verified that this cell works properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the table now to drop bad columns\n",
    "the_whills.drop(labels=['level_0','index','SDSS','f_SDSS','Vel','logL','Type','Q','zr','zh','MJD','Plate',\\\n",
    "                        'Fib','e_z','rmag','sigma','e_sigma','DelVb','e_DelVb','DelVr','e_DelVr','sigb',\\\n",
    "                        'e_sigb','sigr','e_sigr','FbO3','e_FbO3','FrO3','e_FrO3','FbHa','e_FbHa','FrHa',\\\n",
    "                        'e_FrHa','T','z1','NDWFS','delta_z','zsdss','Table_flag','Designation','r','Fiber',\\\n",
    "                        'zg','n_zg','FWHM[OIII]1','FWHM[OIII]2','v[OIII]1','v[OIII]2','vHb1','vHb2','EW[OIII]1',\\\n",
    "                        'EW[OIII]2','plate','fiber','mjd','z_x','select','ra','dec','magu','magg','magr',\\\n",
    "                        'magi','magz','emagu','emagg','emagr','emagi','emagz','oiiilum','oiiiflux','oiiirew',\\\n",
    "                        'oiiiw80','nevflux','nevrew','nevsnr','w1','w2','w3','w4','ew1','ew2','ew3','ew4',\\\n",
    "                        'lum5','lum12','unique','z_y','amp1','vel1','sig1','amp2','vel2','sig2','amp3','vel3',\\\n",
    "                        'sig3','amp4','vel4','sig4','fwhm','fwqm','w50','w80','w90','relasym','r9050','dp',\\\n",
    "                        'ra_sexagesimal','dec_sexagesimal','J_format','Name(SDSS)','r(mag)','Reduced_chi2',\\\n",
    "                        'Reduced_chi2.1','deltaV'], axis=1, inplace=True)\n",
    "\n",
    "the_whills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming RA and Dec to RA1 and Dec1\n",
    "\n",
    "the_whills.rename(columns={\"RA\": \"RA1\", \"Dec\": \"Dec1\"}, inplace=True)\n",
    "\n",
    "#the_whills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_whills.rename(columns={\"Name\": \"Name1\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally saving the table now\n",
    "the_whills.to_csv('Double-peaked_dual_agn_candidates.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated programs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here's a function that we'll use to match catalogs together, since by this point our column names will be uniform\n",
    "\n",
    "\n",
    "# def match_tables(t1,t2,match_tol):\n",
    "#     # First we begin by matching RA1 and Dec1 of t1 to RA1 and Dec1 of t2\n",
    "#     c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "#     c2 = SkyCoord(ra=t2['RA1_deg']*u.degree, dec=t2['Dec1_deg']*u.degree) # storing coordinates for table 2\n",
    "#     idx, d2d, d3d = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "#     # idx are the indices in table 2 which are the closest matching rows to table 1\n",
    "    \n",
    "#     # Adding a match tolerance here, with user input for the function\n",
    "#     max_sep = match_tol * u.arcsec # The max match tolerance will be 5'\n",
    "#     sep_constraint = d2d <= max_sep # Filtering the angular separations from the matches to ≤ 5''\n",
    "    \n",
    "#     # Now applying the match tolerance to the matches for each table\n",
    "#     c1_matches = c1[sep_constraint] # applying to the matches in table 1\n",
    "#     c1_matches = c2[idx[sep_constraint]] # applying to the matches in table 2\n",
    "\n",
    "#     # Now storing the relevant index information for later...\n",
    "#     c2_matchesRA1 = pd.DataFrame() # Creating an empty dataframe to store the indices and coordinates of the matches in the KT catalog\n",
    "#     c2_matchesRA1['idx'] = idx[sep_constraint] # Storing the indices\n",
    "#     c2_matchesRA1['RA1'] = c2_matches.ra.degree # Storing the coordinates in these two rows\n",
    "#     c2_matchesRA1['Dec1'] = c2_matches.dec.degree\n",
    "    \n",
    "#     # Next, we match RA1 and Dec1 of t1 to RA2 and Dec2 of t2, just to ensure we haven't matched to the wrong nucleus \\\n",
    "#     # and missed a matching system...\n",
    "#     c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "#     c2 = SkyCoord(ra=t2['RA2_deg']*u.degree, dec=t2['Dec2_deg']*u.degree) # storing coordinates for table 2\n",
    "#     idx, d2d, d3d = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "    \n",
    "#     # Adding a match tolerance again here\n",
    "#     sep_constraint = d2d <= max_sep # Filtering the angular separations from the matches to ≤ 5''\n",
    "    \n",
    "#     # Now applying the match tolerance to the matches for each table\n",
    "#     c1_matches = c1[sep_constraint] # applying to the matches in table 1\n",
    "#     c1_matches = c2[idx[sep_constraint]] # applying to the matches in table 2\n",
    "\n",
    "#     # Now storing the relevant index information for later...\n",
    "#     c2_matchesRA2 = pd.DataFrame() # Creating an empty dataframe to store the indices and coordinates of the matches in the KT catalog\n",
    "#     c2_matchesRA2['idx'] = idx[sep_constraint] # Storing the indices\n",
    "#     c2_matchesRA2['RA1'] = c2_matches.ra.degree # Storing the coordinates in these two rows\n",
    "#     c2_matchesRA2['Dec1'] = c2_matches.dec.degree\n",
    "\n",
    "#     # Concatenating these frames now\n",
    "#     frames = [c2_matchesRA1,c2_matchesRA2]\n",
    "#     c2_dups = (pd.concat(frames)).reset_index(drop=True)\n",
    "    \n",
    "#     # We will defer to table 1, so we're dropping the rows from table 2 below\n",
    "#     # Now removing from table 2 the sources that also appear in table 1 by removing the indices listed in the \\\n",
    "#     # c2_matches table\n",
    "#     t2 = (t2[~t2['index'].isin(c2_dups['idx'])]).reset_index(drop=True)\n",
    "#     t2.drop(labels=['index'], axis=1, inplace=True) # Now dropping the index column as it is not needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here's a function that we'll use to match catalogs together, since by this point our column names will be uniform\n",
    "\n",
    "# def match_tables_fib(t1,t2,match_tol):\n",
    "#     if 'level_0' in t1.columns:\n",
    "#         t1.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "#     t1.reset_index(drop=False, inplace=True)\n",
    "#     if 'level_0' in t2.columns:\n",
    "#         t2.drop(labels=['level_0'], axis=1, inplace=True)\n",
    "#     t2.reset_index(inplace=True, drop=False)\n",
    "#     # First we begin by matching RA1 and Dec1 of t1 to RA1 and Dec1 of t2\n",
    "#     c1 = SkyCoord(ra=t1['RA1_deg']*u.degree, dec=t1['Dec1_deg']*u.degree) # Storing coordinates for table 1\n",
    "#     c2 = SkyCoord(ra=t2['RA1_deg']*u.degree, dec=t2['Dec1_deg']*u.degree) # storing coordinates for table 2\n",
    "#     idx2, d2d2, d3d2 = match_coordinates_sky(c1, c2) # Now matching table 1 to table 2\n",
    "#     # idx2 are the indices in table 2 which are the closest matching rows to table 1\n",
    "    \n",
    "#     # Adding a match tolerance here, with user input for the function\n",
    "#     max_sep = match_tol * u.arcsec # The max match tolerance will be 5'\n",
    "#     sep_constraint = d2d2 <= max_sep # Filtering the angular separations from the matches to ≤ 5''\n",
    "    \n",
    "#     # Now applying the match tolerance to the matches for each table\n",
    "#     #c1_matches = c1[sep_constraint] # applying to the matches in table 1\n",
    "#     c2_matches = c2[idx2[sep_constraint]] # applying to the matches in table 2\n",
    "\n",
    "#     #print(c1_matches)\n",
    "#     #print(c2_matches)\n",
    "#     # Now storing the relevant index information for later...\n",
    "#     c2_matchesRA1 = pd.DataFrame() # Creating an empty dataframe to store the indices and coordinates of the matches in table 2\n",
    "#     c2_matchesRA1['idx2'] = idx2[sep_constraint] # Storing the indices\n",
    "#     c2_matchesRA1['RA1'] = c2_matches.ra.degree # Storing the coordinates in these two rows\n",
    "#     c2_matchesRA1['Dec1'] = c2_matches.dec.degree\n",
    "    \n",
    "#     matches = pd.DataFrame()\n",
    "#     matches['idx2'] = idx2[sep_constraint]\n",
    "    \n",
    "#     # Concatenating these frames now\n",
    "#     frames = [c2_matchesRA1]\n",
    "#     c2_dups = (pd.concat(frames)).reset_index(drop=True)\n",
    "#     #print(t2)\n",
    "#     # We will defer to table 1, so we're dropping the rows from table 2 below\n",
    "#     # Now removing from table 2 the sources that also appear in table 1 by removing the indices listed in the \\\n",
    "#     # c2_matches table\n",
    "#     t2unique = (t2[~t2['index'].isin(c2_dups['idx2'])]).reset_index(drop=True)\n",
    "#     # And retaining a copy of the matches:\n",
    "#     t2matches = (t2[t2['index'].isin(c2_dups['idx2'])]).reset_index(drop=True)\n",
    "#     t2unique.drop(labels=['index'], axis=1, inplace=True) # Now dropping the index column as it is not needed\n",
    "#     #print(c1_matches)\n",
    "#     #print(c2_matches)\n",
    "#     #print(t2)\n",
    "    \n",
    "    \n",
    "#     # Need to add in some commands here that append bib and author info to table 1 before removing the rows \\\n",
    "#     # from table 2\n",
    "#     idx1, d2d1, d3d1 = match_coordinates_sky(c2, c1) # Now matching table 1 to table 2\n",
    "#     sep_constraint = d2d1 <= max_sep # Filtering the angular separations from the matches to ≤ 5''\n",
    "#     c1_matches = c1[idx1[sep_constraint]] # applying to the matches in table 2\n",
    "#     #print(c1_matches)\n",
    "    \n",
    "#     # Now storing the relevant index information for later...\n",
    "#     c1_matchesRA1 = pd.DataFrame() # Creating an empty dataframe to store the indices and coordinates of the matches in table 2\n",
    "#     c1_matchesRA1['idx1'] = idx1[sep_constraint] # Storing the indices\n",
    "#     c1_matchesRA1['RA1'] = c1_matches.ra.degree # Storing the coordinates in these two rows\n",
    "#     c1_matchesRA1['Dec1'] = c1_matches.dec.degree\n",
    "    \n",
    "#     # Concatenating these frames now\n",
    "#     frames = [c1_matchesRA1]\n",
    "#     c1_dups = (pd.concat(frames)).reset_index(drop=True)\n",
    "    \n",
    "#     matches['idx1'] = idx1[sep_constraint]\n",
    "#     #matches['index'] = matches['idx1']\n",
    "#     #print(matches)\n",
    "\n",
    "\n",
    "#     # Now adding in info from Table 2 into Table 1\n",
    "#     # First we'll remove the non-unique rows:\n",
    "#     t1unique = (t1[~t1['index'].isin(c1_dups['idx1'])]).reset_index(drop=True)\n",
    "#     # Next we'll make a dataframe for the matches, and begin adding in the information from table 2\n",
    "#     t1matches = (t1[t1['index'].isin(c1_dups['idx1'])])#.reset_index(drop=True) #I'm not sure why I have to use . notation here instead of brackets\n",
    "#     #t1\n",
    "#     #print(matches)\n",
    "    \n",
    "#     #print(t1matches)\n",
    "#     #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'Paper(s)'] += \" ; \" + t2['Paper(s)'][0]\n",
    "#     #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'BibCode(s)'] += \" ; \" + t2['BibCode(s)'][0]\n",
    "#     #t1matches.loc[t1matches['index'].isin(c1_dups['idx1']), 'DOI(s)'] += \" ; \" + t2['DOI(s)'][0]\n",
    "#     t1matches.merge(matches, left_on=['index'], right_on=['idx1'])\n",
    "#     #print(t1matches)\n",
    "#     print(matches.dtypes)\n",
    "#     #.loc[matches.loc['idx1','idx2'],\n",
    "#     #.loc[matches.loc['idx1','idx2'],\n",
    "#     #.loc[matches.loc['idx1','idx2'],\n",
    "#     t1unique.drop(labels=['index'], axis=1, inplace=True) # Now dropping the index column as it is not needed\n",
    "#     #print(c1_matchesRA1)\n",
    "#     #print(c2_matches)\n",
    "#     #print(matches)\n",
    "#     #print(t1)\n",
    "\n",
    "#     return t1matches, t2matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def name_to_coords(df,dfcol): #for WANG+2009\n",
    "#    # A function to take in the SDSS designation in a form JXXXXXX+XXXXXX and convert this to sexagesimal coordinates, where the '+' could be '+' or '-'.\n",
    "#    # Output form will be: XX:XX:XX +XX:XX:XX, where the '+' could be '+' or '-'\n",
    "#    df['Coordinates'] = dfcol.str.slice(start=1) # Stripping the J\n",
    "#    df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=6) # Stripping the DEC parts \n",
    "#    df['Dec_test'] = df['Coordinates'].str.slice(start=6, stop=13) # Stripping the RA parts\n",
    "#    df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=6) # Putting together the RA coordinates separated by colons\n",
    "#    df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=8) # Putting together the Dec coodinates separated by colons\n",
    "#    df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "#    return\n",
    "#\n",
    "## This function is for converting namings in the following format to coordinates. This is useful for later \\\n",
    "## catalogs 2010-2020\n",
    "#\n",
    "#def name_to_coords(df,dfcol): # FOR LIU+2010a\n",
    "#    # A function to take in the SDSS designation in a form JXXXXXX.XX+XXXXXX.X and convert this to sexagesimal coordinates, where the '+' could be '+' or '-'.\n",
    "#    # Output form will be: XX:XX:XX.XX +XX:XX:XX.X, where the '+' could be '+' or '-'\n",
    "#    df['Coordinates'] = dfcol.str.slice(start=1) # Stripping the J\n",
    "#    df['RA_test'] = df['Coordinates'].str.slice(start=0, stop=9) # Stripping the DEC parts \n",
    "#    df['Dec_test'] = df['Coordinates'].str.slice(start=9, stop=19) # Stripping the RA parts\n",
    "#    df['RA'] = df['RA_test'].str.slice(start=0, stop=2)+\":\"+df['RA_test'].str.slice(start=2, stop=4)+\":\"+df['RA_test'].str.slice(start=4, stop=9) # Putting together the RA coordinates separated by colons\n",
    "#    df['Dec'] = df['Dec_test'].str.slice(start=0, stop=3)+\":\"+df['Dec_test'].str.slice(start=3, stop=5)+\":\"+df['Dec_test'].str.slice(start=5, stop=10) # Putting together the Dec coodinates separated by colons\n",
    "#    df.drop(columns=['Coordinates','RA_test','Dec_test'], inplace=True)\n",
    "#    return\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
